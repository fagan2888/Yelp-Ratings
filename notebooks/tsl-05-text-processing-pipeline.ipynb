{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Yelp Review Corpus\n",
    "\n",
    "This notebook outlines the steps to load, preprocess, and clean yelp review text.\n",
    "\n",
    "It was heavily inspired by: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4).pprint\n",
    "\n",
    "data_dir = '/home/tlappas/data_science/Yelp-Ratings/data/processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move to ./Yelp-Ratings/data/corpus folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...This needs to be fixed. Breaks if run more than once.\n",
    "\n",
    "path = os.getcwd()\n",
    "print('Notebook path: {}'.format(path))\n",
    "\n",
    "os.chdir('..')\n",
    "data_path = os.path.join(os.getcwd(), 'corpus')\n",
    "print('Corpus path: {}'.format(data_path))\n",
    "\n",
    "if os.path.exists(data_path) == False:\n",
    "    os.mkdir(data_path)\n",
    "\n",
    "os.chdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "Text is stored in yelp db. Query and store in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect('dbname=yelp user=tlappas host=/var/run/postgresql')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    select review.review_text \n",
    "    from review, business, user_info\n",
    "    where review.user_id = user_info.user_id\n",
    "    and business.business_id = review.business_id\n",
    "    and business.categories LIKE '%Restaurants%'\n",
    "    and length(user_info.elite) != 0 \n",
    "    limit 1\n",
    "\"\"\")\n",
    "\n",
    "cols = ['review_text']\n",
    "\n",
    "data = pd.DataFrame(cur.fetchall(), columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('DataFrame shape: {}\\n'.format(data.shape))\n",
    "print('First instance: \\n')\n",
    "print(data.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(data.loc[1, 'review_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Pre-Preprocessing\n",
    "\n",
    "1. Remove any non-ASCII characters\n",
    "2. Replace any characters that aren't alphanumeric/whitespace/\"'\"\n",
    "3. Convert all letters to lowercase\n",
    "4. Replace whitespace with ' '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(data.loc[:,'review_text']):\n",
    "\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s']\", '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[\\n|\\r|\\n\\r|\\r\\n]', ' ', text)\n",
    "    data.loc[i,'review_text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get the a wreck  it has everything roast beef turkey and ham oh my  i like to add oil and italian seasoning with hot giardiniera if you really want to thrill your tastebuds   love the selection of zapp's chips here goes great with your sammie op\n"
     ]
    }
   ],
   "source": [
    "print(data.loc[0,'review_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Corpus - Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing = ['ascii-only', 'whitespace alphanumeric only', 'lowercase', 'remove whitespace chars']\n",
    "with open(os.path.join(data_dir, 'reviews-clean.pkl'), 'wb') as f:\n",
    "    pickle.dump([data, processing], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "for i, text in data.loc[:,'review_text'].iterrows():\n",
    "    text = [word for word in text.split() if word not in stops]\n",
    "    text = ' '.join(text)\n",
    "    data.at[i,'review_text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get wreck everything roast beef turkey ham oh like add oil italian seasoning hot giardiniera really want thrill tastebuds love selection zapp's chips goes great sammie op\n"
     ]
    }
   ],
   "source": [
    "print(data.at[0,'review_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Corpus - Cleaned + No Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing.append('no stopwords')\n",
    "with open(os.path.join(data_dir, 'reviews-clean-nostop.pkl'), 'wb') as f:\n",
    "    pickle.dump([data, processing], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove neutral stopwords\n",
    "\n",
    "Since we're attempting to differentiate negative reviews from positive reviews negations may be important. For example 'not', 'no', 'hasn't', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_stops = [\n",
    "    'no', 'not', 'nor', 'don\\'', 'don\\'t', 'ain', \n",
    "    'ain\\'t', 'aren\\'t', 'aren', 'couldn', 'couldn\\'t', \n",
    "    'didn', 'didn\\'t', 'doesn', 'doesn\\'t', 'hadn', \n",
    "    'hadn\\'t', 'hasn', 'hasn\\'t', 'haven', 'haven\\'t',\n",
    "    'isn', 'isn\\'t', 'mightn', 'mightn\\'t', 'mustn', \n",
    "    'mustn\\'t', 'needn', 'needn\\'t', 'shan', 'shan\\'t',\n",
    "    'shouldn', 'shouldn\\'t', 'wasn', 'wasn\\'t', 'weren',\n",
    "    'weren\\'t', 'won', 'won\\'t', 'wouldn', 'wouldn\\'t'\n",
    "]\n",
    "\n",
    "no_neg_stops = [word for word in stops if word in stops and word not in neg_stops]\n",
    "\n",
    "data_neg_stops = []\n",
    "processing = []\n",
    "\n",
    "with open(os.path.join(data_dir, 'reviews-clean.pkl'), 'rb') as f:\n",
    "    [data_neg_stops, processing] = pickle.load(f)\n",
    "    \n",
    "for i, text in data_neg_stops.iterrows():\n",
    "    text = [word for word in text.split() if word not in stops]\n",
    "    text = ' '.join(text)\n",
    "    data.at[i,'review_text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_text\n"
     ]
    }
   ],
   "source": [
    "print(data.at[0,'review_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Corpus - Cleaned + No Neutral Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing.append('Remove all pos/neutral stopwords')\n",
    "with open(os.path.join(data_dir, 'reviews-clean-neg-stops.pkl'), 'wb') as f:\n",
    "    pickle.dump([data, processing], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize Text\n",
    "\n",
    "The wordnet lemmatizer only lemmatizes a single pos at a time. Default pos param is 'N' (noun). [Set of pos tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "\n",
    "[Here's an example](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#wordnetlemmatizerwithappropriatepostag) that ties in the nltk pos tagger to identify the pos, then use the lemmatizer to lem just that type. Which will save a huge amount of time, since it won't need to go through every one.\n",
    "\n",
    "[Wordnet documentation](http://www.nltk.org/howto/wordnet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "for i, text in data.loc[:,'review_text'].iterrows():\n",
    "    text = text.split()\n",
    "    text = [wnl.lemmatize(word, pos='n') for word in text]\n",
    "    text = [wnl.lemmatize(word, pos='v') for word in text]\n",
    "    text = [wnl.lemmatize(word, pos='a') for word in text]\n",
    "    text = [wnl.lemmatize(word, pos='r') for word in text]\n",
    "    text = ' '.join(text)\n",
    "    data.at[i,'review_text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_text\n"
     ]
    }
   ],
   "source": [
    "print(data.at[0,'review_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Corpus - Cleaned + No Stopwords + Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing.append('lemmatize: NVJR')\n",
    "with open(os.path.join(data_dir, 'reviews-clean-nostop-lemmed.pkl'), 'wb') as f:\n",
    "    pickle.dump([data, processing], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
