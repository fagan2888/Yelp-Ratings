{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorry for this crazy import cell... this will be cleaned up\n",
    "\n",
    "import psycopg2\n",
    "import numpy\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import nltk.corpus\n",
    "from nltk.text import TextCollection\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "stops = nltk.corpus.stopwords.words('english')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input your PostGres credentials to connect\n",
    "\n",
    "dbname = 'yelp'\n",
    "username = 'postgres'\n",
    "host = 'localhost'\n",
    "password = 'Khoobam1234!'\n",
    "\n",
    "conn = psycopg2.connect('dbname={} user={} host={} password={}'.format(dbname, username, host, password))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    SELECT * FROM review LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "cols = ['review_id', 'user_id', 'business_id', 'stars', 'review_date', 'review_text', 'useful', 'funny', 'cool']\n",
    "\n",
    "review_sample = pd.DataFrame(cur.fetchall(), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contractions:\n",
    "https://gist.github.com/J3RN/ed7b420a6ea1d5bd6d06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_nonneg_contr = [\"could've\",\n",
    "\"he'd\",\n",
    "\"he'd've\",\n",
    "\"he'll\",\n",
    "\"he's\",\n",
    "\"how'd\",\n",
    "\"how'll\",\n",
    "\"how's\",\n",
    "\"i'd\",\n",
    "\"i'd've\",\n",
    "\"i'll\",\n",
    "\"i'm\",\n",
    "\"i've\",\n",
    "\"it'd\",\n",
    "\"it'd've\",\n",
    "\"it'll\",\n",
    "\"it's\",\n",
    "\"let's\",\n",
    "\"ma'am\",\n",
    "\"might've\",\n",
    "\"must've\",\n",
    "\"o'clock\",\n",
    "\"'ow's'at\",\n",
    "\"she'd\",\n",
    "\"she'd've\",\n",
    "\"she'll\",\n",
    "\"she's\",\n",
    "\"should've\",\n",
    "\"somebody'd\",\n",
    "\"somebody'd've\",\n",
    "\"somebody'll\",\n",
    "\"somebody's\",\n",
    "\"someone'd\",\n",
    "\"someone'd've\",\n",
    "\"someone'll\",\n",
    "\"someone's\",\n",
    "\"something'd\",\n",
    "\"something'd've\",\n",
    "\"something'll\",\n",
    "\"something's\",\n",
    "\"that'll\",\n",
    "\"that's\",\n",
    "\"there'd\",\n",
    "\"there'd've\",\n",
    "\"there're\",\n",
    "\"there's\",\n",
    "\"they'd\",\n",
    "\"they'd've\",\n",
    "\"they'll\",\n",
    "\"they're\",\n",
    "\"they've\",\n",
    "\"'twas\",\n",
    "\"we'd\",\n",
    "\"we'd've\",\n",
    "\"we'll\",\n",
    "\"we're\",\n",
    "\"we've\",\n",
    "\"what'll\",\n",
    "\"what're\",\n",
    "\"what's\",\n",
    "\"what've\",\n",
    "\"when's\",\n",
    "\"where'd\",\n",
    "\"where's\",\n",
    "\"where've\",\n",
    "\"who'd\",\n",
    "\"who'd've\",\n",
    "\"who'll\",\n",
    "\"who're\",\n",
    "\"who's\",\n",
    "\"who've\",\n",
    "\"why'll\",\n",
    "\"why're\",\n",
    "\"why's\",\n",
    "\"would've\",\n",
    "\"y'all\",\n",
    "\"y'all'll\",\n",
    "\"y'all'd've\",\n",
    "\"you'd\",\n",
    "\"you'd've\",\n",
    "\"you'll\",\n",
    "\"you're\",\n",
    "\"you've\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_stops1 = ['no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'don',\n",
    " \"don't\",\n",
    " 'ain',\n",
    " 'aren',\n",
    " \"aren't\",\n",
    " 'couldn',\n",
    " \"couldn't\",\n",
    " 'didn',\n",
    " \"didn't\",\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'hadn',\n",
    " \"hadn't\",\n",
    " 'hasn',\n",
    " \"hasn't\",\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'isn',\n",
    " \"isn't\",\n",
    " 'mightn',\n",
    " \"mightn't\",\n",
    " 'mustn',\n",
    " \"mustn't\",\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'shouldn',\n",
    " \"shouldn't\",\n",
    " 'wasn',\n",
    " \"wasn't\",\n",
    " 'weren',\n",
    " \"weren't\",\n",
    " 'won',\n",
    " \"won't\",\n",
    " 'wouldn',\n",
    " \"wouldn't\",\n",
    " 'but',\n",
    " \"don'\",\n",
    " \"ain't\"]\n",
    "\n",
    "words_to_be_added = ['us']\n",
    "stops1 = ['i',\n",
    " 'me',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'we',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'you',\n",
    " \"you're\",\n",
    " \"you've\",\n",
    " \"you'll\",\n",
    " \"you'd\",\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves',\n",
    " 'he',\n",
    " 'him',\n",
    " 'his',\n",
    " 'himself',\n",
    " 'she',\n",
    " \"she's\",\n",
    " 'her',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'it',\n",
    " \"it's\",\n",
    " 'its',\n",
    " 'itself',\n",
    " 'they',\n",
    " 'them',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'themselves',\n",
    " 'what',\n",
    " 'which',\n",
    " 'who',\n",
    " 'whom',\n",
    " 'this',\n",
    " 'that',\n",
    " \"that'll\",\n",
    " 'these',\n",
    " 'those',\n",
    " 'am',\n",
    " 'is',\n",
    " 'are',\n",
    " 'was',\n",
    " 'were',\n",
    " 'be',\n",
    " 'been',\n",
    " 'being',\n",
    " 'have',\n",
    " 'has',\n",
    " 'had',\n",
    " 'having',\n",
    " 'do',\n",
    " 'does',\n",
    " 'did',\n",
    " 'doing',\n",
    " 'a',\n",
    " 'an',\n",
    " 'the',\n",
    " 'and',\n",
    " 'but',\n",
    " 'if',\n",
    " 'or',\n",
    " 'because',\n",
    " 'as',\n",
    " 'until',\n",
    " 'while',\n",
    " 'of',\n",
    " 'at',\n",
    " 'by',\n",
    " 'for',\n",
    " 'with',\n",
    " 'about',\n",
    " 'against',\n",
    " 'between',\n",
    " 'into',\n",
    " 'through',\n",
    " 'during',\n",
    " 'before',\n",
    " 'after',\n",
    " 'above',\n",
    " 'below',\n",
    " 'to',\n",
    " 'from',\n",
    " 'up',\n",
    " 'down',\n",
    " 'in',\n",
    " 'out',\n",
    " 'on',\n",
    " 'off',\n",
    " 'over',\n",
    " 'under',\n",
    " 'again',\n",
    " 'further',\n",
    " 'then',\n",
    " 'once',\n",
    " 'here',\n",
    " 'there',\n",
    " 'when',\n",
    " 'where',\n",
    " 'why',\n",
    " 'how',\n",
    " 'all',\n",
    " 'any',\n",
    " 'both',\n",
    " 'each',\n",
    " 'few',\n",
    " 'more',\n",
    " 'most',\n",
    " 'other',\n",
    " 'some',\n",
    " 'such',\n",
    " 'no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'only',\n",
    " 'own',\n",
    " 'same',\n",
    " 'so',\n",
    " 'than',\n",
    " 'too',\n",
    " 'very',\n",
    " 's',\n",
    " 't',\n",
    " 'can',\n",
    " 'will',\n",
    " 'just',\n",
    " 'don',\n",
    " \"don't\",\n",
    " 'should',\n",
    " \"should've\",\n",
    " 'now',\n",
    " 'd',\n",
    " 'll',\n",
    " 'm',\n",
    " 'o',\n",
    " 're',\n",
    " 've',\n",
    " 'y',\n",
    " 'ain',\n",
    " 'aren',\n",
    " \"aren't\",\n",
    " 'couldn',\n",
    " \"couldn't\",\n",
    " 'didn',\n",
    " \"didn't\",\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'hadn',\n",
    " \"hadn't\",\n",
    " 'hasn',\n",
    " \"hasn't\",\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'isn',\n",
    " \"isn't\",\n",
    " 'ma',\n",
    " 'mightn',\n",
    " \"mightn't\",\n",
    " 'mustn',\n",
    " \"mustn't\",\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'shouldn',\n",
    " \"shouldn't\",\n",
    " 'wasn',\n",
    " \"wasn't\",\n",
    " 'weren',\n",
    " \"weren't\",\n",
    " 'won',\n",
    " \"won't\",\n",
    " 'wouldn',\n",
    " \"wouldn't\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in neg_stops1:\n",
    "    if x in stops1:\n",
    "        stops1.remove(x)\n",
    "        \n",
    "stops1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stops = stops1 + common_nonneg_contr\n",
    "list(set(new_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell below creates a class that will be used in the vectorization code a few cells down. FYI, path in \"_load_words\" is machine specific. Also, you have to download the GloVe training set from GloVe website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._vocab, self._E = self._load_words()\n",
    "        \n",
    "    \n",
    "    def _load_words(self):\n",
    "        E = {}\n",
    "        vocab = []\n",
    "\n",
    "        with open(r'PUT YOUR PATH HERE', encoding=\"utf8\") as file:\n",
    "            for i, line in enumerate(file):\n",
    "                l = line.split(' ')\n",
    "                if l[0].isalpha():\n",
    "                    v = [float(i) for i in l[1:]]\n",
    "                    E[l[0]] = np.array(v)\n",
    "                    vocab.append(l[0])\n",
    "        return np.array(vocab), E            \n",
    "\n",
    "    \n",
    "    def _get_word(self, v):\n",
    "        for i, emb in enumerate(self._E):\n",
    "            if np.array_equal(emb, v):\n",
    "                return self._vocab[i]\n",
    "        return None\n",
    "    #in _doc_mean Ithink maybe we should get rid of w.lower.strip\n",
    "    def _doc_mean(self, doc):\n",
    "        return np.mean(np.array([self._E[w.lower().strip()] for w in doc if w.lower().strip() in self._E]), axis=0)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([self._doc_mean(doc) for doc in X])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    \n",
    "#retrived from https://www.kaggle.com/nhrade/text-classification-using-word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fyi, tweaked Alice's code here to allow map_on_column to accept another function\n",
    "def map_on_column(data, function):\n",
    "    data['review_text'] = data['review_text'].map(lambda x: function(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell below is preproccessing and tokenezation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _remove_stops(tokens):\n",
    "    stops = nltk.corpus.stopwords.words('english')\n",
    "    neg_stops = [\n",
    "    'no', 'not', 'nor', 'don\\'', 'don\\'t', 'ain', \n",
    "    'ain\\'t', 'aren\\'t', 'aren', 'couldn', 'couldn\\'t', \n",
    "    'didn', 'didn\\'t', 'doesn', 'doesn\\'t', 'hadn', \n",
    "    'hadn\\'t', 'hasn', 'hasn\\'t', 'haven', 'haven\\'t',\n",
    "    'isn', 'isn\\'t', 'mightn', 'mightn\\'t', 'mustn', \n",
    "    'mustn\\'t', 'needn', 'needn\\'t', 'shan', 'shan\\'t',\n",
    "    'shouldn', 'shouldn\\'t', 'wasn', 'wasn\\'t', 'weren',\n",
    "    'weren\\'t', 'won', 'won\\'t', 'wouldn', 'wouldn\\'t'\n",
    "    ]\n",
    "#still leaves in but and don.. fix this.. \n",
    "#doesn't get rid of other obvious stopwords, like i'm, they're....\n",
    "    for x in neg_stops:\n",
    "        if x in stops:\n",
    "            stops.remove(x)\n",
    "            \n",
    "    tokens_without_stops = [token for token in tokens if token not in stops]\n",
    "    return tokens_without_stops       \n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def _clean_review2(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    text = re.sub(r\"[^A-Za-z\\s']\", '', text)  \n",
    "    tokens = [token for token in text.split() if token not in new_stops]\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "    return tokens\n",
    "\n",
    "def _process_review2(text):\n",
    "    tokens = _remove_stops(_clean_review2(text))\n",
    "    return tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell below is vectorization. So far it can do TFIDF on specified number of n-grams, and I think the part with GloVe word embedding is almost functioning, but I haven't started \"word2vec\" vectorization yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "def GloVeDistributedRep(text):\n",
    "    GloVeVector = numpy.append(MeanEmbeddingTransformer().fit_transform(text))\n",
    "    return GloVeVector\n",
    "\n",
    "\n",
    "def vectorize(text, method, grams):\n",
    "    if method == 'TFIDF':\n",
    "        tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None, ngram_range = (1,grams))\n",
    "        TDIFvectors = tfidf.fit_transform(map_on_column(text, _clean_review2))\n",
    "        print(TDIFvectors)\n",
    "    if method == 'DistributedRep':\n",
    "        GloVevectors = GloVeDistributedRep(map_on_column(text, _clean_review2))\n",
    "        print(GloVevectors)\n",
    "    if method == 'Word2Vec':\n",
    "        print('not yet defined')\n",
    "    else:\n",
    "        print('choose vectorization method')\n",
    "        \n",
    "\n",
    "def vectorize2(text, method, grams):\n",
    "    if method == 'TFIDF':\n",
    "        tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None, ngram_range = (1,grams))\n",
    "        TDIFvectors = tfidf.fit_transform(text)\n",
    "        print(TDIFvectors)\n",
    "    if method == 'DistributedRep':\n",
    "        GloVevectors = GloVeDistributedRep(text)\n",
    "        print(GloVevectors)\n",
    "    if method == 'Word2Vec':\n",
    "        print('not yet defined')\n",
    "    else:\n",
    "        print('choose vectorization method')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization tests. FYI, takes unprocessed text so after you run one you must rerun query in line 3 to get text back to unprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this one works, or atleast I think it does. The output seems to be right. As you will notice, as you increase ngrams the size of the array increases but the value of each key decreases.\n",
    "vectorize(review_sample, 'TFIDF', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think this works but I was having a path issue on my comp so I don't know yet. So, it's not working \n",
    "#but I think that's just cause of a path issue in the class.\n",
    "vectorize(review_sample, 'DistributedRep', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is pretty technical. haven't started yet.\n",
    "vectorize(review_sample['review_text'], 'Word2Vec', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So the cells below are about n-gram creation. i couldn't get them all to work, but this may be moot work because as you can see above n-grams can be made on the fly within sklearn's TFIDF tool....... the word embeddings don't auto generate ngrams, though so we will probably eventually have to get the below code to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#side note: I couldn't find a way for the n_grams function to work by iterating through an array of a list of unigrams,\n",
    "#which is what Alice's map/apply function outputs. So I tried doing it this way, which is more in line with what the\n",
    "#orignal code from stack overflow shows. If it turns out that this approach is too slow or unworkable we may need to find\n",
    "#antoher way to make ngrams\n",
    "\n",
    "\n",
    "def uni_to_ngrams(text, n):\n",
    "        n_grams = ngrams(_clean_review2(text), n)\n",
    "        return [' '.join(grams) for grams in n_grams]\n",
    "    \n",
    "    \n",
    "def get_grams1(text, n):\n",
    "    for i in text:\n",
    "        for r in range(1, n):\n",
    "            return uni_to_ngrams(text, r)\n",
    "        \n",
    "def get_grams2(text, n):\n",
    "    for i in text:\n",
    "        return uni_to_ngrams(text, n)\n",
    "\n",
    "def get_all_grams1(text, n):\n",
    "    for r in range(1, n):\n",
    "        return get_grams2(text, r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this works when you feed it a string of text directly, but as you will see later if you try to iterate through the review_text column you get an error.\n",
    "uni_to_ngrams('Thisoooo isooo aoooo testoooo ooooffff theeee cooodeee hoppeeee itttt wooorrrksss kjaiofad', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this does not throw an error, but the point of the code is to return n sets of n-grams of r length, but instead it only returns one set of unigrams.\n",
    "get_all_grams1('Thisoooo isooo aoooo testoooo ooooffff theeee cooodeee hoppeeee itttt wooorrrksss kjaiofad', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this does not work. The error makes me think that I'm misunderstanding something about iteration or the input/output of Alice's _clean_review2 code\n",
    "get_all_grams1(review_sample['review_text'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample['review_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_process_review2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nltk.corpus\n",
    "from nltk.text import TextCollection\n",
    "\n",
    "def TFIDFvectorize(corpus):\n",
    "    corpus = [_clean_review2(doc) for doc in corpus]\n",
    "    texts = TextCollection(corpus)\n",
    "    \n",
    "    for doc in corpus:\n",
    "        return {\n",
    "            term: texts.tf_idf(term, doc)\n",
    "            for term in doc\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so as the function stands now it works on one review... although the way it's written I thought it would do all reviews.\n",
    "#also as you will see in next line, when I try to run it on the entie column it does not.\n",
    "#I think it doesnt work in apply column because TFIDF for each doc relies on the values of other docs\n",
    "#, so you can't overwrite in place as you iterate\n",
    "TFIDFvectorize(review_sample['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
