{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size:3em;color:red;\"> <b> Yelp </p> Ratings </p> Presentation </p> Notebook </b> </span>\n",
    "<br><br> Let's see who's better at classifying Yelp reviews -- you (humans) or our model? </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:left\" src=\"https://uproxx.files.wordpress.com/2015/10/south-park-yelp.png?w=650\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '../data/new_review_examples.csv'\n",
    "review_1 = pd.read_csv(csv_path, usecols = [3], names=['reviews'], nrows=1)\n",
    "review_2 = pd.read_csv(csv_path, usecols = [4], names=['reviews'], nrows=1)\n",
    "review_3 = pd.read_csv(csv_path, usecols = [5], names=['reviews'], nrows=1)\n",
    "review_4 = pd.read_csv(csv_path, usecols = [6], names=['reviews'], nrows=1)\n",
    "review_5 = pd.read_csv(csv_path, usecols = [7], names=['reviews'], nrows=1)\n",
    "review_1_stars = pd.read_csv(csv_path, usecols = [3], names=['stars'], skiprows=1)\n",
    "review_2_stars = pd.read_csv(csv_path, usecols = [4], names=['stars'], skiprows=1)\n",
    "review_3_stars = pd.read_csv(csv_path, usecols = [5], names=['stars'], skiprows=1)\n",
    "review_4_stars = pd.read_csv(csv_path, usecols = [6], names=['stars'], skiprows=1)\n",
    "review_5_stars = pd.read_csv(csv_path, usecols = [7], names=['stars'], skiprows=1)\n",
    "premade_reviews = pd.concat([review_1, review_2, review_3, review_4, review_5], ignore_index=True)\n",
    "premade_reviews['actual_stars'] = [2,4,1,3,5]\n",
    "submitted_reviews = pd.read_csv(csv_path, usecols = [1,2], names=['reviews', 'stars'], skiprows=1, encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1_counts = dict(Counter(review_1_stars.stars.tolist()))\n",
    "review_1_counts = [review_1_counts.get(1, 0), review_1_counts.get(2,0), review_1_counts.get(3,0), review_1_counts.get(4,0), review_1_counts.get(5,0)]\n",
    "review_2_counts = dict(Counter(review_2_stars.stars.tolist()))\n",
    "review_2_counts = [review_2_counts.get(1, 0), review_2_counts.get(2,0), review_2_counts.get(3,0), review_2_counts.get(4,0), review_2_counts.get(5,0)]\n",
    "review_3_counts = dict(Counter(review_3_stars.stars.tolist()))\n",
    "review_3_counts = [review_3_counts.get(1, 0), review_3_counts.get(2,0), review_3_counts.get(3,0), review_3_counts.get(4,0), review_3_counts.get(5,0)]\n",
    "review_4_counts = dict(Counter(review_4_stars.stars.tolist()))\n",
    "review_4_counts = [review_4_counts.get(1, 0), review_4_counts.get(2,0), review_4_counts.get(3,0), review_4_counts.get(4,0), review_4_counts.get(5,0)]\n",
    "review_5_counts = dict(Counter(review_5_stars.stars.tolist()))\n",
    "review_5_counts = [review_5_counts.get(1, 0), review_5_counts.get(2,0), review_5_counts.get(3,0), review_5_counts.get(4,0), review_5_counts.get(5,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize survey data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:left\" src=\"https://img.memecdn.com/bad-yelp-rebiew_c_4185847.webp\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "barWidth = 0.2\n",
    " \n",
    "bars1 = review_1_counts\n",
    "bars2 = review_2_counts\n",
    "bars3 = review_3_counts\n",
    "bars4 = review_4_counts\n",
    "bars5 = review_5_counts\n",
    " \n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "r5 = [x + barWidth for x in r4]\n",
    "\n",
    "plt.bar(r1, bars1, color='#101357', width=barWidth, edgecolor='white', label='review_1')\n",
    "plt.bar(r2, bars2, color='#fea49f', width=barWidth, edgecolor='white', label='review_2')\n",
    "plt.bar(r3, bars3, color='#fbaf08', width=barWidth, edgecolor='white', label='review_3')\n",
    "plt.bar(r4, bars4, color='#00a0a0', width=barWidth, edgecolor='white', label='review_4')\n",
    "plt.bar(r5, bars5, color='#007f4f', width=barWidth, edgecolor='white', label='review_5')\n",
    "\n",
    "plt.xlabel('Star', fontsize=22)\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], ['1', '2', '3', '4', '5'], fontsize=20)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,max(max(bars1), max(bars2), max(bars3), max(bars4), max(bars5))])\n",
    "plt.title('Star Count per Review', fontweight='bold', fontsize=25)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how your guesses measured up to the actual star ratings given to these reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The five reviews for which we solicited guesses from you all:\\n')\n",
    "for i in range(premade_reviews.shape[0]):\n",
    "    print('Review {}:\\n{}\\n\\nActual Star Rating: {}\\n\\n'.format(i+1, premade_reviews.loc[i, 'reviews'],premade_reviews.loc[i, 'actual_stars']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's better visualize your accuracy in guessing reviews' star ratings with pie charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pie(review_counts):\n",
    "    def get_labels():\n",
    "        non_zero_labels = []\n",
    "        for i in range(len(review_counts)):\n",
    "            if review_counts[i] != 0:\n",
    "                non_zero_labels.append(i+1)\n",
    "        return non_zero_labels\n",
    "\n",
    "    labels = get_labels()\n",
    "    sizes = [review_counts[i-1] for i in labels]\n",
    "    colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue', 'pink']\n",
    "\n",
    "    plt.pie(sizes, labels=labels, colors=colors,  shadow=True, startangle=140, autopct='%1.1f%%')\n",
    "    \n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, counts in enumerate([review_1_counts, review_2_counts, review_3_counts, review_4_counts, review_5_counts]):\n",
    "    print('Review {}'.format(i+1))\n",
    "    print('Actual star rating: {}'.format(premade_reviews.loc[i, 'actual_stars']))\n",
    "    make_pie(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at some of the reviews you guys submitted in our circulated survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of submitted reviews: {}'.format(submitted_reviews.shape[0]))\n",
    "print('Submitted reviews dataframe:')\n",
    "submitted_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    print('{}\\n\\nStar Rating: {}\\n\\n'.format(submitted_reviews.loc[i, 'reviews'], submitted_reviews.loc[i, 'stars']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the distribution of the star ratings you assigned to the reviews you wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted_reviews_counts = dict(Counter(submitted_reviews.stars.tolist()))\n",
    "submitted_reviews_counts = [submitted_reviews_counts.get(1,0), submitted_reviews_counts.get(2,0), submitted_reviews_counts.get(3,0), submitted_reviews_counts.get(4,0), submitted_reviews_counts.get(5,0)]\n",
    "\n",
    "bins = np.arange(7) - 0.5\n",
    "plt.hist(submitted_reviews.stars, bins, edgecolor='black')\n",
    "plt.xticks(range(6))\n",
    "plt.xlim([0, 6])\n",
    "plt.yticks(range(max(submitted_reviews_counts)+1))\n",
    "plt.ylim([0, max(submitted_reviews_counts)+1])\n",
    "plt.title('Star Counts for Submitted Reviews', fontsize=15, fontweight='bold')\n",
    "plt.xlabel('Star', fontsize=13)\n",
    "plt.ylabel('Count', fontsize=13)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean reviews for modeling ##\n",
    "\n",
    "Before we can compare your results to our model's results, we need to clean the reviews and vectorize them for input into the model.\n",
    "\n",
    "Cleaning/preprocessing steps:\n",
    "\n",
    "- Lowercase<br>\n",
    "- Remove non-ASCII characters<br>\n",
    "- Tokenize on pattern \\w+\\'?\\w+ (maintains internal apostrophes)<br>\n",
    "- Replace non-alphabetic characters (e.g., digits, %, *, &, $) with ''<br>\n",
    "- Remove stopwords (based on customized list that retains words with negative connotation (e.g., not, won't, no))<br>\n",
    "- Lemmatize (canonical/dictionary form of a word... babies --> baby)<br>\n",
    "- Remove rare tokens (30 threshold, tokens that appear less than or equal to 30 times in corpus will be removed)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:left\" src=\"https://pics.me.me/if-beauty-and-the-beast-happened-today-be-our-guest-11644791.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_stops = ['no',\n",
    "    'nor',\n",
    "    'not',\n",
    "    'don',\n",
    "    \"don't\",\n",
    "    'ain',\n",
    "    'aren',\n",
    "    \"aren't\",\n",
    "    'couldn',\n",
    "    \"couldn't\",\n",
    "    'didn',\n",
    "    \"didn't\",\n",
    "    'doesn',\n",
    "    \"doesn't\",\n",
    "    'hadn',\n",
    "    \"hadn't\",\n",
    "    'hasn',\n",
    "    \"hasn't\",\n",
    "    'haven',\n",
    "    \"haven't\",\n",
    "    'isn',\n",
    "    \"isn't\",\n",
    "    'mightn',\n",
    "    \"mightn't\",\n",
    "    'mustn',\n",
    "    \"mustn't\",\n",
    "    'needn',\n",
    "    \"needn't\",\n",
    "    'shan',\n",
    "    \"shan't\",\n",
    "    'shouldn',\n",
    "    \"shouldn't\",\n",
    "    'wasn',\n",
    "    \"wasn't\",\n",
    "    'weren',\n",
    "    \"weren't\",\n",
    "    \"won'\",\n",
    "    \"won't\",\n",
    "    'wouldn',\n",
    "    \"wouldn't\",\n",
    "    'but',\n",
    "    \"don'\",\n",
    "    \"ain't\"]\n",
    "\n",
    "common_nonneg_contr = [\"could've\",\n",
    "    \"he'd\",\n",
    "    \"he'd've\",\n",
    "    \"he'll\",\n",
    "    \"he's\",\n",
    "    \"how'd\",\n",
    "    \"how'll\",\n",
    "    \"how's\",\n",
    "    \"i'd\",\n",
    "    \"i'd've\",\n",
    "    \"i'll\",\n",
    "    \"i'm\",\n",
    "    \"i've\",\n",
    "    \"it'd\",\n",
    "    \"it'd've\",\n",
    "    \"it'll\",\n",
    "    \"it's\",\n",
    "    \"let's\",\n",
    "    \"ma'am\",\n",
    "    \"might've\",\n",
    "    \"must've\",\n",
    "    \"o'clock\",\n",
    "    \"'ow's'at\",\n",
    "    \"she'd\",\n",
    "    \"she'd've\",\n",
    "    \"she'll\",\n",
    "    \"she's\",\n",
    "    \"should've\",\n",
    "    \"somebody'd\",\n",
    "    \"somebody'd've\",\n",
    "    \"somebody'll\",\n",
    "    \"somebody's\",\n",
    "    \"someone'd\",\n",
    "    \"someone'd've\",\n",
    "    \"someone'll\",\n",
    "    \"someone's\",\n",
    "    \"something'd\",\n",
    "    \"something'd've\",\n",
    "    \"something'll\",\n",
    "    \"something's\",\n",
    "    \"that'll\",\n",
    "    \"that's\",\n",
    "    \"there'd\",\n",
    "    \"there'd've\",\n",
    "    \"there're\",\n",
    "    \"there's\",\n",
    "    \"they'd\",\n",
    "    \"they'd've\",\n",
    "    \"they'll\",\n",
    "    \"they're\",\n",
    "    \"they've\",\n",
    "    \"'twas\",\n",
    "    \"we'd\",\n",
    "    \"we'd've\",\n",
    "    \"we'll\",\n",
    "    \"we're\",\n",
    "    \"we've\",\n",
    "    \"what'll\",\n",
    "    \"what're\",\n",
    "    \"what's\",\n",
    "    \"what've\",\n",
    "    \"when's\",\n",
    "    \"where'd\",\n",
    "    \"where's\",\n",
    "    \"where've\",\n",
    "    \"who'd\",\n",
    "    \"who'd've\",\n",
    "    \"who'll\",\n",
    "    \"who're\",\n",
    "    \"who's\",\n",
    "    \"who've\",\n",
    "    \"why'll\",\n",
    "    \"why're\",\n",
    "    \"why's\",\n",
    "    \"would've\",\n",
    "    \"y'all\",\n",
    "    \"y'all'll\",\n",
    "    \"y'all'd've\",\n",
    "    \"you'd\",\n",
    "    \"you'd've\",\n",
    "    \"you'll\",\n",
    "    \"you're\",\n",
    "    \"you've\"]\n",
    "\n",
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
    "  'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "ranks = ['st', 'nd', 'rd', 'th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stopword_list(nltk_english = True, contractions = True, single_letters = True, rank_suffixes = True, remove_negs = True):\n",
    "\n",
    "    # Figure out if the stopwords corpus is present\n",
    "    try:\n",
    "        dir(nltk.corpus.stopwords)\n",
    "    except AttributeError:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "    # Assemble all the stopwords into a list\n",
    "    stops = []\n",
    "    if nltk_english:\n",
    "        stops += nltk.corpus.stopwords.words('english')\n",
    "    if contractions:\n",
    "        stops += common_nonneg_contr\n",
    "    if single_letters:\n",
    "        stops += letters\n",
    "    if rank_suffixes:\n",
    "        stops += ranks\n",
    "    stops += [\"\"] + ['us'] + [''] + [\"'\"]\n",
    "\n",
    "    # Remove all negative stopwords and any duplicates\n",
    "    if remove_negs:\n",
    "        stops = list(set(stops) - set(neg_stops))\n",
    "\n",
    "    return stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_review(df):\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "        tag_dict = {\"a\": wordnet.ADJ,\n",
    "                    \"n\": wordnet.NOUN,\n",
    "                    \"v\": wordnet.VERB,\n",
    "                    \"r\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    def _clean_review(text):\n",
    "        text = text.lower()\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "        tokenizer = nltk.RegexpTokenizer('\\w+\\'?\\w+')\n",
    "        filtered_tokens = [(re.sub(r\"[^A-Za-z\\s']\", '', token)) for token in tokenizer.tokenize(text)]\n",
    "        stops = create_stopword_list()\n",
    "        tokens = [token for token in filtered_tokens if token not in stops]\n",
    "        tokens = [re.sub(\"'s\", '', token) for token in tokens if re.sub(\"'s\", '', token) != '']\n",
    "        for i, token in enumerate(tokens):\n",
    "            tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "        tokens = [token for token in tokens if token not in stops]\n",
    "        return tokens\n",
    "    df['review_tokens'] = df['reviews'].apply(lambda x: _clean_review(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now clean/preprocess the reviews you submitted in our survey..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submitted_reviews = _process_review(submitted_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some examples of what the reviews look like in tokenized form after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):  \n",
    "    print('Full review:\\n{}'.format(submitted_reviews.loc[i, 'reviews']))\n",
    "    print('\\nTokenized review: \\n{}\\n\\n'.format(submitted_reviews.loc[i, 'review_tokens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our reviews in tokenized form, let's remove rare tokens using our previously pickled list of rare tokens based on our corpus of over 6.6 million Yelp reviews. Our list of rare tokens included 583595 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_tokens_30 = pickle.load(open('../data/rare_tokens_threshold30_copy.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_rare_tokens(df):\n",
    "    def _filter_rare_tokens(tokens):   \n",
    "        tokens_to_remove = list((set(tokens) & set(rare_tokens_30)))\n",
    "        frequent_tokens = [token for token in tokens if token not in tokens_to_remove]\n",
    "        return frequent_tokens\n",
    "    df['review_tokens'] = df['review_tokens'].apply(lambda x: _filter_rare_tokens(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted_reviews = _remove_rare_tokens(submitted_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fully preprocessed the reviews you submitted, let's do the same for the five reviews for which we had you guess star ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premade_reviews = _process_review(premade_reviews)\n",
    "premade_reviews = _remove_rare_tokens(premade_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model to predict ratings\n",
    "\n",
    "Below we will load in our pickled TF-IDF vectorizer fit to our data and our pickled stochastic gradient descent classifier (SGDC) model. We will first vectorize the reviews and then feed them (both preexisting and submitted by you) into the model to compare model predictions with human guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:left\" src=\"https://img.wonderhowto.com/img/17/27/63452217416656/0/human-vs-computer-scrabble-showdown.w1456.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfer = pickle.load(open('../data/pickled_TfidfVectorizer.pkl', 'rb'))\n",
    "SGDC = pickle.load(open('../data/SGDC.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premade_reviews['review_tokens'] = premade_reviews['review_tokens'].apply(' '.join)\n",
    "submitted_reviews['review_tokens'] = submitted_reviews['review_tokens'].apply(' '.join)\n",
    "tfidf_premade_reviews = tfidfer.transform(premade_reviews['review_tokens'])\n",
    "tfidf_submitted_reviews = tfidfer.transform(submitted_reviews['review_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's feed our vectorized reviews into the model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted_reviews_predictions = SGDC.predict(tfidf_submitted_reviews)\n",
    "premade_reviews_predictions = SGDC.predict(tfidf_premade_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize how the classifier performed in classifying the reviews you submitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(submitted_reviews.stars, submitted_reviews_predictions, [1,2,3,4,5],\n",
    "                          title='Confusion matrix for submitted reviews',\n",
    "                          cmap=plt.cm.Blues, normalize=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output above, how did the model do in classifying the reviews you all submitted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at who outperformed whom in classifying the five premade reviews -- humans or the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(premade_reviews.actual_stars, premade_reviews_predictions, [1,2,3,4,5],\n",
    "                          title='Confusion matrix for premade reviews',\n",
    "                          cmap=plt.cm.Blues, normalize=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the output above to you guys' performance in classifying the reviews via revisiting the pie charts generated earlier in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, counts in enumerate([review_1_counts, review_2_counts, review_3_counts, review_4_counts, review_5_counts]):\n",
    "    print('Review {}'.format(i+1))\n",
    "    print('Actual star rating: {}'.format(premade_reviews.loc[i, 'actual_stars']))\n",
    "    make_pie(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do you think? Would you trust yourselves more than the model for this classification task?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
