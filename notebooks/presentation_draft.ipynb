{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size:2.5em;color:red;\"> Yelp </p> Ratings </p> Presentation </p> Notebook\n",
    "\n",
    "Let's see how our model performs on your reviews and how its results compare to your review ratings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '../data/new_review_examples.csv'\n",
    "review_1 = pd.read_csv(csv_path, usecols = [3], names=['reviews'], nrows=1)\n",
    "review_2 = pd.read_csv(csv_path, usecols = [4], names=['reviews'], nrows=1)\n",
    "review_3 = pd.read_csv(csv_path, usecols = [5], names=['reviews'], nrows=1)\n",
    "review_4 = pd.read_csv(csv_path, usecols = [6], names=['reviews'], nrows=1)\n",
    "review_5 = pd.read_csv(csv_path, usecols = [7], names=['reviews'], nrows=1)\n",
    "review_1_stars = pd.read_csv(csv_path, usecols = [3], names=['stars'], skiprows=1)\n",
    "review_2_stars = pd.read_csv(csv_path, usecols = [4], names=['stars'], skiprows=1)\n",
    "review_3_stars = pd.read_csv(csv_path, usecols = [5], names=['stars'], skiprows=1)\n",
    "review_4_stars = pd.read_csv(csv_path, usecols = [6], names=['stars'], skiprows=1)\n",
    "review_5_stars = pd.read_csv(csv_path, usecols = [7], names=['stars'], skiprows=1)\n",
    "premade_reviews = pd.concat([review_1, review_2, review_3, review_4, review_5], ignore_index=True)\n",
    "premade_reviews_actual_ratings = [2,4,1,3,5]\n",
    "submitted_reviews = pd.read_csv(csv_path, usecols = [1,2], names=['reviews', 'stars'], skiprows=1, encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize survey data\n",
    "\n",
    "First let's take a look at people's guesses for the star ratings of the five preexisting Yelp reviews in our survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1_stars = review_1_stars.stars.tolist()\n",
    "review_2_stars = review_2_stars.stars.tolist()\n",
    "review_3_stars = review_3_stars.stars.tolist()\n",
    "review_4_stars = review_4_stars.stars.tolist()\n",
    "review_5_stars = review_5_stars.stars.tolist()\n",
    "review_1_counts = dict(Counter(review_1_stars))\n",
    "review_1_counts = [review_1_counts.get(1, 0), review_1_counts.get(2,0), review_1_counts.get(3,0), review_1_counts.get(4,0), review_1_counts.get(5,0)]\n",
    "review_2_counts = dict(Counter(review_2_stars))\n",
    "review_2_counts = [review_2_counts.get(1, 0), review_2_counts.get(2,0), review_2_counts.get(3,0), review_2_counts.get(4,0), review_2_counts.get(5,0)]\n",
    "review_3_counts = dict(Counter(review_3_stars))\n",
    "review_3_counts = [review_3_counts.get(1, 0), review_3_counts.get(2,0), review_3_counts.get(3,0), review_3_counts.get(4,0), review_3_counts.get(5,0)]\n",
    "review_4_counts = dict(Counter(review_4_stars))\n",
    "review_4_counts = [review_4_counts.get(1, 0), review_4_counts.get(2,0), review_4_counts.get(3,0), review_4_counts.get(4,0), review_4_counts.get(5,0)]\n",
    "review_5_counts = dict(Counter(review_5_stars))\n",
    "review_5_counts = [review_5_counts.get(1, 0), review_5_counts.get(2,0), review_5_counts.get(3,0), review_5_counts.get(4,0), review_5_counts.get(5,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "barWidth = 0.2\n",
    " \n",
    "bars1 = review_1_counts\n",
    "bars2 = review_2_counts\n",
    "bars3 = review_3_counts\n",
    "bars4 = review_4_counts\n",
    "bars5 = review_5_counts\n",
    " \n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "r5 = [x + barWidth for x in r4]\n",
    "\n",
    "plt.bar(r1, bars1, color='#101357', width=barWidth, edgecolor='white', label='review_1')\n",
    "plt.bar(r2, bars2, color='#fea49f', width=barWidth, edgecolor='white', label='review_2')\n",
    "plt.bar(r3, bars3, color='#fbaf08', width=barWidth, edgecolor='white', label='review_3')\n",
    "plt.bar(r4, bars4, color='#00a0a0', width=barWidth, edgecolor='white', label='review_4')\n",
    "plt.bar(r5, bars5, color='#007f4f', width=barWidth, edgecolor='white', label='review_5')\n",
    "\n",
    "plt.xlabel('Star', fontsize=22)\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], ['1', '2', '3', '4', '5'], fontsize=20)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,max(max(bars1), max(bars2), max(bars3), max(bars4), max(bars5))])\n",
    "plt.title('Star Count per Review', fontweight='bold', fontsize=25)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how your guesses measured up to the actual star ratings given to these reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The five reviews for which we solicited guesses from you all:\\n')\n",
    "for i in range(premade_reviews.shape[0]):\n",
    "    print('Review {}:\\n{}\\n\\nActual Star Rating:{}\\n\\n'.format(i+1, premade_reviews.loc[i, 'reviews'],premade_reviews_actual_ratings[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's better visualize your accuracy in guessing reviews' star ratings with pie charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pie(review_counts):\n",
    "    def get_labels():\n",
    "        non_zero_labels = []\n",
    "        for i in range(len(review_counts)):\n",
    "            if review_counts[i] != 0:\n",
    "                non_zero_labels.append(i+1)\n",
    "        return non_zero_labels\n",
    "\n",
    "    labels = get_labels()\n",
    "    sizes = [review_counts[i-1] for i in labels]\n",
    "    colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue', 'pink']\n",
    "\n",
    "    plt.pie(sizes, labels=labels, colors=colors,  shadow=True, startangle=140, autopct='%1.1f%%')\n",
    "    \n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, counts in enumerate([review_1_counts, review_2_counts, review_3_counts, review_4_counts, review_5_counts]):\n",
    "    print('Review {}'.format(i+1))\n",
    "    print('Actual star rating: {}'.format(premade_reviews_actual_ratings[i]))\n",
    "    make_pie(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at some of the reviews you guys submitted in our circulated survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of submitted reviews: {}'.format(submitted_reviews.shape[0]))\n",
    "print('Submitted reviews dataframe:')\n",
    "submitted_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    print('{}\\n\\nStar Rating: {}\\n\\n'.format(submitted_reviews.loc[i, 'reviews'], submitted_reviews.loc[i, 'stars']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the distribution of the star ratings you assigned to the reviews you wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted_reviews_counts = dict(Counter(submitted_reviews.stars.tolist()))\n",
    "submitted_reviews_counts = [submitted_reviews_counts.get(1,0), submitted_reviews_counts.get(2,0), submitted_reviews_counts.get(3,0), submitted_reviews_counts.get(4,0), submitted_reviews_counts.get(5,0)]\n",
    "\n",
    "bins = np.arange(7) - 0.5\n",
    "plt.hist(submitted_reviews.stars, bins, edgecolor='black')\n",
    "plt.xticks(range(6))\n",
    "plt.xlim([0, 6])\n",
    "plt.yticks(range(max(submitted_reviews_counts)+1))\n",
    "plt.ylim([0, max(submitted_reviews_counts)+1])\n",
    "plt.title('Star Counts for Submitted Reviews', fontsize=15, fontweight='bold')\n",
    "plt.xlabel('Star', fontsize=13)\n",
    "plt.ylabel('Count', fontsize=13)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean reviews for modeling ##\n",
    "\n",
    "Before we can compare your results to our model's results, we need to clean the reviews and vectorize them for input into the model.\n",
    "\n",
    "Cleaning/preprocessing steps:\n",
    "\n",
    "- Lowercase<br>\n",
    "- Remove non-ASCII characters<br>\n",
    "- Tokenize on pattern \\w+\\'?\\w+ (maintains internal apostrophes)<br>\n",
    "- Replace non-alphabetic (e.g., digits, %, *, &, $) with ''<br>\n",
    "- Remove stopwords (based on customized list that retains words with negative connotation (e.g., not, won't, no))<br>\n",
    "- Lemmatize (canonical/dictionary form of a word... babies --> baby)<br>\n",
    "- Remove rare tokens (30 threshold, tokens that appear less than or equal to 30 times will be removed)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_stops = ['no',\n",
    "    'nor',\n",
    "    'not',\n",
    "    'don',\n",
    "    \"don't\",\n",
    "    'ain',\n",
    "    'aren',\n",
    "    \"aren't\",\n",
    "    'couldn',\n",
    "    \"couldn't\",\n",
    "    'didn',\n",
    "    \"didn't\",\n",
    "    'doesn',\n",
    "    \"doesn't\",\n",
    "    'hadn',\n",
    "    \"hadn't\",\n",
    "    'hasn',\n",
    "    \"hasn't\",\n",
    "    'haven',\n",
    "    \"haven't\",\n",
    "    'isn',\n",
    "    \"isn't\",\n",
    "    'mightn',\n",
    "    \"mightn't\",\n",
    "    'mustn',\n",
    "    \"mustn't\",\n",
    "    'needn',\n",
    "    \"needn't\",\n",
    "    'shan',\n",
    "    \"shan't\",\n",
    "    'shouldn',\n",
    "    \"shouldn't\",\n",
    "    'wasn',\n",
    "    \"wasn't\",\n",
    "    'weren',\n",
    "    \"weren't\",\n",
    "    \"won'\",\n",
    "    \"won't\",\n",
    "    'wouldn',\n",
    "    \"wouldn't\",\n",
    "    'but',\n",
    "    \"don'\",\n",
    "    \"ain't\"]\n",
    "\n",
    "common_nonneg_contr = [\"could've\",\n",
    "    \"he'd\",\n",
    "    \"he'd've\",\n",
    "    \"he'll\",\n",
    "    \"he's\",\n",
    "    \"how'd\",\n",
    "    \"how'll\",\n",
    "    \"how's\",\n",
    "    \"i'd\",\n",
    "    \"i'd've\",\n",
    "    \"i'll\",\n",
    "    \"i'm\",\n",
    "    \"i've\",\n",
    "    \"it'd\",\n",
    "    \"it'd've\",\n",
    "    \"it'll\",\n",
    "    \"it's\",\n",
    "    \"let's\",\n",
    "    \"ma'am\",\n",
    "    \"might've\",\n",
    "    \"must've\",\n",
    "    \"o'clock\",\n",
    "    \"'ow's'at\",\n",
    "    \"she'd\",\n",
    "    \"she'd've\",\n",
    "    \"she'll\",\n",
    "    \"she's\",\n",
    "    \"should've\",\n",
    "    \"somebody'd\",\n",
    "    \"somebody'd've\",\n",
    "    \"somebody'll\",\n",
    "    \"somebody's\",\n",
    "    \"someone'd\",\n",
    "    \"someone'd've\",\n",
    "    \"someone'll\",\n",
    "    \"someone's\",\n",
    "    \"something'd\",\n",
    "    \"something'd've\",\n",
    "    \"something'll\",\n",
    "    \"something's\",\n",
    "    \"that'll\",\n",
    "    \"that's\",\n",
    "    \"there'd\",\n",
    "    \"there'd've\",\n",
    "    \"there're\",\n",
    "    \"there's\",\n",
    "    \"they'd\",\n",
    "    \"they'd've\",\n",
    "    \"they'll\",\n",
    "    \"they're\",\n",
    "    \"they've\",\n",
    "    \"'twas\",\n",
    "    \"we'd\",\n",
    "    \"we'd've\",\n",
    "    \"we'll\",\n",
    "    \"we're\",\n",
    "    \"we've\",\n",
    "    \"what'll\",\n",
    "    \"what're\",\n",
    "    \"what's\",\n",
    "    \"what've\",\n",
    "    \"when's\",\n",
    "    \"where'd\",\n",
    "    \"where's\",\n",
    "    \"where've\",\n",
    "    \"who'd\",\n",
    "    \"who'd've\",\n",
    "    \"who'll\",\n",
    "    \"who're\",\n",
    "    \"who's\",\n",
    "    \"who've\",\n",
    "    \"why'll\",\n",
    "    \"why're\",\n",
    "    \"why's\",\n",
    "    \"would've\",\n",
    "    \"y'all\",\n",
    "    \"y'all'll\",\n",
    "    \"y'all'd've\",\n",
    "    \"you'd\",\n",
    "    \"you'd've\",\n",
    "    \"you'll\",\n",
    "    \"you're\",\n",
    "    \"you've\"]\n",
    "\n",
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
    "  'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "ranks = ['st', 'nd', 'rd', 'th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stopword_list(nltk_english = True, contractions = True, single_letters = True, rank_suffixes = True, remove_negs = True):\n",
    "\n",
    "    # Figure out if the stopwords corpus is present\n",
    "    try:\n",
    "        dir(nltk.corpus.stopwords)\n",
    "    except AttributeError:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "    # Assemble all the stopwords into a list\n",
    "    stops = []\n",
    "    if nltk_english:\n",
    "        stops += nltk.corpus.stopwords.words('english')\n",
    "    if contractions:\n",
    "        stops += common_nonneg_contr\n",
    "    if single_letters:\n",
    "        stops += letters\n",
    "    if rank_suffixes:\n",
    "        stops += ranks\n",
    "    stops += [\"\"] + ['us'] + [''] + [\"'\"]\n",
    "\n",
    "    # Remove all negative stopwords and any duplicates\n",
    "    if remove_negs:\n",
    "        stops = list(set(stops) - set(neg_stops))\n",
    "\n",
    "    return stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_review(text):\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "        tag_dict = {\"a\": wordnet.ADJ,\n",
    "                    \"n\": wordnet.NOUN,\n",
    "                    \"v\": wordnet.VERB,\n",
    "                    \"r\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    def _clean_review(text):\n",
    "        text = text.lower()\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "        tokenizer = nltk.RegexpTokenizer('\\w+\\'?\\w+')\n",
    "        filtered_tokens = [(re.sub(r\"[^A-Za-z\\s']\", '', token)) for token in tokenizer.tokenize(text)]\n",
    "        stops = create_stopword_list()\n",
    "        tokens = [token for token in filtered_tokens if token not in stops]\n",
    "        tokens = [re.sub(\"'s\", '', token) for token in tokens if re.sub(\"'s\", '', token) != '']\n",
    "        for i, token in enumerate(tokens):\n",
    "            tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "        tokens = [token for token in tokens if token not in stops]\n",
    "        return tokens\n",
    "    submitted_reviews['review_tokens'] = submitted_reviews['reviews'].apply(lambda x: _clean_review(x))\n",
    "    return submitted_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now clean/preprocess the reviews you submitted in our survey..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submitted_reviews = _process_review(submitted_reviews['reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some examples of what the reviews look like in tokenized form after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):  \n",
    "    print('Full review:\\n{}'.format(submitted_reviews.loc[i, 'reviews']))\n",
    "    print('\\nTokenized review: \\n{}\\n\\n'.format(submitted_reviews.loc[i, 'review_tokens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our reviews in tokenized form, let's remove rare tokens using our previously pickled list of rare tokens based on our corpus of over 6.6 million Yelp reviews. Our list of rare tokens included 583595 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_tokens_30 = pickle.load(open('../data/rare_tokens_threshold30 copy.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_rare_tokens(df):\n",
    "    def _filter_rare_tokens(tokens):   \n",
    "        tokens_to_remove = list((set(tokens) & set(rare_tokens_30)))\n",
    "        frequent_tokens = [token for token in tokens if token not in tokens_to_remove]\n",
    "        return frequent_tokens\n",
    "    submitted_reviews['review_tokens'] = submitted_reviews['review_tokens'].apply(lambda x: _filter_rare_tokens(x))\n",
    "    return submitted_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted_reviews = _remove_rare_tokens(submitted_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fully preprocessed the reviews you submitted, let's do the same for the five reviews for which we had you guess star ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premade_reviews = _process_review(premade_reviews['reviews'])\n",
    "premade_reviews = _remove_rare_tokens(premade_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model to predict ratings\n",
    "\n",
    "Below we will load in our pickled TF-IDF vectorizer fit to our data and our pickled stochastic gradient descent classifier (SGDC) model. We will first vectorize the reviews and then feed the them (both preexisting and submitted by you) into the model to compare model predictions with human guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfer = pickle.load(open('../data/pickled_TfidfVectorizer.pkl', 'rb'))\n",
    "SGDC = pickle.load(open('../data/SGDClog.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premade_reviews['review_tokens'] = premade_reviews['review_tokens'].apply(' '.join)\n",
    "submitted_reviews['review_tokens'] = submitted_reviews['review_tokens'].apply(' '.join)\n",
    "tfidf_premade_reviews = tfidfer.transform(premade_reviews['review_tokens'])\n",
    "tfidf_submitted_reviews = tfidfer.transform(submitted_reviews['review_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDC.predict(tfidf_submitted_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDC.score(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
