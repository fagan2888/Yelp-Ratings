{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = ''\n",
    "username = ''\n",
    "host = ''\n",
    "password = ''\n",
    "\n",
    "conn = psycopg2.connect('dbname={} user={} host={} password={}'.format(dbname, username, host, password))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "    SELECT business.business_id, name, categories, review_text FROM business\n",
    "    JOIN review ON business.business_id = review.business_id WHERE business.restaurant_dummy = 1\n",
    "\"\"\")\n",
    "\n",
    "cols = ['business_id', 'name', 'categories', 'review_text']\n",
    "\n",
    "restaurant_sample = pd.DataFrame(cur.fetchall(), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "restaurant_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_review(text):\n",
    "    def _create_stop_words():\n",
    "        stops = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "        neg_stops = ['no',\n",
    "         'nor',\n",
    "         'not',\n",
    "         'don',\n",
    "         \"don't\",\n",
    "         'ain',\n",
    "         'aren',\n",
    "         \"aren't\",\n",
    "         'couldn',\n",
    "         \"couldn't\",\n",
    "         'didn',\n",
    "         \"didn't\",\n",
    "         'doesn',\n",
    "         \"doesn't\",\n",
    "         'hadn',\n",
    "         \"hadn't\",\n",
    "         'hasn',\n",
    "         \"hasn't\",\n",
    "         'haven',\n",
    "         \"haven't\",\n",
    "         'isn',\n",
    "         \"isn't\",\n",
    "         'mightn',\n",
    "         \"mightn't\",\n",
    "         'mustn',\n",
    "         \"mustn't\",\n",
    "         'needn',\n",
    "         \"needn't\",\n",
    "         'shan',\n",
    "         \"shan't\",\n",
    "         'shouldn',\n",
    "         \"shouldn't\",\n",
    "         'wasn',\n",
    "         \"wasn't\",\n",
    "         'weren',\n",
    "         \"weren't\",\n",
    "         \"won'\",\n",
    "         \"won't\",\n",
    "         'wouldn',\n",
    "         \"wouldn't\",\n",
    "         'but',\n",
    "         \"don'\",\n",
    "         \"ain't\"]\n",
    "\n",
    "        common_nonneg_contr = [\"could've\",\n",
    "        \"he'd\",\n",
    "        \"he'd've\",\n",
    "        \"he'll\",\n",
    "        \"he's\",\n",
    "        \"how'd\",\n",
    "        \"how'll\",\n",
    "        \"how's\",\n",
    "        \"i'd\",\n",
    "        \"i'd've\",\n",
    "        \"i'll\",\n",
    "        \"i'm\",\n",
    "        \"i've\",\n",
    "        \"it'd\",\n",
    "        \"it'd've\",\n",
    "        \"it'll\",\n",
    "        \"it's\",\n",
    "        \"let's\",\n",
    "        \"ma'am\",\n",
    "        \"might've\",\n",
    "        \"must've\",\n",
    "        \"o'clock\",\n",
    "        \"'ow's'at\",\n",
    "        \"she'd\",\n",
    "        \"she'd've\",\n",
    "        \"she'll\",\n",
    "        \"she's\",\n",
    "        \"should've\",\n",
    "        \"somebody'd\",\n",
    "        \"somebody'd've\",\n",
    "        \"somebody'll\",\n",
    "        \"somebody's\",\n",
    "        \"someone'd\",\n",
    "        \"someone'd've\",\n",
    "        \"someone'll\",\n",
    "        \"someone's\",\n",
    "        \"something'd\",\n",
    "        \"something'd've\",\n",
    "        \"something'll\",\n",
    "        \"something's\",\n",
    "        \"that'll\",\n",
    "        \"that's\",\n",
    "        \"there'd\",\n",
    "        \"there'd've\",\n",
    "        \"there're\",\n",
    "        \"there's\",\n",
    "        \"they'd\",\n",
    "        \"they'd've\",\n",
    "        \"they'll\",\n",
    "        \"they're\",\n",
    "        \"they've\",\n",
    "        \"'twas\",\n",
    "        \"we'd\",\n",
    "        \"we'd've\",\n",
    "        \"we'll\",\n",
    "        \"we're\",\n",
    "        \"we've\",\n",
    "        \"what'll\",\n",
    "        \"what're\",\n",
    "        \"what's\",\n",
    "        \"what've\",\n",
    "        \"when's\",\n",
    "        \"where'd\",\n",
    "        \"where's\",\n",
    "        \"where've\",\n",
    "        \"who'd\",\n",
    "        \"who'd've\",\n",
    "        \"who'll\",\n",
    "        \"who're\",\n",
    "        \"who's\",\n",
    "        \"who've\",\n",
    "        \"why'll\",\n",
    "        \"why're\",\n",
    "        \"why's\",\n",
    "        \"would've\",\n",
    "        \"y'all\",\n",
    "        \"y'all'll\",\n",
    "        \"y'all'd've\",\n",
    "        \"you'd\",\n",
    "        \"you'd've\",\n",
    "        \"you'll\",\n",
    "        \"you're\",\n",
    "        \"you've\"]\n",
    "\n",
    "        letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
    "          'u', 'v', 'w', 'x', 'y', 'z']\n",
    "        \n",
    "        ranks = ['st', 'nd', 'rd', 'th']\n",
    "        \n",
    "        for x in neg_stops:\n",
    "            if x in stops:\n",
    "                stops.remove(x)\n",
    "\n",
    "        new_stops = stops + common_nonneg_contr + letters + ranks + [\"\"] + ['us'] + [''] + [\"'\"]\n",
    "        stops = list(set(new_stops))\n",
    "        return stops\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "        tag_dict = {\"a\": wordnet.ADJ,\n",
    "                    \"n\": wordnet.NOUN,\n",
    "                    \"v\": wordnet.VERB,\n",
    "                    \"r\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def _clean_review(text):\n",
    "        text = text.lower()\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "        tokenizer = nltk.RegexpTokenizer('\\w+\\'?\\w+')\n",
    "        filtered_tokens = [(re.sub(r\"[^A-Za-z\\s']\", '', token)) for token in tokenizer.tokenize(text)]\n",
    "        stops = _create_stop_words()\n",
    "        tokens = [token for token in filtered_tokens if token not in stops]\n",
    "        tokens = [re.sub(\"'s\", '', token) for token in tokens if re.sub(\"'s\", '', token) != '']\n",
    "        for i, token in enumerate(tokens):\n",
    "            tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "        tokens = [token for token in tokens if token not in stops]\n",
    "        return tokens\n",
    "    \n",
    "    return _clean_review(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_on_column(data):\n",
    "    data['restaurant_review_tokens'] = data['review_text'].apply(lambda x: _process_review(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_sample = restaurant_sample.sample(frac = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "apply_on_column(restaurant_sample)\n",
    "end = time.time()\n",
    "dur = end - start\n",
    "# Verify that the function is working\n",
    "print('Processed {} instances in {} minutes {} seconds.\\n'.format(sample.shape[0], dur//60, dur%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_long_str(tokens_col):\n",
    "    long_list = [token for review in tokens_col.values for token in review]\n",
    "    long_string = ' '.join(long_list)\n",
    "    return long_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(_make_long_str(restaurant_sample['restaurant_review_tokens']))\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()\n",
    "wordcloud.to_file('./restaurants_sample_wordcloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "    SELECT business.business_id, name, categories, review_text FROM business\n",
    "    JOIN review ON business.business_id = review.business_id WHERE business.health_dummy = 1\n",
    "\"\"\")\n",
    "\n",
    "cols = ['business_id', 'name', 'categories', 'review_text']\n",
    "\n",
    "health_sample = pd.DataFrame(cur.fetchall(), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_on_column(data):\n",
    "    data['health_review_tokens'] = data['review_text'].apply(lambda x: _process_review(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "apply_on_column(health_sample)\n",
    "end = time.time()\n",
    "dur = end - start\n",
    "# Verify that the function is working\n",
    "print('Processed {} instances in {} minutes {} seconds.\\n'.format(health_sample.shape[0], dur//60, dur%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(_make_long_str(health_sample['health_review_tokens']))\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()\n",
    "wordcloud.to_file('./health_sample_wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See what tokens correlate with different star ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(text):\n",
    "    return text\n",
    "\n",
    "\n",
    "#This second TF-IDF function takes our already tokenized reviews, so the column review_sample['review_tokens']\n",
    "#This essentially means that we need to run our custom preprocessor _process_review on our review text in raw form\n",
    "tfidf2 = TfidfVectorizer(\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "sublinear_tf=True, min_df=5, ngram_range=(1,2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "    SELECT * FROM review LIMIT 1000\n",
    "\"\"\")\n",
    "\n",
    "cols = ['review_id', 'user_id', 'business_id', 'stars', 'review_date', 'review_text', 'useful', 'funny', 'cool']\n",
    "\n",
    "review_sample = pd.DataFrame(cur.fetchall(), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to apply _process_review function on all review_text column and put tokens in new column titled 'review_tokens'\n",
    "def apply_on_column(data):\n",
    "    data['review_tokens'] = data['review_text'].apply(lambda x: _process_review(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get times for how long it takes to run apply_on_column function on review sample\n",
    "start = time.time()\n",
    "apply_on_column(review_sample)\n",
    "end = time.time()\n",
    "dur = end - start\n",
    "# Verify that the function is working\n",
    "print('Processed {} instances in {} minutes {} seconds.\\n'.format(review_sample.shape[0], dur//60, dur%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stars_df = review_sample[['stars', 'review_tokens']].sort_values('stars')\n",
    "tokens_stars = dict(tokens_stars_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf2.fit_transform(review_sample.review_tokens).toarray()\n",
    "labels = review_sample.stars\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "for star, tokens in sorted(tokens_stars.items()):\n",
    "    features_chi2 = chi2(features, labels == star)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf2.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"'{}' Star:\".format(star))\n",
    "    print(\"Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "    print(\"Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
