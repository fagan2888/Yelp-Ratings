{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorry for this crazy import cell... this will be cleaned up\n",
    "\n",
    "import psycopg2\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input your PostGres credentials to connect\n",
    "\n",
    "dbname = ''\n",
    "username = ''\n",
    "host = ''\n",
    "password = ''\n",
    "\n",
    "conn = psycopg2.connect('dbname={} user={} host={} password={}'.format(dbname, username, host, password))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    SELECT * FROM review LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "cols = ['review_id', 'user_id', 'business_id', 'stars', 'review_date', 'review_text', 'useful', 'funny', 'cool']\n",
    "\n",
    "review_sample = pd.DataFrame(cur.fetchall(), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review_sample.loc[24, 'review_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contractions:\n",
    "https://gist.github.com/J3RN/ed7b420a6ea1d5bd6d06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_stop_words():\n",
    "\n",
    "\tstops = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "\tneg_stops = ['no',\n",
    " \t'nor',\n",
    " \t'not',\n",
    " \t'don',\n",
    " \t\"don't\",\n",
    " \t'ain',\n",
    " \t'aren',\n",
    " \t\"aren't\",\n",
    " \t'couldn',\n",
    " \t\"couldn't\",\n",
    " \t'didn',\n",
    " \t\"didn't\",\n",
    " \t'doesn',\n",
    " \t\"doesn't\",\n",
    " \t'hadn',\n",
    " \t\"hadn't\",\n",
    " \t'hasn',\n",
    " \t\"hasn't\",\n",
    " \t'haven',\n",
    " \t\"haven't\",\n",
    " \t'isn',\n",
    " \t\"isn't\",\n",
    " \t'mightn',\n",
    " \t\"mightn't\",\n",
    " \t'mustn',\n",
    " \t\"mustn't\",\n",
    " \t'needn',\n",
    " \t\"needn't\",\n",
    " \t'shan',\n",
    " \t\"shan't\",\n",
    " \t'shouldn',\n",
    " \t\"shouldn't\",\n",
    " \t'wasn',\n",
    " \t\"wasn't\",\n",
    " \t'weren',\n",
    " \t\"weren't\",\n",
    " \t\"won'\",\n",
    " \t\"won't\",\n",
    " \t'wouldn',\n",
    " \t\"wouldn't\",\n",
    " \t'but',\n",
    " \t\"don'\",\n",
    " \t\"ain't\"]\n",
    "\n",
    "\tcommon_nonneg_contr = [\"could've\",\n",
    "\t\"he'd\",\n",
    "\t\"he'd've\",\n",
    "\t\"he'll\",\n",
    "\t\"he's\",\n",
    "\t\"how'd\",\n",
    "\t\"how'll\",\n",
    "\t\"how's\",\n",
    "\t\"i'd\",\n",
    "\t\"i'd've\",\n",
    "\t\"i'll\",\n",
    "\t\"i'm\",\n",
    "\t\"i've\",\n",
    "\t\"it'd\",\n",
    "\t\"it'd've\",\n",
    "\t\"it'll\",\n",
    "\t\"it's\",\n",
    "\t\"let's\",\n",
    "\t\"ma'am\",\n",
    "\t\"might've\",\n",
    "\t\"must've\",\n",
    "\t\"o'clock\",\n",
    "\t\"'ow's'at\",\n",
    "\t\"she'd\",\n",
    "\t\"she'd've\",\n",
    "\t\"she'll\",\n",
    "\t\"she's\",\n",
    "\t\"should've\",\n",
    "\t\"somebody'd\",\n",
    "\t\"somebody'd've\",\n",
    "\t\"somebody'll\",\n",
    "\t\"somebody's\",\n",
    "\t\"someone'd\",\n",
    "\t\"someone'd've\",\n",
    "\t\"someone'll\",\n",
    "\t\"someone's\",\n",
    "\t\"something'd\",\n",
    "\t\"something'd've\",\n",
    "\t\"something'll\",\n",
    "\t\"something's\",\n",
    "\t\"that'll\",\n",
    "\t\"that's\",\n",
    "\t\"there'd\",\n",
    "\t\"there'd've\",\n",
    "\t\"there're\",\n",
    "\t\"there's\",\n",
    "\t\"they'd\",\n",
    "\t\"they'd've\",\n",
    "\t\"they'll\",\n",
    "\t\"they're\",\n",
    "\t\"they've\",\n",
    "\t\"'twas\",\n",
    "\t\"we'd\",\n",
    "\t\"we'd've\",\n",
    "\t\"we'll\",\n",
    "\t\"we're\",\n",
    "\t\"we've\",\n",
    "\t\"what'll\",\n",
    "\t\"what're\",\n",
    "\t\"what's\",\n",
    "\t\"what've\",\n",
    "\t\"when's\",\n",
    "\t\"where'd\",\n",
    "\t\"where's\",\n",
    "\t\"where've\",\n",
    "\t\"who'd\",\n",
    "\t\"who'd've\",\n",
    "\t\"who'll\",\n",
    "\t\"who're\",\n",
    "\t\"who's\",\n",
    "\t\"who've\",\n",
    "\t\"why'll\",\n",
    "\t\"why're\",\n",
    "\t\"why's\",\n",
    "\t\"would've\",\n",
    "\t\"y'all\",\n",
    "\t\"y'all'll\",\n",
    "\t\"y'all'd've\",\n",
    "\t\"you'd\",\n",
    "\t\"you'd've\",\n",
    "\t\"you'll\",\n",
    "\t\"you're\",\n",
    "\t\"you've\"]\n",
    "\n",
    "\tfor x in neg_stops:\n",
    "\t\tif x in stops:\n",
    "\t\t\tstops.remove(x)\n",
    "        \n",
    "\tnew_stops = stops + common_nonneg_contr + [\"\"] + ['us']\n",
    "\tstops = list(set(new_stops))\n",
    "\treturn stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_create_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning this into text, as it's the less efficient code\n",
    "\"\"\"\n",
    "\n",
    "def _remove_stops(tokens):\n",
    "    stops = nltk.corpus.stopwords.words('english')\n",
    "    neg_stops = [\n",
    "    'no', 'not', 'nor', 'don\\'', 'don\\'t', 'ain', \n",
    "    'ain\\'t', 'aren\\'t', 'aren', 'couldn', 'couldn\\'t', \n",
    "    'didn', 'didn\\'t', 'doesn', 'doesn\\'t', 'hadn', \n",
    "    'hadn\\'t', 'hasn', 'hasn\\'t', 'haven', 'haven\\'t',\n",
    "    'isn', 'isn\\'t', 'mightn', 'mightn\\'t', 'mustn', \n",
    "    'mustn\\'t', 'needn', 'needn\\'t', 'shan', 'shan\\'t',\n",
    "    'shouldn', 'shouldn\\'t', 'wasn', 'wasn\\'t', 'weren',\n",
    "    'weren\\'t', 'won', 'won\\'t', 'wouldn', 'wouldn\\'t'\n",
    "    ]\n",
    "#still leaves in but and don.. fix this.. \n",
    "#doesn't get rid of other obvious stopwords, like i'm, they're....\n",
    "    for x in neg_stops:\n",
    "        if x in stops:\n",
    "            stops.remove(x)\n",
    "            \n",
    "    tokens_without_stops = [token for token in tokens if token not in stops]\n",
    "    return tokens_without_stops\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "#I think I adjusted this properly to remove digits\n",
    "#however for phrases that are hyphenated, like chilled-to-the-bone, the code returns the token 'chilledtothebone.'\n",
    "#This will need to be fixed\n",
    "def _clean_review(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    text = re.sub(r\"[^A-Za-z\\s']\", '', text)  \n",
    "    tokens = text.split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "    return tokens\n",
    "\n",
    "def _process_review(text):\n",
    "    tokens = _remove_stops(_clean_review(text))\n",
    "    return tokens\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = nltk.corpus.stopwords.words('english')\n",
    "neg_stops = [\n",
    "    'no', 'not', 'nor', 'don\\'', 'don\\'t', 'ain', \n",
    "    'ain\\'t', 'aren\\'t', 'aren', 'couldn', 'couldn\\'t', \n",
    "    'didn', 'didn\\'t', 'doesn', 'doesn\\'t', 'hadn', \n",
    "    'hadn\\'t', 'hasn', 'hasn\\'t', 'haven', 'haven\\'t',\n",
    "    'isn', 'isn\\'t', 'mightn', 'mightn\\'t', 'mustn', \n",
    "    'mustn\\'t', 'needn', 'needn\\'t', 'shan', 'shan\\'t',\n",
    "    'shouldn', 'shouldn\\'t', 'wasn', 'wasn\\'t', 'weren',\n",
    "    'weren\\'t', 'won', 'won\\'t', 'wouldn', 'wouldn\\'t'\n",
    "    ]\n",
    "#still leaves in but and don.. fix this.. \n",
    "#doesn't get rid of other obvious stopwords, like i'm, they're....\n",
    "for x in neg_stops:\n",
    "    if x in stops:\n",
    "        stops.remove(x)\n",
    "        \n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def _clean_review2(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    text = re.sub(r\"[^A-Za-z\\s']\", '', text)  \n",
    "    tokens = [token for token in text.split() if token not in new_stops]\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "    return tokens\n",
    "\n",
    "def _process_review2(text):\n",
    "    tokens = _remove_stops(_clean_review2(text))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning this into text, as it is less efficient than 2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "stops = nltk.corpus.stopwords.words('english')\n",
    "neg_stops = [\n",
    "    'no', 'not', 'nor', 'don\\'', 'don\\'t', 'ain', \n",
    "    'ain\\'t', 'aren\\'t', 'aren', 'couldn', 'couldn\\'t', \n",
    "    'didn', 'didn\\'t', 'doesn', 'doesn\\'t', 'hadn', \n",
    "    'hadn\\'t', 'hasn', 'hasn\\'t', 'haven', 'haven\\'t',\n",
    "    'isn', 'isn\\'t', 'mightn', 'mightn\\'t', 'mustn', \n",
    "    'mustn\\'t', 'needn', 'needn\\'t', 'shan', 'shan\\'t',\n",
    "    'shouldn', 'shouldn\\'t', 'wasn', 'wasn\\'t', 'weren',\n",
    "    'weren\\'t', 'won', 'won\\'t', 'wouldn', 'wouldn\\'t'\n",
    "    ]\n",
    "#still leaves in but and don.. fix this.. \n",
    "#doesn't get rid of other obvious stopwords, like i'm, they're....\n",
    "for x in neg_stops:\n",
    "    if x in stops:\n",
    "        stops.remove(x)\n",
    "        \n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def _clean_review3(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    text = re.sub(r\"[^A-Za-z\\s']\", '', text)\n",
    "    tokens = [wnl.lemmatize(token, pos = get_wordnet_pos(token)) for token in text.split() if token not in stops]\n",
    "    return tokens\n",
    "\n",
    "def _process_review3(text):\n",
    "    tokens = _remove_stops(_clean_review2(text))\n",
    "    return tokens\n",
    "    \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is slow\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "stops = nltk.corpus.stopwords.words('english')\n",
    "neg_stops = [\n",
    "    'no', 'not', 'nor', 'don\\'', 'don\\'t', 'ain', \n",
    "    'ain\\'t', 'aren\\'t', 'aren', 'couldn', 'couldn\\'t', \n",
    "    'didn', 'didn\\'t', 'doesn', 'doesn\\'t', 'hadn', \n",
    "    'hadn\\'t', 'hasn', 'hasn\\'t', 'haven', 'haven\\'t',\n",
    "    'isn', 'isn\\'t', 'mightn', 'mightn\\'t', 'mustn', \n",
    "    'mustn\\'t', 'needn', 'needn\\'t', 'shan', 'shan\\'t',\n",
    "    'shouldn', 'shouldn\\'t', 'wasn', 'wasn\\'t', 'weren',\n",
    "    'weren\\'t', 'won', 'won\\'t', 'wouldn', 'wouldn\\'t'\n",
    "    ]\n",
    "#still leaves in but and don.. fix this.. \n",
    "#doesn't get rid of other obvious stopwords, like i'm, they're....\n",
    "for x in neg_stops:\n",
    "    if x in stops:\n",
    "        stops.remove(x)\n",
    "        \n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def _clean_review4(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    text = re.sub(r\"[^A-Za-z\\s']\", '', text)  \n",
    "    tokens = [wnl.lemmatize(token, pos= get_wordnet_pos(token)) for token in text.split()]\n",
    "    tokens = [token for token in tokens if token not in stops]\n",
    "    return tokens\n",
    "\n",
    "def _process_review4(text):\n",
    "    tokens = _remove_stops(_clean_review2(text))\n",
    "    return tokens\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "stops = nltk.corpus.stopwords.words('english')\n",
    "neg_stops = [\n",
    "    'no', 'not', 'nor', 'don\\'', 'don\\'t', 'ain', \n",
    "    'ain\\'t', 'aren\\'t', 'aren', 'couldn', 'couldn\\'t', \n",
    "    'didn', 'didn\\'t', 'doesn', 'doesn\\'t', 'hadn', \n",
    "    'hadn\\'t', 'hasn', 'hasn\\'t', 'haven', 'haven\\'t',\n",
    "    'isn', 'isn\\'t', 'mightn', 'mightn\\'t', 'mustn', \n",
    "    'mustn\\'t', 'needn', 'needn\\'t', 'shan', 'shan\\'t',\n",
    "    'shouldn', 'shouldn\\'t', 'wasn', 'wasn\\'t', 'weren',\n",
    "    'weren\\'t', 'won', 'won\\'t', 'wouldn', 'wouldn\\'t'\n",
    "    ]\n",
    "#still leaves in but and don.. fix this.. \n",
    "#doesn't get rid of other obvious stopwords, like i'm, they're....\n",
    "for x in neg_stops:\n",
    "    if x in stops:\n",
    "        stops.remove(x)\n",
    "        \n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def _clean_review5(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    text = re.sub(r\"[^A-Za-z\\s']\", '', text)\n",
    "    tokens = text.split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "    tokens = [token for token in tokens if token not in stops]\n",
    "    return tokens\n",
    "\n",
    "def _process_review5(text):\n",
    "    tokens = _remove_stops(_clean_review2(text))\n",
    "    return tokens\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_clean_review2(review_sample.loc[24, 'review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_on_column(data):\n",
    "    data['review_text'] = data['review_text'].apply(lambda x: _clean_review9(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_on_column(data):\n",
    "    data['review_text'] = data['review_text'].map(lambda x: _clean_review2(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about 8 min 8 sec for 50000 (_clean_review2)\n",
    "# 15 min 24.965 seconds for 100000\n",
    "start = time.time()\n",
    "apply_on_column(review_sample)\n",
    "end = time.time()\n",
    "dur = end - start\n",
    "# Verify that the function is working\n",
    "print('Processed {} instances in {} minutes {} seconds.\\n'.format(review_sample.shape[0], dur//60, dur%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample.loc[27, 'review_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterrows_at(data):\n",
    "    for i, row in data.iterrows():\n",
    "        data.at[i, 'review_text'] = _clean_review2(data.at[i, 'review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a little longer.. like 7 min 56 seconds for 50000 reviews (_clean_review2)\n",
    "start = time.time()\n",
    "iterrows_at(review_sample)\n",
    "end = time.time()\n",
    "dur = end - start\n",
    "# Verify that the function is working\n",
    "print('Processed {} instances in {} minutes {} seconds.\\n'.format(review_sample.shape[0], dur//60, dur%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 min 31.66 seconds\n",
    "#15 min 50.93 seconds for 100000\n",
    "start = time.time()\n",
    "map_on_column(review_sample)\n",
    "end = time.time()\n",
    "dur = end - start\n",
    "# Verify that the function is working\n",
    "print('Processed {} instances in {} minutes {} seconds.\\n'.format(review_sample.shape[0], dur//60, dur%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def _clean_review6(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    text = re.sub(r\"[^A-Za-z\\s'\\\\\\/\\-]\", '', text)\n",
    "    tokenizer = nltk.RegexpTokenizer('\\w+\\'?\\w*')\n",
    "    tokens = [token for token in tokenizer.tokenize(text) if token not in new_stops]\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "    return tokens\n",
    "\n",
    "def _process_review6(text):\n",
    "    tokens = _remove_stops(_clean_review2(text))\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def _filter_characters(text):\n",
    "    text = re.sub(r\"[^A-Za-z\\s'\\\\\\/\\-]\", '', text)\n",
    "    text = re.sub(\"' \", ' ', text)\n",
    "    text = re.sub(\"'s\", '', text)\n",
    "    text = re.sub(\"'\", '', text)\n",
    "    return text\n",
    "\n",
    "def _clean_review7(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    text = re.sub(r\"[^A-Za-z\\s'\\\\\\/\\-]\", '', text)\n",
    "    text = re.sub(\"' \", ' ', text)\n",
    "    tokenizer = nltk.RegexpTokenizer('\\w+\\'?\\w*')\n",
    "    tokens = [token for token in tokenizer.tokenize(text) if token not in new_stops]\n",
    "    tokens = [token for token in tokens]\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "        tokens[i] = re.sub(\"'s\", '', token)\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def _filter_characters(text):\n",
    "    text = re.sub(r\"[^A-Za-z\\s'\\\\\\/\\-]\", '', text)\n",
    "    text = re.sub(\"' \", ' ', text)\n",
    "    text = re.sub(\"'s\", '', text)\n",
    "    text = re.sub(\"'\", '', text)\n",
    "    return text\n",
    "\n",
    "#For instance like the....cat it's spitting out token 'thecat'\n",
    "def _clean_review8(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    text = re.sub(r\"[^A-Za-z\\s'\\\\\\/\\-]\", '', text)    \n",
    "    tokenizer = nltk.RegexpTokenizer('\\w+\\'?\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token for token in tokenizer.tokenize(text) if token not in new_stops]\n",
    "    for i, token in enumerate(tokens):\n",
    "        filtered_token = token.replace(\"'s\", '')\n",
    "        tokens[i] = wnl.lemmatize(filtered_token, pos= get_wordnet_pos(filtered_token))\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def _clean_review9(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    tokenizer = nltk.RegexpTokenizer('\\w+\\'?\\w+')\n",
    "    filtered_tokens = [(re.sub(r\"[^A-Za-z\\s']\", '', token)) for token in tokenizer.tokenize(text)]\n",
    "    stops = _create_stop_words()\n",
    "    tokens = [token for token in filtered_tokens if token not in stops]\n",
    "    for i, token in enumerate(tokens):\n",
    "        filtered_token = re.sub(\"'s\", '', token)\n",
    "        tokens[i] = wnl.lemmatize(filtered_token, pos= get_wordnet_pos(token))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_on_column(review_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tokens in enumerate(review_sample['review_text']):\n",
    "    print(\"\\n{}\\n\".format(i))\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
