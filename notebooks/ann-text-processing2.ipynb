{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input your PostGres credentials to connect\n",
    "\n",
    "dbname = ''\n",
    "username = ''\n",
    "host = ''\n",
    "password = ''\n",
    "\n",
    "conn = psycopg2.connect('dbname={} user={} host={} password={}'.format(dbname, username, host, password))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust the sample size by changing the number of instances you request following LIMIT\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    SELECT * FROM review LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "cols = ['review_id', 'user_id', 'business_id', 'stars', 'review_date', 'review_text', 'useful', 'funny', 'cool']\n",
    "\n",
    "review_sample = pd.DataFrame(cur.fetchall(), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure you got the sample\n",
    "review_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View specific instance\n",
    "print(review_sample.loc[24, 'review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create customized stopword list that retains words with negative connotation and removes common,\n",
    "#non-negative contrations\n",
    "def _create_stop_words():\n",
    "\n",
    "    stops = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    neg_stops = ['no',\n",
    "     'nor',\n",
    "     'not',\n",
    "     'don',\n",
    "     \"don't\",\n",
    "     'ain',\n",
    "     'aren',\n",
    "     \"aren't\",\n",
    "     'couldn',\n",
    "     \"couldn't\",\n",
    "     'didn',\n",
    "     \"didn't\",\n",
    "     'doesn',\n",
    "     \"doesn't\",\n",
    "     'hadn',\n",
    "     \"hadn't\",\n",
    "     'hasn',\n",
    "     \"hasn't\",\n",
    "     'haven',\n",
    "     \"haven't\",\n",
    "     'isn',\n",
    "     \"isn't\",\n",
    "     'mightn',\n",
    "     \"mightn't\",\n",
    "     'mustn',\n",
    "     \"mustn't\",\n",
    "     'needn',\n",
    "     \"needn't\",\n",
    "     'shan',\n",
    "     \"shan't\",\n",
    "     'shouldn',\n",
    "     \"shouldn't\",\n",
    "     'wasn',\n",
    "     \"wasn't\",\n",
    "     'weren',\n",
    "     \"weren't\",\n",
    "     \"won'\",\n",
    "     \"won't\",\n",
    "     'wouldn',\n",
    "     \"wouldn't\",\n",
    "     'but',\n",
    "     \"don'\",\n",
    "     \"ain't\"]\n",
    "\n",
    "    common_nonneg_contr = [\"could've\",\n",
    "    \"he'd\",\n",
    "    \"he'd've\",\n",
    "    \"he'll\",\n",
    "    \"he's\",\n",
    "    \"how'd\",\n",
    "    \"how'll\",\n",
    "    \"how's\",\n",
    "    \"i'd\",\n",
    "    \"i'd've\",\n",
    "    \"i'll\",\n",
    "    \"i'm\",\n",
    "    \"i've\",\n",
    "    \"it'd\",\n",
    "    \"it'd've\",\n",
    "    \"it'll\",\n",
    "    \"it's\",\n",
    "    \"let's\",\n",
    "    \"ma'am\",\n",
    "    \"might've\",\n",
    "    \"must've\",\n",
    "    \"o'clock\",\n",
    "    \"'ow's'at\",\n",
    "    \"she'd\",\n",
    "    \"she'd've\",\n",
    "    \"she'll\",\n",
    "    \"she's\",\n",
    "    \"should've\",\n",
    "    \"somebody'd\",\n",
    "    \"somebody'd've\",\n",
    "    \"somebody'll\",\n",
    "    \"somebody's\",\n",
    "    \"someone'd\",\n",
    "    \"someone'd've\",\n",
    "    \"someone'll\",\n",
    "    \"someone's\",\n",
    "    \"something'd\",\n",
    "    \"something'd've\",\n",
    "    \"something'll\",\n",
    "    \"something's\",\n",
    "    \"that'll\",\n",
    "    \"that's\",\n",
    "    \"there'd\",\n",
    "    \"there'd've\",\n",
    "    \"there're\",\n",
    "    \"there's\",\n",
    "    \"they'd\",\n",
    "    \"they'd've\",\n",
    "    \"they'll\",\n",
    "    \"they're\",\n",
    "    \"they've\",\n",
    "    \"'twas\",\n",
    "    \"we'd\",\n",
    "    \"we'd've\",\n",
    "    \"we'll\",\n",
    "    \"we're\",\n",
    "    \"we've\",\n",
    "    \"what'll\",\n",
    "    \"what're\",\n",
    "    \"what's\",\n",
    "    \"what've\",\n",
    "    \"when's\",\n",
    "    \"where'd\",\n",
    "    \"where's\",\n",
    "    \"where've\",\n",
    "    \"who'd\",\n",
    "    \"who'd've\",\n",
    "    \"who'll\",\n",
    "    \"who're\",\n",
    "    \"who's\",\n",
    "    \"who've\",\n",
    "    \"why'll\",\n",
    "    \"why're\",\n",
    "    \"why's\",\n",
    "    \"would've\",\n",
    "    \"y'all\",\n",
    "    \"y'all'll\",\n",
    "    \"y'all'd've\",\n",
    "    \"you'd\",\n",
    "    \"you'd've\",\n",
    "    \"you'll\",\n",
    "    \"you're\",\n",
    "    \"you've\"]\n",
    "\n",
    "    for x in neg_stops:\n",
    "        if x in stops:\n",
    "            stops.remove(x)\n",
    "        \n",
    "    new_stops = stops + common_nonneg_contr + [\"\"] + ['us']\n",
    "    stops = list(set(new_stops))\n",
    "    return stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def _clean_review(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    tokenizer = nltk.RegexpTokenizer('\\w+\\'?\\w+')\n",
    "    filtered_tokens = [(re.sub(r\"[^A-Za-z\\s']\", '', token)) for token in tokenizer.tokenize(text)]\n",
    "    stops = _create_stop_words()\n",
    "    tokens = [token for token in filtered_tokens if token not in stops]\n",
    "    for i, token in enumerate(tokens):\n",
    "        filtered_token = re.sub(\"'s\", '', token)\n",
    "        tokens[i] = wnl.lemmatize(filtered_token, pos= get_wordnet_pos(token))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to apply _clean_review function on all review_text column and put tokens in new column titled 'review_tokens'\n",
    "def apply_on_column(data):\n",
    "    data['review_tokens'] = data['review_text'].apply(lambda x: _clean_review(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get times for how long it takes to run apply_on_column function on review sample\n",
    "start = time.time()\n",
    "apply_on_column(review_sample)\n",
    "end = time.time()\n",
    "dur = end - start\n",
    "# Verify that the function is working\n",
    "print('Processed {} instances in {} minutes {} seconds.\\n'.format(review_sample.shape[0], dur//60, dur%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out example full review and its associated tokens after running _clean_review()\n",
    "print('Full review:\\n\\n{}'.format(review_sample.loc[24, 'review_text']))\n",
    "print('\\n\\n Tokenized review: \\n\\n{}'.format(review_sample.loc[24, 'review_tokens']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
