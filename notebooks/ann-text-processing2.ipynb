{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input your PostGres credentials to connect\n",
    "\n",
    "dbname = ''\n",
    "username = ''\n",
    "host = ''\n",
    "password = ''\n",
    "\n",
    "conn = psycopg2.connect('dbname={} user={} host={} password={}'.format(dbname, username, host, password))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust the sample size by changing the number of instances you request following LIMIT\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    SELECT * FROM review LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "cols = ['review_id', 'user_id', 'business_id', 'stars', 'review_date', 'review_text', 'useful', 'funny', 'cool']\n",
    "\n",
    "review_sample = pd.DataFrame(cur.fetchall(), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure you got the sample\n",
    "review_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View specific instance\n",
    "print(review_sample.loc[9, 'review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Function to create customized stopword list that retains words with negative connotation and removes common, non-negative contrations\n",
    "def _create_stop_words():\n",
    "\n",
    "    stops = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    neg_stops = ['no',\n",
    "     'nor',\n",
    "     'not',\n",
    "     'don',\n",
    "     \"don't\",\n",
    "     'ain',\n",
    "     'aren',\n",
    "     \"aren't\",\n",
    "     'couldn',\n",
    "     \"couldn't\",\n",
    "     'didn',\n",
    "     \"didn't\",\n",
    "     'doesn',\n",
    "     \"doesn't\",\n",
    "     'hadn',\n",
    "     \"hadn't\",\n",
    "     'hasn',\n",
    "     \"hasn't\",\n",
    "     'haven',\n",
    "     \"haven't\",\n",
    "     'isn',\n",
    "     \"isn't\",\n",
    "     'mightn',\n",
    "     \"mightn't\",\n",
    "     'mustn',\n",
    "     \"mustn't\",\n",
    "     'needn',\n",
    "     \"needn't\",\n",
    "     'shan',\n",
    "     \"shan't\",\n",
    "     'shouldn',\n",
    "     \"shouldn't\",\n",
    "     'wasn',\n",
    "     \"wasn't\",\n",
    "     'weren',\n",
    "     \"weren't\",\n",
    "     \"won'\",\n",
    "     \"won't\",\n",
    "     'wouldn',\n",
    "     \"wouldn't\",\n",
    "     'but',\n",
    "     \"don'\",\n",
    "     \"ain't\"]\n",
    "\n",
    "    common_nonneg_contr = [\"could've\",\n",
    "    \"he'd\",\n",
    "    \"he'd've\",\n",
    "    \"he'll\",\n",
    "    \"he's\",\n",
    "    \"how'd\",\n",
    "    \"how'll\",\n",
    "    \"how's\",\n",
    "    \"i'd\",\n",
    "    \"i'd've\",\n",
    "    \"i'll\",\n",
    "    \"i'm\",\n",
    "    \"i've\",\n",
    "    \"it'd\",\n",
    "    \"it'd've\",\n",
    "    \"it'll\",\n",
    "    \"it's\",\n",
    "    \"let's\",\n",
    "    \"ma'am\",\n",
    "    \"might've\",\n",
    "    \"must've\",\n",
    "    \"o'clock\",\n",
    "    \"'ow's'at\",\n",
    "    \"she'd\",\n",
    "    \"she'd've\",\n",
    "    \"she'll\",\n",
    "    \"she's\",\n",
    "    \"should've\",\n",
    "    \"somebody'd\",\n",
    "    \"somebody'd've\",\n",
    "    \"somebody'll\",\n",
    "    \"somebody's\",\n",
    "    \"someone'd\",\n",
    "    \"someone'd've\",\n",
    "    \"someone'll\",\n",
    "    \"someone's\",\n",
    "    \"something'd\",\n",
    "    \"something'd've\",\n",
    "    \"something'll\",\n",
    "    \"something's\",\n",
    "    \"that'll\",\n",
    "    \"that's\",\n",
    "    \"there'd\",\n",
    "    \"there'd've\",\n",
    "    \"there're\",\n",
    "    \"there's\",\n",
    "    \"they'd\",\n",
    "    \"they'd've\",\n",
    "    \"they'll\",\n",
    "    \"they're\",\n",
    "    \"they've\",\n",
    "    \"'twas\",\n",
    "    \"we'd\",\n",
    "    \"we'd've\",\n",
    "    \"we'll\",\n",
    "    \"we're\",\n",
    "    \"we've\",\n",
    "    \"what'll\",\n",
    "    \"what're\",\n",
    "    \"what's\",\n",
    "    \"what've\",\n",
    "    \"when's\",\n",
    "    \"where'd\",\n",
    "    \"where's\",\n",
    "    \"where've\",\n",
    "    \"who'd\",\n",
    "    \"who'd've\",\n",
    "    \"who'll\",\n",
    "    \"who're\",\n",
    "    \"who's\",\n",
    "    \"who've\",\n",
    "    \"why'll\",\n",
    "    \"why're\",\n",
    "    \"why's\",\n",
    "    \"would've\",\n",
    "    \"y'all\",\n",
    "    \"y'all'll\",\n",
    "    \"y'all'd've\",\n",
    "    \"you'd\",\n",
    "    \"you'd've\",\n",
    "    \"you'll\",\n",
    "    \"you're\",\n",
    "    \"you've\"]\n",
    "\n",
    "    letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
    "          'u', 'v', 'w', 'x', 'y', 'z']\n",
    "        \n",
    "    ranks = ['st', 'nd', 'rd', 'th']\n",
    "    \n",
    "    for x in neg_stops:\n",
    "        if x in stops:\n",
    "            stops.remove(x)\n",
    "        \n",
    "    new_stops = stops + common_nonneg_contr + letters + ranks + [\"\"] + ['us'] + [''] \n",
    "    stops = list(set(new_stops))\n",
    "    return stops\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#The if len(word) > 0 check is still not sufficient.. as it will leave in '' tokens\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    #Added in this line because originally broke when trying to pass through '', which occured when there was\n",
    "    #a token like '2's' that got reduced to \"'s\" and then '' before being passed through lemmatizer\n",
    "    if len(word) > 0:\n",
    "        tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "        tag_dict = {\"a\": wordnet.ADJ,\n",
    "                    \"n\": wordnet.NOUN,\n",
    "                    \"v\": wordnet.VERB,\n",
    "                    \"r\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def _clean_review(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    tokenizer = nltk.RegexpTokenizer('\\w+\\'?\\w+')\n",
    "    filtered_tokens = [(re.sub(r\"[^A-Za-z\\s']\", '', token)) for token in tokenizer.tokenize(text)]\n",
    "    stops = _create_stop_words()\n",
    "    tokens = [token for token in filtered_tokens if token not in stops]\n",
    "    for i, token in enumerate(tokens):\n",
    "        filtered_token = re.sub(\"'s\", '', token)\n",
    "        tokens[i] = wnl.lemmatize(filtered_token, pos= get_wordnet_pos(filtered_token))\n",
    "    return tokens\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def get_wordnet_pos2(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def _clean_review2(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    tokenizer = nltk.RegexpTokenizer('\\w+\\'?\\w+')\n",
    "    filtered_tokens = [(re.sub(r\"[^A-Za-z\\s']\", '', token)) for token in tokenizer.tokenize(text)]\n",
    "    stops = _create_stop_words()\n",
    "    tokens = [token for token in filtered_tokens if token not in stops]\n",
    "    tokens = [re.sub(\"'s\", '', token) for token in tokens if re.sub(\"'s\", '', token) != '']\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos2(token))\n",
    "    tokens = [token for token in tokens if token != '']\n",
    "    return tokens\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_review(text):\n",
    "    def _create_stop_words():\n",
    "        stops = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "        neg_stops = ['no',\n",
    "         'nor',\n",
    "         'not',\n",
    "         'don',\n",
    "         \"don't\",\n",
    "         'ain',\n",
    "         'aren',\n",
    "         \"aren't\",\n",
    "         'couldn',\n",
    "         \"couldn't\",\n",
    "         'didn',\n",
    "         \"didn't\",\n",
    "         'doesn',\n",
    "         \"doesn't\",\n",
    "         'hadn',\n",
    "         \"hadn't\",\n",
    "         'hasn',\n",
    "         \"hasn't\",\n",
    "         'haven',\n",
    "         \"haven't\",\n",
    "         'isn',\n",
    "         \"isn't\",\n",
    "         'mightn',\n",
    "         \"mightn't\",\n",
    "         'mustn',\n",
    "         \"mustn't\",\n",
    "         'needn',\n",
    "         \"needn't\",\n",
    "         'shan',\n",
    "         \"shan't\",\n",
    "         'shouldn',\n",
    "         \"shouldn't\",\n",
    "         'wasn',\n",
    "         \"wasn't\",\n",
    "         'weren',\n",
    "         \"weren't\",\n",
    "         \"won'\",\n",
    "         \"won't\",\n",
    "         'wouldn',\n",
    "         \"wouldn't\",\n",
    "         'but',\n",
    "         \"don'\",\n",
    "         \"ain't\"]\n",
    "\n",
    "        common_nonneg_contr = [\"could've\",\n",
    "        \"he'd\",\n",
    "        \"he'd've\",\n",
    "        \"he'll\",\n",
    "        \"he's\",\n",
    "        \"how'd\",\n",
    "        \"how'll\",\n",
    "        \"how's\",\n",
    "        \"i'd\",\n",
    "        \"i'd've\",\n",
    "        \"i'll\",\n",
    "        \"i'm\",\n",
    "        \"i've\",\n",
    "        \"it'd\",\n",
    "        \"it'd've\",\n",
    "        \"it'll\",\n",
    "        \"it's\",\n",
    "        \"let's\",\n",
    "        \"ma'am\",\n",
    "        \"might've\",\n",
    "        \"must've\",\n",
    "        \"o'clock\",\n",
    "        \"'ow's'at\",\n",
    "        \"she'd\",\n",
    "        \"she'd've\",\n",
    "        \"she'll\",\n",
    "        \"she's\",\n",
    "        \"should've\",\n",
    "        \"somebody'd\",\n",
    "        \"somebody'd've\",\n",
    "        \"somebody'll\",\n",
    "        \"somebody's\",\n",
    "        \"someone'd\",\n",
    "        \"someone'd've\",\n",
    "        \"someone'll\",\n",
    "        \"someone's\",\n",
    "        \"something'd\",\n",
    "        \"something'd've\",\n",
    "        \"something'll\",\n",
    "        \"something's\",\n",
    "        \"that'll\",\n",
    "        \"that's\",\n",
    "        \"there'd\",\n",
    "        \"there'd've\",\n",
    "        \"there're\",\n",
    "        \"there's\",\n",
    "        \"they'd\",\n",
    "        \"they'd've\",\n",
    "        \"they'll\",\n",
    "        \"they're\",\n",
    "        \"they've\",\n",
    "        \"'twas\",\n",
    "        \"we'd\",\n",
    "        \"we'd've\",\n",
    "        \"we'll\",\n",
    "        \"we're\",\n",
    "        \"we've\",\n",
    "        \"what'll\",\n",
    "        \"what're\",\n",
    "        \"what's\",\n",
    "        \"what've\",\n",
    "        \"when's\",\n",
    "        \"where'd\",\n",
    "        \"where's\",\n",
    "        \"where've\",\n",
    "        \"who'd\",\n",
    "        \"who'd've\",\n",
    "        \"who'll\",\n",
    "        \"who're\",\n",
    "        \"who's\",\n",
    "        \"who've\",\n",
    "        \"why'll\",\n",
    "        \"why're\",\n",
    "        \"why's\",\n",
    "        \"would've\",\n",
    "        \"y'all\",\n",
    "        \"y'all'll\",\n",
    "        \"y'all'd've\",\n",
    "        \"you'd\",\n",
    "        \"you'd've\",\n",
    "        \"you'll\",\n",
    "        \"you're\",\n",
    "        \"you've\"]\n",
    "\n",
    "        letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
    "          'u', 'v', 'w', 'x', 'y', 'z']\n",
    "        \n",
    "        ranks = ['st', 'nd', 'rd', 'th']\n",
    "        \n",
    "        for x in neg_stops:\n",
    "            if x in stops:\n",
    "                stops.remove(x)\n",
    "\n",
    "        new_stops = stops + common_nonneg_contr + letters + ranks + [\"\"] + ['us'] + ['']\n",
    "        stops = list(set(new_stops))\n",
    "        return stops\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "        tag_dict = {\"a\": wordnet.ADJ,\n",
    "                    \"n\": wordnet.NOUN,\n",
    "                    \"v\": wordnet.VERB,\n",
    "                    \"r\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def _clean_review(text):\n",
    "        text = text.lower()\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "        tokenizer = nltk.RegexpTokenizer('\\w+\\'?\\w+')\n",
    "        filtered_tokens = [(re.sub(r\"[^A-Za-z\\s']\", '', token)) for token in tokenizer.tokenize(text)]\n",
    "        stops = _create_stop_words()\n",
    "        tokens = [token for token in filtered_tokens if token not in stops]\n",
    "        tokens = [re.sub(\"'s\", '', token) for token in tokens if re.sub(\"'s\", '', token) != '']\n",
    "        for i, token in enumerate(tokens):\n",
    "            tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "        tokens = [token for token in tokens if token != '' and token not in stops]\n",
    "        return tokens\n",
    "    \n",
    "    return _clean_review(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Code to apply _clean_review function on all review_text column and put tokens in new column titled 'review_tokens'\n",
    "def apply_on_column(data):\n",
    "    data['review_tokens'] = data['review_text'].apply(lambda x: _clean_review(x))\n",
    "    return data\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to apply _process_review function on all review_text column and put tokens in new column titled 'review_tokens'\n",
    "def apply_on_column(data):\n",
    "    data['review_tokens'] = data['review_text'].apply(lambda x: _process_review(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Get times for how long it takes to run apply_on_column function on review sample\n",
    "start = time.time()\n",
    "apply_on_column(review_sample)\n",
    "end = time.time()\n",
    "dur = end - start\n",
    "# Verify that the function is working\n",
    "print('Processed {} instances in {} minutes {} seconds.\\n'.format(review_sample.shape[0], dur//60, dur%60))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get times for how long it takes to run apply_on_column2 function on review sample\n",
    "start = time.time()\n",
    "apply_on_column(review_sample)\n",
    "end = time.time()\n",
    "dur = end - start\n",
    "# Verify that the function is working\n",
    "print('Processed {} instances in {} minutes {} seconds.\\n'.format(review_sample.shape[0], dur//60, dur%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see that the 'review_tokens' column was properly created\n",
    "review_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out example full review and its associated tokens after running _clean_review()\n",
    "print('Full review:\\n\\n{}'.format(review_sample.loc[9, 'review_text']))\n",
    "print('\\n\\nTokenized review: \\n\\n{}'.format(review_sample.loc[9, 'review_tokens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram codes for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(tokens, n):\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure ngrams code is working for bi-grams\n",
    "get_ngrams(review_sample.loc[9, 'review_tokens'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what about tri-grams?\n",
    "get_ngrams(review_sample.loc[9, 'review_tokens'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ngrams_on_column(data):\n",
    "    for n in range(2,6):\n",
    "        data['{}_grams'.format(n)] = data['review_tokens'].apply(lambda x: get_ngrams(x, n))\n",
    "        print('Done creating {}-grams...'.format(n))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_ngrams_on_column(review_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure the ngrams function worked as you thought by viewing a few exmples from the dataframe\n",
    "\n",
    "print('Full review:\\n\\n{}'.format(review_sample.loc[9, 'review_text']))\n",
    "print('\\n\\nTokenized review:\\n\\n{}'.format(review_sample.loc[9, 'review_tokens']))\n",
    "print('\\n\\n2-grams review: \\n\\n{}'.format(review_sample.loc[9, '2_grams']))\n",
    "print('\\n\\n3-grams review: \\n\\n{}'.format(review_sample.loc[9, '3_grams']))\n",
    "print('\\n\\n4-grams review: \\n\\n{}'.format(review_sample.loc[9, '4_grams']))\n",
    "print('\\n\\n5-grams review: \\n\\n{}'.format(review_sample.loc[9, '5_grams']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts of Tokens in Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates function to show us a vizualization of our top 50 token counts\n",
    "#Adjust n to show n top tokens. Default is 50\n",
    "def _get_top_tokens(tokens, n = 50):\n",
    "    \n",
    "    def dummy_fun(text):\n",
    "        return text\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "    tokenizer = dummy_fun,\n",
    "    preprocessor= dummy_fun,\n",
    "    token_pattern=None)\n",
    "    \n",
    "    docs = vectorizer.fit_transform(tokens)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    visualizer = FreqDistVisualizer(features=features, size=(1080, 720), n = n)\n",
    "    visualizer.fit(docs)\n",
    "    visualizer.poof()\n",
    "\n",
    "#I'm going to update this function to spit out a histogram\n",
    "def _get_least_tokens(tokens, n = 50):\n",
    "    def dummy_fun(text):\n",
    "        return text\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "    tokenizer = dummy_fun,\n",
    "    preprocessor= dummy_fun,\n",
    "    token_pattern=None)\n",
    "    \n",
    "    docs = vectorizer.fit_transform(tokens)\n",
    "    counts = docs.sum(axis=0).A1\n",
    "    features = vectorizer.get_feature_names()\n",
    "    freq_distribution = Counter(dict(zip(features, counts)))\n",
    "    return list(reversed(freq_distribution.most_common()[-n:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see what our top 50 tokens are!\n",
    "_get_top_tokens(review_sample['review_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_least_tokens(review_sample['review_tokens'], 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(text):\n",
    "    return text\n",
    "\n",
    "#This first TF-IDF function creates a vectorizer that takes the review text in string format (review_text column)\n",
    "#So if our dataframe review_sample, it would take: review_sample['review_text']\n",
    "#That's because it calls our pre-made preprocessor, _process_review\n",
    "tfidf1 = TfidfVectorizer(\n",
    "    tokenizer=_process_review,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)\n",
    "\n",
    "#This second TF-IDF function takes our already tokenized reviews, so the column review_sample['review_tokens']\n",
    "#This essentially means that we need to run our custom preprocessor _process_review on our review text in raw form\n",
    "tfidf2 = TfidfVectorizer(\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run tfidf1\n",
    "Y = tfidf1.fit_transform(review_sample['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df1 = pd.DataFrame(Y.toarray(), columns=tfidf1.get_feature_names())\n",
    "idf_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review_sample.loc[11, 'review_text'])\n",
    "print(idf_df1.loc[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run tfidf2\n",
    "Z = tfidf2.fit_transform(review_sample['review_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df2 = pd.DataFrame(Z.toarray(), columns=tfidf2.get_feature_names())\n",
    "idf_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review_sample.loc[11, 'review_text'])\n",
    "print(idf_df2.loc[11])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
