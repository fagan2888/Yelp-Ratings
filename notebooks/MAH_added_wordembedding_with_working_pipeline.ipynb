{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sorry for this crazy import cell... this will be cleaned up\n",
    "\n",
    "import psycopg2\n",
    "import numpy\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import nltk.corpus\n",
    "from nltk.text import TextCollection\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "stops = nltk.corpus.stopwords.words('english')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input your PostGres credentials to connect\n",
    "\n",
    "dbname = 'yelp'\n",
    "username = 'postgres'\n",
    "host = 'localhost'\n",
    "password = 'Khoobam1234!'\n",
    "\n",
    "conn = psycopg2.connect('dbname={} user={} host={} password={}'.format(dbname, username, host, password))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    SELECT * FROM review LIMIT 1000\n",
    "\"\"\")\n",
    "\n",
    "cols = ['review_id', 'user_id', 'business_id', 'stars', 'review_date', 'review_text', 'useful', 'funny', 'cool']\n",
    "\n",
    "review_sample = pd.DataFrame(cur.fetchall(), columns=cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample['review_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contractions:\n",
    "https://gist.github.com/J3RN/ed7b420a6ea1d5bd6d06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_nonneg_contr = [\"could've\",\n",
    "\"he'd\",\n",
    "\"he'd've\",\n",
    "\"he'll\",\n",
    "\"he's\",\n",
    "\"how'd\",\n",
    "\"how'll\",\n",
    "\"how's\",\n",
    "\"i'd\",\n",
    "\"i'd've\",\n",
    "\"i'll\",\n",
    "\"i'm\",\n",
    "\"i've\",\n",
    "\"it'd\",\n",
    "\"it'd've\",\n",
    "\"it'll\",\n",
    "\"it's\",\n",
    "\"let's\",\n",
    "\"ma'am\",\n",
    "\"might've\",\n",
    "\"must've\",\n",
    "\"o'clock\",\n",
    "\"'ow's'at\",\n",
    "\"she'd\",\n",
    "\"she'd've\",\n",
    "\"she'll\",\n",
    "\"she's\",\n",
    "\"should've\",\n",
    "\"somebody'd\",\n",
    "\"somebody'd've\",\n",
    "\"somebody'll\",\n",
    "\"somebody's\",\n",
    "\"someone'd\",\n",
    "\"someone'd've\",\n",
    "\"someone'll\",\n",
    "\"someone's\",\n",
    "\"something'd\",\n",
    "\"something'd've\",\n",
    "\"something'll\",\n",
    "\"something's\",\n",
    "\"that'll\",\n",
    "\"that's\",\n",
    "\"there'd\",\n",
    "\"there'd've\",\n",
    "\"there're\",\n",
    "\"there's\",\n",
    "\"they'd\",\n",
    "\"they'd've\",\n",
    "\"they'll\",\n",
    "\"they're\",\n",
    "\"they've\",\n",
    "\"'twas\",\n",
    "\"we'd\",\n",
    "\"we'd've\",\n",
    "\"we'll\",\n",
    "\"we're\",\n",
    "\"we've\",\n",
    "\"what'll\",\n",
    "\"what're\",\n",
    "\"what's\",\n",
    "\"what've\",\n",
    "\"when's\",\n",
    "\"where'd\",\n",
    "\"where's\",\n",
    "\"where've\",\n",
    "\"who'd\",\n",
    "\"who'd've\",\n",
    "\"who'll\",\n",
    "\"who're\",\n",
    "\"who's\",\n",
    "\"who've\",\n",
    "\"why'll\",\n",
    "\"why're\",\n",
    "\"why's\",\n",
    "\"would've\",\n",
    "\"y'all\",\n",
    "\"y'all'll\",\n",
    "\"y'all'd've\",\n",
    "\"you'd\",\n",
    "\"you'd've\",\n",
    "\"you'll\",\n",
    "\"you're\",\n",
    "\"you've\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_stops1 = ['no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'don',\n",
    " \"don't\",\n",
    " 'ain',\n",
    " 'aren',\n",
    " \"aren't\",\n",
    " 'couldn',\n",
    " \"couldn't\",\n",
    " 'didn',\n",
    " \"didn't\",\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'hadn',\n",
    " \"hadn't\",\n",
    " 'hasn',\n",
    " \"hasn't\",\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'isn',\n",
    " \"isn't\",\n",
    " 'mightn',\n",
    " \"mightn't\",\n",
    " 'mustn',\n",
    " \"mustn't\",\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'shouldn',\n",
    " \"shouldn't\",\n",
    " 'wasn',\n",
    " \"wasn't\",\n",
    " 'weren',\n",
    " \"weren't\",\n",
    " 'won',\n",
    " \"won't\",\n",
    " 'wouldn',\n",
    " \"wouldn't\",\n",
    " 'but',\n",
    " \"don'\",\n",
    " \"ain't\"]\n",
    "\n",
    "words_to_be_added = ['us']\n",
    "stops1 = ['i',\n",
    " 'me',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'we',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'you',\n",
    " \"you're\",\n",
    " \"you've\",\n",
    " \"you'll\",\n",
    " \"you'd\",\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves',\n",
    " 'he',\n",
    " 'him',\n",
    " 'his',\n",
    " 'himself',\n",
    " 'she',\n",
    " \"she's\",\n",
    " 'her',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'it',\n",
    " \"it's\",\n",
    " 'its',\n",
    " 'itself',\n",
    " 'they',\n",
    " 'them',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'themselves',\n",
    " 'what',\n",
    " 'which',\n",
    " 'who',\n",
    " 'whom',\n",
    " 'this',\n",
    " 'that',\n",
    " \"that'll\",\n",
    " 'these',\n",
    " 'those',\n",
    " 'am',\n",
    " 'is',\n",
    " 'are',\n",
    " 'was',\n",
    " 'were',\n",
    " 'be',\n",
    " 'been',\n",
    " 'being',\n",
    " 'have',\n",
    " 'has',\n",
    " 'had',\n",
    " 'having',\n",
    " 'do',\n",
    " 'does',\n",
    " 'did',\n",
    " 'doing',\n",
    " 'a',\n",
    " 'an',\n",
    " 'the',\n",
    " 'and',\n",
    " 'but',\n",
    " 'if',\n",
    " 'or',\n",
    " 'because',\n",
    " 'as',\n",
    " 'until',\n",
    " 'while',\n",
    " 'of',\n",
    " 'at',\n",
    " 'by',\n",
    " 'for',\n",
    " 'with',\n",
    " 'about',\n",
    " 'against',\n",
    " 'between',\n",
    " 'into',\n",
    " 'through',\n",
    " 'during',\n",
    " 'before',\n",
    " 'after',\n",
    " 'above',\n",
    " 'below',\n",
    " 'to',\n",
    " 'from',\n",
    " 'up',\n",
    " 'down',\n",
    " 'in',\n",
    " 'out',\n",
    " 'on',\n",
    " 'off',\n",
    " 'over',\n",
    " 'under',\n",
    " 'again',\n",
    " 'further',\n",
    " 'then',\n",
    " 'once',\n",
    " 'here',\n",
    " 'there',\n",
    " 'when',\n",
    " 'where',\n",
    " 'why',\n",
    " 'how',\n",
    " 'all',\n",
    " 'any',\n",
    " 'both',\n",
    " 'each',\n",
    " 'few',\n",
    " 'more',\n",
    " 'most',\n",
    " 'other',\n",
    " 'some',\n",
    " 'such',\n",
    " 'no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'only',\n",
    " 'own',\n",
    " 'same',\n",
    " 'so',\n",
    " 'than',\n",
    " 'too',\n",
    " 'very',\n",
    " 's',\n",
    " 't',\n",
    " 'can',\n",
    " 'will',\n",
    " 'just',\n",
    " 'don',\n",
    " \"don't\",\n",
    " 'should',\n",
    " \"should've\",\n",
    " 'now',\n",
    " 'd',\n",
    " 'll',\n",
    " 'm',\n",
    " 'o',\n",
    " 're',\n",
    " 've',\n",
    " 'y',\n",
    " 'ain',\n",
    " 'aren',\n",
    " \"aren't\",\n",
    " 'couldn',\n",
    " \"couldn't\",\n",
    " 'didn',\n",
    " \"didn't\",\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'hadn',\n",
    " \"hadn't\",\n",
    " 'hasn',\n",
    " \"hasn't\",\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'isn',\n",
    " \"isn't\",\n",
    " 'ma',\n",
    " 'mightn',\n",
    " \"mightn't\",\n",
    " 'mustn',\n",
    " \"mustn't\",\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'shouldn',\n",
    " \"shouldn't\",\n",
    " 'wasn',\n",
    " \"wasn't\",\n",
    " 'weren',\n",
    " \"weren't\",\n",
    " 'won',\n",
    " \"won't\",\n",
    " 'wouldn',\n",
    " \"wouldn't\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ma']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in neg_stops1:\n",
    "    if x in stops1:\n",
    "        stops1.remove(x)\n",
    "        \n",
    "stops1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"it'd've\",\n",
       " 'against',\n",
       " \"y'all'll\",\n",
       " 'as',\n",
       " \"that's\",\n",
       " \"y'all'd've\",\n",
       " 'the',\n",
       " \"there'd\",\n",
       " 'again',\n",
       " 'which',\n",
       " 'who',\n",
       " 'do',\n",
       " 'ours',\n",
       " \"i'll\",\n",
       " 'its',\n",
       " \"something'd\",\n",
       " 'why',\n",
       " 'all',\n",
       " \"they're\",\n",
       " 're',\n",
       " 'am',\n",
       " 'below',\n",
       " 'off',\n",
       " \"what're\",\n",
       " 'my',\n",
       " 'should',\n",
       " 'during',\n",
       " 'will',\n",
       " 'over',\n",
       " 'same',\n",
       " \"when's\",\n",
       " 'theirs',\n",
       " \"i'd've\",\n",
       " 'other',\n",
       " 's',\n",
       " 'those',\n",
       " 'her',\n",
       " 'and',\n",
       " 'from',\n",
       " \"something'd've\",\n",
       " 'for',\n",
       " \"somebody'd\",\n",
       " 'he',\n",
       " 'myself',\n",
       " 'until',\n",
       " \"he'd've\",\n",
       " \"she'd've\",\n",
       " \"somebody's\",\n",
       " 'very',\n",
       " 'll',\n",
       " 'above',\n",
       " 'a',\n",
       " 'were',\n",
       " 'such',\n",
       " \"where's\",\n",
       " \"who'd\",\n",
       " 'then',\n",
       " 'that',\n",
       " \"why's\",\n",
       " \"they'd've\",\n",
       " 'here',\n",
       " 'itself',\n",
       " 'themselves',\n",
       " 'are',\n",
       " 'few',\n",
       " \"you'd've\",\n",
       " \"there're\",\n",
       " 'she',\n",
       " \"where've\",\n",
       " \"might've\",\n",
       " 'or',\n",
       " 'them',\n",
       " \"someone'd've\",\n",
       " 'ourselves',\n",
       " \"you've\",\n",
       " 'of',\n",
       " 'in',\n",
       " 'because',\n",
       " 'doing',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'on',\n",
       " \"who'll\",\n",
       " 'does',\n",
       " 'out',\n",
       " 'between',\n",
       " \"'ow's'at\",\n",
       " 'whom',\n",
       " \"they'd\",\n",
       " 'to',\n",
       " 'up',\n",
       " \"something'll\",\n",
       " 'where',\n",
       " 'both',\n",
       " 'any',\n",
       " 've',\n",
       " \"it'll\",\n",
       " 'these',\n",
       " \"let's\",\n",
       " 'than',\n",
       " \"y'all\",\n",
       " \"what've\",\n",
       " 'there',\n",
       " \"they'll\",\n",
       " \"we'll\",\n",
       " 'this',\n",
       " 'was',\n",
       " 'yourself',\n",
       " \"who're\",\n",
       " 'after',\n",
       " \"could've\",\n",
       " \"someone'll\",\n",
       " 'into',\n",
       " \"we've\",\n",
       " 'his',\n",
       " \"somebody'd've\",\n",
       " 'had',\n",
       " 'only',\n",
       " \"there's\",\n",
       " 'just',\n",
       " \"it's\",\n",
       " 'their',\n",
       " \"we're\",\n",
       " 'an',\n",
       " \"she'd\",\n",
       " \"you'll\",\n",
       " 'they',\n",
       " \"i've\",\n",
       " \"someone'd\",\n",
       " \"how'd\",\n",
       " \"he'd\",\n",
       " 'at',\n",
       " 'own',\n",
       " \"who'd've\",\n",
       " 'most',\n",
       " \"why're\",\n",
       " 'herself',\n",
       " 'hers',\n",
       " 'it',\n",
       " 'has',\n",
       " 'when',\n",
       " 'o',\n",
       " \"he's\",\n",
       " 'if',\n",
       " \"you're\",\n",
       " 't',\n",
       " 'your',\n",
       " \"ma'am\",\n",
       " \"where'd\",\n",
       " 'we',\n",
       " \"she'll\",\n",
       " 'with',\n",
       " \"who've\",\n",
       " 'by',\n",
       " 'further',\n",
       " 'so',\n",
       " \"someone's\",\n",
       " 'under',\n",
       " 'what',\n",
       " 'you',\n",
       " 'can',\n",
       " 'yourselves',\n",
       " 'himself',\n",
       " \"how'll\",\n",
       " 'be',\n",
       " 'now',\n",
       " \"we'd\",\n",
       " 'm',\n",
       " \"must've\",\n",
       " 'more',\n",
       " 'y',\n",
       " 'yours',\n",
       " 'being',\n",
       " \"should've\",\n",
       " 'having',\n",
       " \"what's\",\n",
       " \"o'clock\",\n",
       " \"there'd've\",\n",
       " 'down',\n",
       " 'through',\n",
       " \"they've\",\n",
       " \"something's\",\n",
       " 'did',\n",
       " \"i'd\",\n",
       " \"he'll\",\n",
       " 'is',\n",
       " 'our',\n",
       " \"what'll\",\n",
       " \"who's\",\n",
       " 'him',\n",
       " \"would've\",\n",
       " 'been',\n",
       " 'before',\n",
       " \"it'd\",\n",
       " \"'twas\",\n",
       " 'about',\n",
       " 'too',\n",
       " 'have',\n",
       " 'once',\n",
       " \"we'd've\",\n",
       " \"you'd\",\n",
       " \"somebody'll\",\n",
       " 'while',\n",
       " 'some',\n",
       " 'how',\n",
       " 'each',\n",
       " \"that'll\",\n",
       " 'd',\n",
       " \"i'm\",\n",
       " 'i',\n",
       " \"she's\",\n",
       " \"how's\",\n",
       " \"why'll\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_stops = stops1 + common_nonneg_contr\n",
    "list(set(new_stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell below is preproccessing and tokenezation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_stops(tokens):\n",
    "    stops = nltk.corpus.stopwords.words('english')\n",
    "    neg_stops = [\n",
    "    'no', 'not', 'nor', 'don\\'', 'don\\'t', 'ain', \n",
    "    'ain\\'t', 'aren\\'t', 'aren', 'couldn', 'couldn\\'t', \n",
    "    'didn', 'didn\\'t', 'doesn', 'doesn\\'t', 'hadn', \n",
    "    'hadn\\'t', 'hasn', 'hasn\\'t', 'haven', 'haven\\'t',\n",
    "    'isn', 'isn\\'t', 'mightn', 'mightn\\'t', 'mustn', \n",
    "    'mustn\\'t', 'needn', 'needn\\'t', 'shan', 'shan\\'t',\n",
    "    'shouldn', 'shouldn\\'t', 'wasn', 'wasn\\'t', 'weren',\n",
    "    'weren\\'t', 'won', 'won\\'t', 'wouldn', 'wouldn\\'t'\n",
    "    ]\n",
    "#still leaves in but and don.. fix this.. \n",
    "#doesn't get rid of other obvious stopwords, like i'm, they're....\n",
    "    for x in neg_stops:\n",
    "        if x in stops:\n",
    "            stops.remove(x)\n",
    "            \n",
    "    tokens_without_stops = [token for token in tokens if token not in stops]\n",
    "    return tokens_without_stops       \n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def _clean_review2(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    text = re.sub(r\"[^A-Za-z\\s']\", '', text)  \n",
    "    tokens = [token for token in text.split() if token not in new_stops]\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "    return tokens\n",
    "\n",
    "def _process_review2(text):\n",
    "    tokens = _remove_stops(_clean_review2(text))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _process_review(text):\n",
    "    def _create_stop_words():\n",
    "        stops = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "        neg_stops = ['no',\n",
    "         'nor',\n",
    "         'not',\n",
    "         'don',\n",
    "         \"don't\",\n",
    "         'ain',\n",
    "         'aren',\n",
    "         \"aren't\",\n",
    "         'couldn',\n",
    "         \"couldn't\",\n",
    "         'didn',\n",
    "         \"didn't\",\n",
    "         'doesn',\n",
    "         \"doesn't\",\n",
    "         'hadn',\n",
    "         \"hadn't\",\n",
    "         'hasn',\n",
    "         \"hasn't\",\n",
    "         'haven',\n",
    "         \"haven't\",\n",
    "         'isn',\n",
    "         \"isn't\",\n",
    "         'mightn',\n",
    "         \"mightn't\",\n",
    "         'mustn',\n",
    "         \"mustn't\",\n",
    "         'needn',\n",
    "         \"needn't\",\n",
    "         'shan',\n",
    "         \"shan't\",\n",
    "         'shouldn',\n",
    "         \"shouldn't\",\n",
    "         'wasn',\n",
    "         \"wasn't\",\n",
    "         'weren',\n",
    "         \"weren't\",\n",
    "         \"won'\",\n",
    "         \"won't\",\n",
    "         'wouldn',\n",
    "         \"wouldn't\",\n",
    "         'but',\n",
    "         \"don'\",\n",
    "         \"ain't\"]\n",
    "\n",
    "        common_nonneg_contr = [\"could've\",\n",
    "        \"he'd\",\n",
    "        \"he'd've\",\n",
    "        \"he'll\",\n",
    "        \"he's\",\n",
    "        \"how'd\",\n",
    "        \"how'll\",\n",
    "        \"how's\",\n",
    "        \"i'd\",\n",
    "        \"i'd've\",\n",
    "        \"i'll\",\n",
    "        \"i'm\",\n",
    "        \"i've\",\n",
    "        \"it'd\",\n",
    "        \"it'd've\",\n",
    "        \"it'll\",\n",
    "        \"it's\",\n",
    "        \"let's\",\n",
    "        \"ma'am\",\n",
    "        \"might've\",\n",
    "        \"must've\",\n",
    "        \"o'clock\",\n",
    "        \"'ow's'at\",\n",
    "        \"she'd\",\n",
    "        \"she'd've\",\n",
    "        \"she'll\",\n",
    "        \"she's\",\n",
    "        \"should've\",\n",
    "        \"somebody'd\",\n",
    "        \"somebody'd've\",\n",
    "        \"somebody'll\",\n",
    "        \"somebody's\",\n",
    "        \"someone'd\",\n",
    "        \"someone'd've\",\n",
    "        \"someone'll\",\n",
    "        \"someone's\",\n",
    "        \"something'd\",\n",
    "        \"something'd've\",\n",
    "        \"something'll\",\n",
    "        \"something's\",\n",
    "        \"that'll\",\n",
    "        \"that's\",\n",
    "        \"there'd\",\n",
    "        \"there'd've\",\n",
    "        \"there're\",\n",
    "        \"there's\",\n",
    "        \"they'd\",\n",
    "        \"they'd've\",\n",
    "        \"they'll\",\n",
    "        \"they're\",\n",
    "        \"they've\",\n",
    "        \"'twas\",\n",
    "        \"we'd\",\n",
    "        \"we'd've\",\n",
    "        \"we'll\",\n",
    "        \"we're\",\n",
    "        \"we've\",\n",
    "        \"what'll\",\n",
    "        \"what're\",\n",
    "        \"what's\",\n",
    "        \"what've\",\n",
    "        \"when's\",\n",
    "        \"where'd\",\n",
    "        \"where's\",\n",
    "        \"where've\",\n",
    "        \"who'd\",\n",
    "        \"who'd've\",\n",
    "        \"who'll\",\n",
    "        \"who're\",\n",
    "        \"who's\",\n",
    "        \"who've\",\n",
    "        \"why'll\",\n",
    "        \"why're\",\n",
    "        \"why's\",\n",
    "        \"would've\",\n",
    "        \"y'all\",\n",
    "        \"y'all'll\",\n",
    "        \"y'all'd've\",\n",
    "        \"you'd\",\n",
    "        \"you'd've\",\n",
    "        \"you'll\",\n",
    "        \"you're\",\n",
    "        \"you've\"]\n",
    "\n",
    "        letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
    "          'u', 'v', 'w', 'x', 'y', 'z']\n",
    "        \n",
    "        ranks = ['st', 'nd', 'rd', 'th']\n",
    "        \n",
    "        for x in neg_stops:\n",
    "            if x in stops:\n",
    "                stops.remove(x)\n",
    "\n",
    "        new_stops = stops + common_nonneg_contr + letters + ranks + [\"\"] + ['us'] + ['']\n",
    "        stops = list(set(new_stops))\n",
    "        return stops\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "        tag_dict = {\"a\": wordnet.ADJ,\n",
    "                    \"n\": wordnet.NOUN,\n",
    "                    \"v\": wordnet.VERB,\n",
    "                    \"r\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def _clean_review(text):\n",
    "        text = text.lower()\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "        tokenizer = nltk.RegexpTokenizer('\\w+\\'?\\w+')\n",
    "        filtered_tokens = [(re.sub(r\"[^A-Za-z\\s']\", '', token)) for token in tokenizer.tokenize(text)]\n",
    "        stops = _create_stop_words()\n",
    "        tokens = [token for token in filtered_tokens if token not in stops]\n",
    "        tokens = [re.sub(\"'s\", '', token) for token in tokens if re.sub(\"'s\", '', token) != '']\n",
    "        for i, token in enumerate(tokens):\n",
    "            tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "        tokens = [token for token in tokens if token != '' and token not in stops]\n",
    "        return tokens\n",
    "    \n",
    "    return _clean_review(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fyi, tweaked Alice's code here to allow map_on_column to accept another function\n",
    "def map_on_column(data, function):\n",
    "    data['review_text'] = data['review_text'].map(lambda x: function(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fomc2nZfjK6Se9v-Ba5oWQ</td>\n",
       "      <td>bBQcAreoMmfd8LICFGWjtw</td>\n",
       "      <td>coHUUrQSdWpiFCfF2ayFNA</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>[church, like, big, beautiful, oak, tree, many...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seHlbqQhMNqCQQPb7drsRQ</td>\n",
       "      <td>D_RjBQlUppSLIJhSgvf1xg</td>\n",
       "      <td>CGpC_L37PBnIbs8TsuBEpA</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-09-21</td>\n",
       "      <td>[trek, great, line, quality, bike, entry, leve...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SwIV2t-U0QhF4GnHmPXcWw</td>\n",
       "      <td>P34rd7qs2LhNa1K0VYuVJA</td>\n",
       "      <td>wJVW-Bgc9j2aaMnwI7TH3Q</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-04-22</td>\n",
       "      <td>[experienced, charcut, alleyburger, don't, kno...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eJXmg_cHbofsjKwgcSpPXw</td>\n",
       "      <td>iLZ8eMMafDrNnRqI5164-Q</td>\n",
       "      <td>thlAnPN1ApoNxSnok_fcvA</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-03-27</td>\n",
       "      <td>[first, time, restaurant, want, try, base, rec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tI2xPNMV9N2Z6ucMPV6vUQ</td>\n",
       "      <td>lkS0tx-RWnzvBQ1lipatXQ</td>\n",
       "      <td>CrIWqmuO2uQWwl3z11K_BA</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-10-08</td>\n",
       "      <td>[like, pizza, told, friend, place, nothing, fa...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sxWZklgHXb7ovVndf1iNew</td>\n",
       "      <td>Hh3_5hP9KsPvtn6eDHPNPw</td>\n",
       "      <td>k0JaQ2w3NIgUweASFor0lg</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-07-16</td>\n",
       "      <td>[relaxed, fun, patient, great, experience, kid...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>d1WXoi0JgoNtrjLCO1xgRw</td>\n",
       "      <td>EgBYA6D6g4SrA6WNbbOtGg</td>\n",
       "      <td>ujHiaprwCQ5ewziu0Vi9rw</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>[go, buffet, around, pm, saturday, could, not,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>j1KJJv7K7LI9b3i6NDAT8A</td>\n",
       "      <td>CqdvpF3YXkWhEjL69qpPvQ</td>\n",
       "      <td>wk3D48R3Brvhci3gYcA7nw</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-06-19</td>\n",
       "      <td>[hand, best, hvac, experience, ever, team, inc...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vQBB1Ehm8jDvTlTjp7Ol3Q</td>\n",
       "      <td>5Lvl0F_0Q3PS54xi-0fURA</td>\n",
       "      <td>e8ZuGeCuJARJZnd4SERG8w</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-08-25</td>\n",
       "      <td>[mixed, review, regular, salon, close, sunday,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ObH3YRz3GfR-8EPF4lxVEw</td>\n",
       "      <td>GiFcYCH6jgs--_Dwmk2JpA</td>\n",
       "      <td>-PJgXks184iZSA9ueAHe_A</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-28</td>\n",
       "      <td>[staff, friendly, visit, quick, get, bill, tho...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4Jnyy9DbUCKK1F_MmFUomA</td>\n",
       "      <td>ZE2_VhRyEfu5MyuEO2kwsw</td>\n",
       "      <td>nsNONDHbV7Vudqh21uicqw</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-11-30</td>\n",
       "      <td>[husband, go, salt, cellar, date, go, back, la...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>T_CxiuE4jGvQyTPwVzHE1w</td>\n",
       "      <td>PMlHqaPeSlnCjejxlFPfcg</td>\n",
       "      <td>096iGHoQ-UImxUExuyqlZA</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-12-03</td>\n",
       "      <td>[great, food, great, people, everything, cooke...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Cplw3vV7qihZxEs8LTVSHQ</td>\n",
       "      <td>l6Cmcou-ZmABUBTLwhs7vQ</td>\n",
       "      <td>SVGApDPNdpFlEjwRQThCxA</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-08-25</td>\n",
       "      <td>[star, service, food, value, hand, favorite, m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>U-CZvx5gY0iXiuQB9Q7QHg</td>\n",
       "      <td>hbF8TACKA2WsyBPdqtYGTQ</td>\n",
       "      <td>9edPSkfXKsJmkZYIaOmA7Q</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-08-15</td>\n",
       "      <td>[pretty, big, outlet, happy, selection, store,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2bmwY8f5L14F8h5MLU3aqw</td>\n",
       "      <td>PV0VEhlmhc2BjzMZEbpWlg</td>\n",
       "      <td>YILyHegzhy1vlc_LNVfObw</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>[great, restaurant, great, food, great, servic...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>qaVQ0L6JnAzRryOPZnaKpQ</td>\n",
       "      <td>wVUX76TAeTYBvf7R6_AiHw</td>\n",
       "      <td>cS7RpQt0MAUlcheCGo3iXw</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-12-04</td>\n",
       "      <td>[short, bike, don't, last, month, normal, wear...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>frmT578YO-ImGFG27DzCeA</td>\n",
       "      <td>lyYofpGnyMhLtuXboEVcWg</td>\n",
       "      <td>KjcL7wuKLj0brSv8YuDdzg</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-05-12</td>\n",
       "      <td>[banking, nsb, year, not, major, issue, new, f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>jhddOLNUxjM8qsN72Ptt3g</td>\n",
       "      <td>RV5u-WbaZd-sSpGPga0Wwg</td>\n",
       "      <td>fptA-55RhpECH-zZJUbBPw</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-23</td>\n",
       "      <td>[go, place, groupon, not, come, back, gel, chi...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Txt_WQoJvIw1N6YvUfqEbQ</td>\n",
       "      <td>Cp6WNjA9iV-NCAVv3Pa3KA</td>\n",
       "      <td>thlAnPN1ApoNxSnok_fcvA</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-23</td>\n",
       "      <td>[recently, move, avondale, glendale, always, l...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wezD0t9KgjOWF6-BOzYolw</td>\n",
       "      <td>zp2KELPMbOiiMMuU73qL5w</td>\n",
       "      <td>SZpzMUdhh1xu2qlegzSyLg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-06-21</td>\n",
       "      <td>[bad, service, go, place, tonight, even, thoug...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Z9bymM69pEOZQ7-7wDSwMw</td>\n",
       "      <td>UAlZb-ZsAOL8MvEbcSUHSQ</td>\n",
       "      <td>uC3qwaxsOkdJzpOc0v1Mog</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-05-13</td>\n",
       "      <td>[surprised, never, write, review, least, time,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>QG9JWutO2gV-bKjqf1ya3Q</td>\n",
       "      <td>8ulm6mzJy4rH06ojjePwxw</td>\n",
       "      <td>fC367v5P1BRSo0DgnpQmJQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-02-19</td>\n",
       "      <td>[stop, coffee, breakfast, wait, area, sat, coz...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>_vJA9rPK9OHFZAqZoZNNqw</td>\n",
       "      <td>KbaLzIBhLJZQ1HJbQXtKSw</td>\n",
       "      <td>XXW_OFaYQkkGOGniujZFHg</td>\n",
       "      <td>3</td>\n",
       "      <td>2009-08-04</td>\n",
       "      <td>[pretty, decent, brunch, maserati, omelet, not...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mCt_gbWKdghvp-Ad8Ky11A</td>\n",
       "      <td>Jx3dP4kkKonDSrWCRH_oMw</td>\n",
       "      <td>_wvUsnWiRZNS1uBD2Rmn_A</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-05-29</td>\n",
       "      <td>[really, want, like, baby, hip, live, street, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>aeCgHor3E5X3HKfFid46zQ</td>\n",
       "      <td>UuX0pnKVtdFwzXe_8p_7NQ</td>\n",
       "      <td>EAs61Wm1O6tLjCs8t2eP-Q</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-11-19</td>\n",
       "      <td>[far, best, chinese, arizona, really, clean, p...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Do3_N2PqbYGdKqyW3OKnOA</td>\n",
       "      <td>8lP-zqaxDUzHvlt5W408kA</td>\n",
       "      <td>ytWZyS6Ol9bgKCJdXG2ugQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>[found, guy, yelp, call, due, review, wonderfu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ywu1_eJV9kWo3qpGeXnZWw</td>\n",
       "      <td>yE44pO8DD6z2CXB5Zl1tsQ</td>\n",
       "      <td>69snoDreEoaYtlwOQUzSbg</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-02-24</td>\n",
       "      <td>[go, long, time, blanca, best, know, color, st...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6Gz-vZhdcStVputQ7LZPWA</td>\n",
       "      <td>NqF7yKnngpOglCHcys57SQ</td>\n",
       "      <td>ncXQtqJT5Gk1QztwTrBrgw</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>[come, breakfast, julie, waitress, excellent, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>VjXgXZI5aitmEqeLEcPaNg</td>\n",
       "      <td>G_uxSslzefy1gxbdARwg6A</td>\n",
       "      <td>3aRt8JPhjkfvbhqb3kJsbg</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-02-06</td>\n",
       "      <td>[come, year, howard, chris, nice, helpful, eve...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8cU6Kw1dj-DBuMHemTY1YQ</td>\n",
       "      <td>rhVALOZrwbppSQkzmhDiuQ</td>\n",
       "      <td>Y3RWEmu4ykuc2LvG3Qv9Ew</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-05-26</td>\n",
       "      <td>[california, dreamin, pizza, meatball, calzone...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>WF3kKjWAytB04orXaLoubA</td>\n",
       "      <td>Y-aeqCKNf7RQvVBZw6oCkQ</td>\n",
       "      <td>2c9Vptks_vowLgVUMnCgjw</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>[hate, country, say, love, bar, live, music, g...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>0P4k4zHfTHxBGusVvx_Plg</td>\n",
       "      <td>ybR77UCn0wfo5OyhhfH0ZQ</td>\n",
       "      <td>WBdmaZzhrcumB2_hsQwaNQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-02-24</td>\n",
       "      <td>[drop, vega, vacation, los, angeles, take, wal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>eiF3F2neNXUZkjvnwIeCvQ</td>\n",
       "      <td>czFj9Ql2_JGIniJhKsPQ0Q</td>\n",
       "      <td>_CF9vpN1F9KcAkXbOIkL8w</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-02-01</td>\n",
       "      <td>[nice, change, taste, area, stuffed, mexican, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>8lpKV020ycX4D-XkmLaMQA</td>\n",
       "      <td>8nP56hi2PQIoFtlh2Hjiew</td>\n",
       "      <td>cMAv53bpH7b32IYgQwCXLA</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>[drink, really, delicious, food, well, good, b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>jcBx2cZ-Kj28vSXkJq2EKA</td>\n",
       "      <td>YHu9ir0kjDYeFHaIEBO3Mg</td>\n",
       "      <td>f2ZWZPENViL92BrFsIgR6w</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-03-04</td>\n",
       "      <td>[heather, not, smart, personable, amaze, color...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>0XK_lHuGmOLaz-FFmHGmbA</td>\n",
       "      <td>DBHCFW3mSmmOEpONHVu1rQ</td>\n",
       "      <td>LzT4mS0SJcCrerKPdza1ag</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-05-12</td>\n",
       "      <td>[get, class, groupon, try, class, always, susp...</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>LP0obTQUMjPmE6Z3nc6ttA</td>\n",
       "      <td>wdCwBv_TA_Y-IjUZv2HWWQ</td>\n",
       "      <td>3fw2X5bZYeW9xCz_zGhOHg</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-01-17</td>\n",
       "      <td>[come, friday, store, say, would, open, pm, ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>d140A6j0mcXRxyqcyqHhrw</td>\n",
       "      <td>S55kaMq-W2YpZH_dPGdesA</td>\n",
       "      <td>DVfCbJhJUDWRlUfrKzaKOA</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-04-01</td>\n",
       "      <td>[would, give, star, poor, quality, bbq, dish, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>yMDul2EsE_tXn8sOsksAmw</td>\n",
       "      <td>TaVGX_tjR7_KTy3kbN7fnQ</td>\n",
       "      <td>zvQIEpJUmLLmMMffNntHXQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-28</td>\n",
       "      <td>[picked, pizza, last, night, cheese, pizza, re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>Heu9Lu90U38oNr0WkRZUaA</td>\n",
       "      <td>AdZLIaL44HWE7YfZFLd1MA</td>\n",
       "      <td>0yHY4FHCTOIshPiPc9EYBQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-11-06</td>\n",
       "      <td>[fast, clean, staff, super, friendly, veggie, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>Iesa46w8eXfkxdr0q6GESg</td>\n",
       "      <td>zcxzTxxwOrsyYgmB5C1tdg</td>\n",
       "      <td>bDUvTOwviImDUIY7EERCgA</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-11-06</td>\n",
       "      <td>[gym, great, really, good, variety, machine, c...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>Pp9CrLijLHjvu1TDzJ9Tgw</td>\n",
       "      <td>UC0sSFpYvPdj4BVJ5LwBUg</td>\n",
       "      <td>sZTxJMS0co1D1k8aZCukBg</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-12-18</td>\n",
       "      <td>[worst, customer, service, ever, receive, purc...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>ztmi78V1rUWBtyqx_Cc2XQ</td>\n",
       "      <td>nDqgYYjuTaE4dEiwAw7NRw</td>\n",
       "      <td>q4i-GbjeSp6ucBDXGE9DqA</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-06-17</td>\n",
       "      <td>[love, pad, thai, right, amount, spicy, sweet,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>LRv3-AiiNvpLwZsWR9NFPw</td>\n",
       "      <td>ON6OwLnffcJJhsOFxJBicQ</td>\n",
       "      <td>b1b1eb3uo-w561D0ZfCEiQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-06-19</td>\n",
       "      <td>[spot, get, pearly, white, look, like, million...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>pdyJYKlLMHEOHP4ljVgzpg</td>\n",
       "      <td>OHjATn0dlAweRy_Ono3a3Q</td>\n",
       "      <td>nlwomMElt7sDwF_njfEEIw</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-05-09</td>\n",
       "      <td>[best, teppanyaki, food, amaze, great, service...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>cqGnDFz4o_704UO47lps2w</td>\n",
       "      <td>m1-HNN4pghUqDIKr2FUcmw</td>\n",
       "      <td>OIhwfK2wTtK23Bp1rtMfDQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-06-21</td>\n",
       "      <td>[ac, go, end, last, summer, temperature, get, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>yQcLftqBTh3tglNzvdToPw</td>\n",
       "      <td>JxxNT5BFP_8dEgKE4mSHcA</td>\n",
       "      <td>kWo972Yj9G41LpNOu7wVjA</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-07-29</td>\n",
       "      <td>[love, team, great, really, nice, knowledgeabl...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>jNho-uVOjqwz3aY89LKrdQ</td>\n",
       "      <td>a8q4IZiolvV0RtEV37_N9Q</td>\n",
       "      <td>kfwUp_dHWKpRdQWucy0ykA</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-05-27</td>\n",
       "      <td>[friend, town, memorial, day, weekend, bachelo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>imPNUJy5zUKv8g6vU69dgQ</td>\n",
       "      <td>oAil_gYOXUhqWnKA8SwKsw</td>\n",
       "      <td>Crq6YLYcGGxS9aP4-iVyGA</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-05-27</td>\n",
       "      <td>[go, see, first, opera, cunning, little, vixen...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>a3pdX3IptIAXgidogpVSGw</td>\n",
       "      <td>dYa9nlm8i6vxs8Iqo7E4mw</td>\n",
       "      <td>PuB-oSX_fMh0822kChScrA</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-07</td>\n",
       "      <td>[far, worst, service, ever, establishment, cus...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>3TSktbCMrLs6peeQZRb98Q</td>\n",
       "      <td>ZxGXQES-SIStZjVFbz2aSQ</td>\n",
       "      <td>M51gw2cz_vXarNBLbKkOxQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-07-04</td>\n",
       "      <td>[ok, since, go, place, breakfast, lenient, mis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>Ex3Xi9nesJouULOB8GItvg</td>\n",
       "      <td>vQ5CxUM__WWwFK6PNHt4_A</td>\n",
       "      <td>XqeZKk07p2Nr76EyM9EOuw</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-10-26</td>\n",
       "      <td>[place, older, yes, need, update, yes, gel, ac...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>jT62S8MOW-KVkkS1y2asAA</td>\n",
       "      <td>g8gn7aJCRDg0bDERU_usvg</td>\n",
       "      <td>cghHesD2NXV5yUu-ZAxE2g</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-11-10</td>\n",
       "      <td>[not, impressed, husband, like, eat, place, ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>0DVGZzZC0EFAvcawfKZO9Q</td>\n",
       "      <td>Aa_gN5ytXmEDGfa1ZJa9OQ</td>\n",
       "      <td>D5aSQhj3uyX4Fj1B31EYRQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-10-15</td>\n",
       "      <td>[first, time, arrowhead, work, jeff, great, de...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Q52uh4phLG0ZA88aRDBMRg</td>\n",
       "      <td>SUnyBS7HB3Nv_C78ygbpnQ</td>\n",
       "      <td>DUThUkWiQBT1VLZdXZvVHA</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>[get, first, shellac, nail, art, know, way, la...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>dZczcpbiwnUGDpa9yCiyIQ</td>\n",
       "      <td>dfgPj43FVRnKIfxjLclqTQ</td>\n",
       "      <td>gkxmQi2cipbysXFgT_w_jA</td>\n",
       "      <td>4</td>\n",
       "      <td>2014-09-26</td>\n",
       "      <td>[go, location, avenue, mont, royal, est, great...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>PBtqfsGdwexwV9gLaB_s4Q</td>\n",
       "      <td>ci06dTB7I2CvzDMX_QMSxw</td>\n",
       "      <td>0qSKZhVC_BHPuKro4QGWJw</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>[best, steak, house, eat, vega, maybe, anywher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>cp0092nze6JTFR6QP_zqog</td>\n",
       "      <td>5MVhYyCIMMpJTrPBx11_qg</td>\n",
       "      <td>LHXisknIbUy_XtdEQc7x9w</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-10-11</td>\n",
       "      <td>[horrible, experience, company, hour, ago, la,...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>W518eGBICDIu9qcbW9vUzQ</td>\n",
       "      <td>ASLIke5kbn7Z95wT_otaUQ</td>\n",
       "      <td>diaiQrxYFU1V5qxrFnW9fg</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-09-11</td>\n",
       "      <td>[fast, efficient, shuttle, quick, take, destin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Rl09Nacb4YdfcCYpD3Pz4Q</td>\n",
       "      <td>s3a1I7rOgkQyzg6_Kfjmgg</td>\n",
       "      <td>kLKtlA3s4Z6wPefSLKDHmA</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-04-12</td>\n",
       "      <td>[new, patient, office, jessica, gdovin, pa, mo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  review_id                 user_id             business_id  \\\n",
       "0    fomc2nZfjK6Se9v-Ba5oWQ  bBQcAreoMmfd8LICFGWjtw  coHUUrQSdWpiFCfF2ayFNA   \n",
       "1    seHlbqQhMNqCQQPb7drsRQ  D_RjBQlUppSLIJhSgvf1xg  CGpC_L37PBnIbs8TsuBEpA   \n",
       "2    SwIV2t-U0QhF4GnHmPXcWw  P34rd7qs2LhNa1K0VYuVJA  wJVW-Bgc9j2aaMnwI7TH3Q   \n",
       "3    eJXmg_cHbofsjKwgcSpPXw  iLZ8eMMafDrNnRqI5164-Q  thlAnPN1ApoNxSnok_fcvA   \n",
       "4    tI2xPNMV9N2Z6ucMPV6vUQ  lkS0tx-RWnzvBQ1lipatXQ  CrIWqmuO2uQWwl3z11K_BA   \n",
       "5    sxWZklgHXb7ovVndf1iNew  Hh3_5hP9KsPvtn6eDHPNPw  k0JaQ2w3NIgUweASFor0lg   \n",
       "6    d1WXoi0JgoNtrjLCO1xgRw  EgBYA6D6g4SrA6WNbbOtGg  ujHiaprwCQ5ewziu0Vi9rw   \n",
       "7    j1KJJv7K7LI9b3i6NDAT8A  CqdvpF3YXkWhEjL69qpPvQ  wk3D48R3Brvhci3gYcA7nw   \n",
       "8    vQBB1Ehm8jDvTlTjp7Ol3Q  5Lvl0F_0Q3PS54xi-0fURA  e8ZuGeCuJARJZnd4SERG8w   \n",
       "9    ObH3YRz3GfR-8EPF4lxVEw  GiFcYCH6jgs--_Dwmk2JpA  -PJgXks184iZSA9ueAHe_A   \n",
       "10   4Jnyy9DbUCKK1F_MmFUomA  ZE2_VhRyEfu5MyuEO2kwsw  nsNONDHbV7Vudqh21uicqw   \n",
       "11   T_CxiuE4jGvQyTPwVzHE1w  PMlHqaPeSlnCjejxlFPfcg  096iGHoQ-UImxUExuyqlZA   \n",
       "12   Cplw3vV7qihZxEs8LTVSHQ  l6Cmcou-ZmABUBTLwhs7vQ  SVGApDPNdpFlEjwRQThCxA   \n",
       "13   U-CZvx5gY0iXiuQB9Q7QHg  hbF8TACKA2WsyBPdqtYGTQ  9edPSkfXKsJmkZYIaOmA7Q   \n",
       "14   2bmwY8f5L14F8h5MLU3aqw  PV0VEhlmhc2BjzMZEbpWlg  YILyHegzhy1vlc_LNVfObw   \n",
       "15   qaVQ0L6JnAzRryOPZnaKpQ  wVUX76TAeTYBvf7R6_AiHw  cS7RpQt0MAUlcheCGo3iXw   \n",
       "16   frmT578YO-ImGFG27DzCeA  lyYofpGnyMhLtuXboEVcWg  KjcL7wuKLj0brSv8YuDdzg   \n",
       "17   jhddOLNUxjM8qsN72Ptt3g  RV5u-WbaZd-sSpGPga0Wwg  fptA-55RhpECH-zZJUbBPw   \n",
       "18   Txt_WQoJvIw1N6YvUfqEbQ  Cp6WNjA9iV-NCAVv3Pa3KA  thlAnPN1ApoNxSnok_fcvA   \n",
       "19   wezD0t9KgjOWF6-BOzYolw  zp2KELPMbOiiMMuU73qL5w  SZpzMUdhh1xu2qlegzSyLg   \n",
       "20   Z9bymM69pEOZQ7-7wDSwMw  UAlZb-ZsAOL8MvEbcSUHSQ  uC3qwaxsOkdJzpOc0v1Mog   \n",
       "21   QG9JWutO2gV-bKjqf1ya3Q  8ulm6mzJy4rH06ojjePwxw  fC367v5P1BRSo0DgnpQmJQ   \n",
       "22   _vJA9rPK9OHFZAqZoZNNqw  KbaLzIBhLJZQ1HJbQXtKSw  XXW_OFaYQkkGOGniujZFHg   \n",
       "23   mCt_gbWKdghvp-Ad8Ky11A  Jx3dP4kkKonDSrWCRH_oMw  _wvUsnWiRZNS1uBD2Rmn_A   \n",
       "24   aeCgHor3E5X3HKfFid46zQ  UuX0pnKVtdFwzXe_8p_7NQ  EAs61Wm1O6tLjCs8t2eP-Q   \n",
       "25   Do3_N2PqbYGdKqyW3OKnOA  8lP-zqaxDUzHvlt5W408kA  ytWZyS6Ol9bgKCJdXG2ugQ   \n",
       "26   Ywu1_eJV9kWo3qpGeXnZWw  yE44pO8DD6z2CXB5Zl1tsQ  69snoDreEoaYtlwOQUzSbg   \n",
       "27   6Gz-vZhdcStVputQ7LZPWA  NqF7yKnngpOglCHcys57SQ  ncXQtqJT5Gk1QztwTrBrgw   \n",
       "28   VjXgXZI5aitmEqeLEcPaNg  G_uxSslzefy1gxbdARwg6A  3aRt8JPhjkfvbhqb3kJsbg   \n",
       "29   8cU6Kw1dj-DBuMHemTY1YQ  rhVALOZrwbppSQkzmhDiuQ  Y3RWEmu4ykuc2LvG3Qv9Ew   \n",
       "..                      ...                     ...                     ...   \n",
       "970  WF3kKjWAytB04orXaLoubA  Y-aeqCKNf7RQvVBZw6oCkQ  2c9Vptks_vowLgVUMnCgjw   \n",
       "971  0P4k4zHfTHxBGusVvx_Plg  ybR77UCn0wfo5OyhhfH0ZQ  WBdmaZzhrcumB2_hsQwaNQ   \n",
       "972  eiF3F2neNXUZkjvnwIeCvQ  czFj9Ql2_JGIniJhKsPQ0Q  _CF9vpN1F9KcAkXbOIkL8w   \n",
       "973  8lpKV020ycX4D-XkmLaMQA  8nP56hi2PQIoFtlh2Hjiew  cMAv53bpH7b32IYgQwCXLA   \n",
       "974  jcBx2cZ-Kj28vSXkJq2EKA  YHu9ir0kjDYeFHaIEBO3Mg  f2ZWZPENViL92BrFsIgR6w   \n",
       "975  0XK_lHuGmOLaz-FFmHGmbA  DBHCFW3mSmmOEpONHVu1rQ  LzT4mS0SJcCrerKPdza1ag   \n",
       "976  LP0obTQUMjPmE6Z3nc6ttA  wdCwBv_TA_Y-IjUZv2HWWQ  3fw2X5bZYeW9xCz_zGhOHg   \n",
       "977  d140A6j0mcXRxyqcyqHhrw  S55kaMq-W2YpZH_dPGdesA  DVfCbJhJUDWRlUfrKzaKOA   \n",
       "978  yMDul2EsE_tXn8sOsksAmw  TaVGX_tjR7_KTy3kbN7fnQ  zvQIEpJUmLLmMMffNntHXQ   \n",
       "979  Heu9Lu90U38oNr0WkRZUaA  AdZLIaL44HWE7YfZFLd1MA  0yHY4FHCTOIshPiPc9EYBQ   \n",
       "980  Iesa46w8eXfkxdr0q6GESg  zcxzTxxwOrsyYgmB5C1tdg  bDUvTOwviImDUIY7EERCgA   \n",
       "981  Pp9CrLijLHjvu1TDzJ9Tgw  UC0sSFpYvPdj4BVJ5LwBUg  sZTxJMS0co1D1k8aZCukBg   \n",
       "982  ztmi78V1rUWBtyqx_Cc2XQ  nDqgYYjuTaE4dEiwAw7NRw  q4i-GbjeSp6ucBDXGE9DqA   \n",
       "983  LRv3-AiiNvpLwZsWR9NFPw  ON6OwLnffcJJhsOFxJBicQ  b1b1eb3uo-w561D0ZfCEiQ   \n",
       "984  pdyJYKlLMHEOHP4ljVgzpg  OHjATn0dlAweRy_Ono3a3Q  nlwomMElt7sDwF_njfEEIw   \n",
       "985  cqGnDFz4o_704UO47lps2w  m1-HNN4pghUqDIKr2FUcmw  OIhwfK2wTtK23Bp1rtMfDQ   \n",
       "986  yQcLftqBTh3tglNzvdToPw  JxxNT5BFP_8dEgKE4mSHcA  kWo972Yj9G41LpNOu7wVjA   \n",
       "987  jNho-uVOjqwz3aY89LKrdQ  a8q4IZiolvV0RtEV37_N9Q  kfwUp_dHWKpRdQWucy0ykA   \n",
       "988  imPNUJy5zUKv8g6vU69dgQ  oAil_gYOXUhqWnKA8SwKsw  Crq6YLYcGGxS9aP4-iVyGA   \n",
       "989  a3pdX3IptIAXgidogpVSGw  dYa9nlm8i6vxs8Iqo7E4mw  PuB-oSX_fMh0822kChScrA   \n",
       "990  3TSktbCMrLs6peeQZRb98Q  ZxGXQES-SIStZjVFbz2aSQ  M51gw2cz_vXarNBLbKkOxQ   \n",
       "991  Ex3Xi9nesJouULOB8GItvg  vQ5CxUM__WWwFK6PNHt4_A  XqeZKk07p2Nr76EyM9EOuw   \n",
       "992  jT62S8MOW-KVkkS1y2asAA  g8gn7aJCRDg0bDERU_usvg  cghHesD2NXV5yUu-ZAxE2g   \n",
       "993  0DVGZzZC0EFAvcawfKZO9Q  Aa_gN5ytXmEDGfa1ZJa9OQ  D5aSQhj3uyX4Fj1B31EYRQ   \n",
       "994  Q52uh4phLG0ZA88aRDBMRg  SUnyBS7HB3Nv_C78ygbpnQ  DUThUkWiQBT1VLZdXZvVHA   \n",
       "995  dZczcpbiwnUGDpa9yCiyIQ  dfgPj43FVRnKIfxjLclqTQ  gkxmQi2cipbysXFgT_w_jA   \n",
       "996  PBtqfsGdwexwV9gLaB_s4Q  ci06dTB7I2CvzDMX_QMSxw  0qSKZhVC_BHPuKro4QGWJw   \n",
       "997  cp0092nze6JTFR6QP_zqog  5MVhYyCIMMpJTrPBx11_qg  LHXisknIbUy_XtdEQc7x9w   \n",
       "998  W518eGBICDIu9qcbW9vUzQ  ASLIke5kbn7Z95wT_otaUQ  diaiQrxYFU1V5qxrFnW9fg   \n",
       "999  Rl09Nacb4YdfcCYpD3Pz4Q  s3a1I7rOgkQyzg6_Kfjmgg  kLKtlA3s4Z6wPefSLKDHmA   \n",
       "\n",
       "     stars review_date                                        review_text  \\\n",
       "0        5  2016-01-17  [church, like, big, beautiful, oak, tree, many...   \n",
       "1        5  2014-09-21  [trek, great, line, quality, bike, entry, leve...   \n",
       "2        4  2011-04-22  [experienced, charcut, alleyburger, don't, kno...   \n",
       "3        4  2017-03-27  [first, time, restaurant, want, try, base, rec...   \n",
       "4        4  2015-10-08  [like, pizza, told, friend, place, nothing, fa...   \n",
       "5        4  2012-07-16  [relaxed, fun, patient, great, experience, kid...   \n",
       "6        1  2017-04-30  [go, buffet, around, pm, saturday, could, not,...   \n",
       "7        5  2018-06-19  [hand, best, hvac, experience, ever, team, inc...   \n",
       "8        2  2014-08-25  [mixed, review, regular, salon, close, sunday,...   \n",
       "9        1  2013-01-28  [staff, friendly, visit, quick, get, bill, tho...   \n",
       "10       4  2011-11-30  [husband, go, salt, cellar, date, go, back, la...   \n",
       "11       5  2017-12-03  [great, food, great, people, everything, cooke...   \n",
       "12       5  2016-08-25  [star, service, food, value, hand, favorite, m...   \n",
       "13       4  2012-08-15  [pretty, big, outlet, happy, selection, store,...   \n",
       "14       4  2016-09-30  [great, restaurant, great, food, great, servic...   \n",
       "15       1  2016-12-04  [short, bike, don't, last, month, normal, wear...   \n",
       "16       3  2015-05-12  [banking, nsb, year, not, major, issue, new, f...   \n",
       "17       2  2018-03-23  [go, place, groupon, not, come, back, gel, chi...   \n",
       "18       2  2018-03-23  [recently, move, avondale, glendale, always, l...   \n",
       "19       1  2015-06-21  [bad, service, go, place, tonight, even, thoug...   \n",
       "20       4  2017-05-13  [surprised, never, write, review, least, time,...   \n",
       "21       5  2017-02-19  [stop, coffee, breakfast, wait, area, sat, coz...   \n",
       "22       3  2009-08-04  [pretty, decent, brunch, maserati, omelet, not...   \n",
       "23       2  2018-05-29  [really, want, like, baby, hip, live, street, ...   \n",
       "24       5  2015-11-19  [far, best, chinese, arizona, really, clean, p...   \n",
       "25       5  2018-02-01  [found, guy, yelp, call, due, review, wonderfu...   \n",
       "26       5  2014-02-24  [go, long, time, blanca, best, know, color, st...   \n",
       "27       5  2018-01-21  [come, breakfast, julie, waitress, excellent, ...   \n",
       "28       5  2014-02-06  [come, year, howard, chris, nice, helpful, eve...   \n",
       "29       5  2015-05-26  [california, dreamin, pizza, meatball, calzone...   \n",
       "..     ...         ...                                                ...   \n",
       "970      4  2015-01-18  [hate, country, say, love, bar, live, music, g...   \n",
       "971      5  2018-02-24  [drop, vega, vacation, los, angeles, take, wal...   \n",
       "972      4  2012-02-01  [nice, change, taste, area, stuffed, mexican, ...   \n",
       "973      4  2018-09-03  [drink, really, delicious, food, well, good, b...   \n",
       "974      4  2017-03-04  [heather, not, smart, personable, amaze, color...   \n",
       "975      5  2015-05-12  [get, class, groupon, try, class, always, susp...   \n",
       "976      1  2014-01-17  [come, friday, store, say, would, open, pm, ho...   \n",
       "977      2  2014-04-01  [would, give, star, poor, quality, bbq, dish, ...   \n",
       "978      5  2015-07-28  [picked, pizza, last, night, cheese, pizza, re...   \n",
       "979      5  2015-11-06  [fast, clean, staff, super, friendly, veggie, ...   \n",
       "980      2  2012-11-06  [gym, great, really, good, variety, machine, c...   \n",
       "981      1  2014-12-18  [worst, customer, service, ever, receive, purc...   \n",
       "982      5  2017-06-17  [love, pad, thai, right, amount, spicy, sweet,...   \n",
       "983      5  2018-06-19  [spot, get, pearly, white, look, like, million...   \n",
       "984      5  2018-05-09  [best, teppanyaki, food, amaze, great, service...   \n",
       "985      5  2015-06-21  [ac, go, end, last, summer, temperature, get, ...   \n",
       "986      5  2017-07-29  [love, team, great, really, nice, knowledgeabl...   \n",
       "987      5  2015-05-27  [friend, town, memorial, day, weekend, bachelo...   \n",
       "988      3  2014-05-27  [go, see, first, opera, cunning, little, vixen...   \n",
       "989      1  2017-11-07  [far, worst, service, ever, establishment, cus...   \n",
       "990      1  2015-07-04  [ok, since, go, place, breakfast, lenient, mis...   \n",
       "991      5  2016-10-26  [place, older, yes, need, update, yes, gel, ac...   \n",
       "992      1  2016-11-10  [not, impressed, husband, like, eat, place, ha...   \n",
       "993      5  2013-10-15  [first, time, arrowhead, work, jeff, great, de...   \n",
       "994      5  2016-10-01  [get, first, shellac, nail, art, know, way, la...   \n",
       "995      4  2014-09-26  [go, location, avenue, mont, royal, est, great...   \n",
       "996      5  2017-04-08  [best, steak, house, eat, vega, maybe, anywher...   \n",
       "997      1  2014-10-11  [horrible, experience, company, hour, ago, la,...   \n",
       "998      5  2012-09-11  [fast, efficient, shuttle, quick, take, destin...   \n",
       "999      5  2016-04-12  [new, patient, office, jessica, gdovin, pa, mo...   \n",
       "\n",
       "     useful  funny  cool  \n",
       "0         2      0     0  \n",
       "1         0      0     0  \n",
       "2         0      0     0  \n",
       "3         0      0     0  \n",
       "4         1      0     0  \n",
       "5         2      0     0  \n",
       "6         0      0     0  \n",
       "7         1      1     1  \n",
       "8         0      0     0  \n",
       "9         2      0     0  \n",
       "10        0      0     1  \n",
       "11        0      0     1  \n",
       "12        1      0     0  \n",
       "13        0      0     0  \n",
       "14        2      2     1  \n",
       "15        3      0     0  \n",
       "16        1      0     0  \n",
       "17        2      0     0  \n",
       "18        2      1     1  \n",
       "19        0      0     0  \n",
       "20        1      0     1  \n",
       "21        0      0     1  \n",
       "22        0      0     0  \n",
       "23        1      0     0  \n",
       "24        3      2     1  \n",
       "25        1      0     0  \n",
       "26        0      0     0  \n",
       "27        0      0     0  \n",
       "28        4      1     1  \n",
       "29        0      0     1  \n",
       "..      ...    ...   ...  \n",
       "970       1      1     1  \n",
       "971       0      0     0  \n",
       "972       1      0     0  \n",
       "973       0      0     0  \n",
       "974       0      0     0  \n",
       "975      14      6    11  \n",
       "976       1      1     0  \n",
       "977       1      0     0  \n",
       "978       0      0     0  \n",
       "979       1      0     0  \n",
       "980       3      0     0  \n",
       "981       2      1     0  \n",
       "982       0      0     0  \n",
       "983       0      0     0  \n",
       "984       0      0     0  \n",
       "985       3      0     0  \n",
       "986       3      1     1  \n",
       "987       0      0     0  \n",
       "988       0      0     0  \n",
       "989       0      0     0  \n",
       "990       0      0     1  \n",
       "991       0      0     0  \n",
       "992       1      3     1  \n",
       "993       0      0     0  \n",
       "994       0      0     0  \n",
       "995       0      0     0  \n",
       "996       0      0     0  \n",
       "997       5      1     1  \n",
       "998       0      0     0  \n",
       "999       2      0     0  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_on_column(review_sample, _process_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell below creates a class that will be used in the vectorization code a few cells down. FYI, path in \"_load_words\" is machine specific. Also, you have to download the GloVe training set from GloVe website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MeanEmbeddingTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._vocab, self._E = self._load_words()\n",
    "        \n",
    "    \n",
    "    def _load_words(self):\n",
    "        E = {}\n",
    "        vocab = []\n",
    "\n",
    "        with open(r'C:\\Users\\Mark\\YelpProject\\poop\\glove.6B.50d.txt', encoding=\"utf8\") as file:\n",
    "            for i, line in enumerate(file):\n",
    "                l = line.split(' ')\n",
    "                if l[0].isalpha():\n",
    "                    v = [float(i) for i in l[1:]]\n",
    "                    E[l[0]] = numpy.array(v)\n",
    "                    vocab.append(l[0])\n",
    "        return numpy.array(vocab), E            \n",
    "\n",
    "    \n",
    "    def _get_word(self, v):\n",
    "        for i, emb in enumerate(self._E):\n",
    "            if numpy.array_equal(emb, v):\n",
    "                return self._vocab[i]\n",
    "        return None\n",
    "    #in _doc_mean Ithink maybe we should get rid of w.lower.strip\n",
    "    def _doc_mean(self, doc):\n",
    "        return numpy.mean(numpy.array([self._E[w.lower().strip()] for w in doc if w.lower().strip() in self._E]), axis=0)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return numpy.array([self._doc_mean(doc) for doc in X])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    \n",
    "#retrived from https://www.kaggle.com/nhrade/text-classification-using-word-embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell below is attempting to create a class for gensim doc2vec but it's not working yet and tbh I'm not sure how close I am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import sparse2full\n",
    "\n",
    "class GensimDoc2Vec(TransformerMixin):\n",
    "    def __init__(self, path=None):\n",
    "        self.path = path\n",
    "        self.id2word = None\n",
    "        self.load()\n",
    "    \n",
    "    def load(self):\n",
    "        if os.path.exists(self.path):\n",
    "            self.id2word = Dictionary.load(self.path)\n",
    "    \n",
    "    def save(self):\n",
    "        self.id2word.save(self.path)\n",
    "    \n",
    "    def fit(self, documents, labels = None):\n",
    "        self.id2word = Dictionary(documents)\n",
    "        self.save()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            docvec = self.id2word.doc2vec\n",
    "            yield sparse2full(docvec, len(self.id2word))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GloVeMeanEmbedding(text):\n",
    "    met = MeanEmbeddingTransformer()\n",
    "    GloVeVector = numpy.array(met.fit_transform(map_on_column(text, _clean_review2)))\n",
    "    return GloVeVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenDoc2Vec(text, path):\n",
    "    met = GensimDoc2Vec()\n",
    "    GenVector = numpy.array(met.transform(map_on_column(text, _clean_review2)))\n",
    "    return GenVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell below is vectorization. So far it can do TFIDF on specified number of n-grams, and I think the part with GloVe word embedding is almost functioning, but I haven't started \"word2vec\" vectorization yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "#def GloVeDistributedRep(text):\n",
    "    ##GloVeVector = numpy.append(MeanEmbeddingTransformer().fit_transform(map_on_column(text, _clean_review2))\n",
    "    #return GloVeVector\n",
    "\n",
    "\n",
    "def vectorize(text, method, grams):\n",
    "    if method == 'TFIDF':\n",
    "        tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None, ngram_range = (1,grams))\n",
    "        TDIFvectors = tfidf.fit_transform(map_on_column(text, _clean_review2))\n",
    "        print(TDIFvectors)\n",
    "    if method == 'DistributedRep':\n",
    "        GloVevectors = GloVeDistributedRep(map_on_column(text, _clean_review2))\n",
    "        print(GloVevectors)\n",
    "    if method == 'Word2Vec':\n",
    "        print('not yet defined')\n",
    "    else:\n",
    "        print('choose vectorization method')\n",
    "        \n",
    "\n",
    "def vectorize2(text, method, grams):\n",
    "    if method == 'TFIDF':\n",
    "        tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None, ngram_range = (1,grams))\n",
    "        TDIFvectors = tfidf.fit_transform(text)\n",
    "        print(TDIFvectors)\n",
    "    if method == 'DistributedRep':\n",
    "        GloVevectors = GloVeDistributedRep(text)\n",
    "        print(GloVevectors)\n",
    "    if method == 'Word2Vec':\n",
    "        print('not yet defined')\n",
    "    else:\n",
    "        print('choose vectorization method')\n",
    "        \n",
    "        \n",
    "        \n",
    "def vectorize3(text, method, grams):\n",
    "    if method == 'TFIDF':\n",
    "        tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None, ngram_range = (1,grams))\n",
    "        TDIFvectors = tfidf.fit_transform(text)\n",
    "        print(TDIFvectors)\n",
    "    if method == 'DistributedRep':\n",
    "        GloVevectors = GloVeDistributedRep(text)\n",
    "        print(GloVevectors)\n",
    "    if method == 'Word2Vec':\n",
    "        print('not yet defined')\n",
    "    else:\n",
    "        print('choose vectorization method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nltk.corpus\n",
    "from nltk.text import TextCollection\n",
    "\n",
    "def TFIDFvectorize(corpus):\n",
    "    corpus = [_clean_review2(doc) for doc in corpus]\n",
    "    texts = TextCollection(corpus)\n",
    "    \n",
    "    for doc in corpus:\n",
    "        return {\n",
    "            term: texts.tf_idf(term, doc)\n",
    "            for term in doc\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# below is first attempt at pipeline. Just as a heads up, some models can't handel negative values and so can't be used with word embedding vectorization. Baysian, for example, can only handle positive vectors and so can be used with TF-IDF but not GloVe word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#the Vectorization_method arugment will be one of the classes that we wrote. That argument itself will have argumens within it.\n",
    "def create_pipeline(documents, estimator, vectorization_method, ngrams, c=10000, reduction=False):\n",
    "    steps = [\n",
    "        ('vectorize', vectorization_method) \n",
    "    ]\n",
    "    \n",
    "    if reduction:\n",
    "        steps.append((\n",
    "        'reduction', TruncatedSVD(n_components=c)\n",
    "        ))\n",
    "    \n",
    "    steps.append(('classifier', estimator))\n",
    "    return Pipeline(steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorize', <__main__.MeanEmbeddingTransformer object at 0x0000025DF4D27358>), ('classifier', <class 'sklearn.naive_bayes.MultinomialNB'>)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this line of code is basically just filler to make sure the pipeline is being used correct. It's functioning output should just be a list of the pipline steps.\n",
    "create_pipeline(review_sample['review_text'], MultinomialNB, MeanEmbeddingTransformer(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rn the models are just using the default hyperparameters (for exxample, n_neighbors = 5. I'm not sure how to change this. I get an error when I try to change it here.\n",
    "models = []\n",
    "for form in (SGDClassifier, KNeighborsClassifier, LogisticRegression, LinearSVC, RandomForestClassifier):\n",
    "    models.append(create_pipeline(review_sample['review_text'],form(), MeanEmbeddingTransformer(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    model.fit(review_sample['review_text'], review_sample['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('vectorize', <__main__.MeanEmbeddingTransformer object at 0x0000025DB45F5E10>), ('classifier', SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', ma...m_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False))])\n",
      "Accuracy of Pipeline(memory=None,\n",
      "     steps=[('vectorize', <__main__.MeanEmbeddingTransformer object at 0x0000025DB45F5E10>), ('classifier', SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', ma...m_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False))]) is 0.403000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.13      0.22       125\n",
      "           2       0.17      0.58      0.26        73\n",
      "           3       0.25      0.01      0.02       124\n",
      "           4       0.33      0.76      0.46       225\n",
      "           5       0.85      0.38      0.52       453\n",
      "\n",
      "   micro avg       0.40      0.40      0.40      1000\n",
      "   macro avg       0.49      0.37      0.30      1000\n",
      "weighted avg       0.61      0.40      0.39      1000\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('vectorize', <__main__.MeanEmbeddingTransformer object at 0x0000025DB45F5DA0>), ('classifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of Pipeline(memory=None,\n",
      "     steps=[('vectorize', <__main__.MeanEmbeddingTransformer object at 0x0000025DB45F5DA0>), ('classifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'))]) is 0.610000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.70      0.62       125\n",
      "           2       0.49      0.42      0.46        73\n",
      "           3       0.56      0.40      0.47       124\n",
      "           4       0.49      0.68      0.57       225\n",
      "           5       0.76      0.64      0.69       453\n",
      "\n",
      "   micro avg       0.61      0.61      0.61      1000\n",
      "   macro avg       0.57      0.57      0.56      1000\n",
      "weighted avg       0.63      0.61      0.61      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('vectorize', <__main__.MeanEmbeddingTransformer object at 0x0000025D956EC7B8>), ('classifier', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))])\n",
      "Accuracy of Pipeline(memory=None,\n",
      "     steps=[('vectorize', <__main__.MeanEmbeddingTransformer object at 0x0000025D956EC7B8>), ('classifier', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))]) is 0.578000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.70      0.66       125\n",
      "           2       0.00      0.00      0.00        73\n",
      "           3       0.53      0.15      0.24       124\n",
      "           4       0.48      0.23      0.31       225\n",
      "           5       0.59      0.93      0.72       453\n",
      "\n",
      "   micro avg       0.58      0.58      0.58      1000\n",
      "   macro avg       0.44      0.40      0.39      1000\n",
      "weighted avg       0.52      0.58      0.51      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('vectorize', <__main__.MeanEmbeddingTransformer object at 0x0000025DA4DAB208>), ('classifier', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))])\n",
      "Accuracy of Pipeline(memory=None,\n",
      "     steps=[('vectorize', <__main__.MeanEmbeddingTransformer object at 0x0000025DA4DAB208>), ('classifier', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))]) is 0.588000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.74      0.65       125\n",
      "           2       0.50      0.07      0.12        73\n",
      "           3       0.51      0.19      0.28       124\n",
      "           4       0.48      0.28      0.36       225\n",
      "           5       0.62      0.89      0.73       453\n",
      "\n",
      "   micro avg       0.59      0.59      0.59      1000\n",
      "   macro avg       0.54      0.44      0.43      1000\n",
      "weighted avg       0.56      0.59      0.54      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('vectorize', <__main__.MeanEmbeddingTransformer object at 0x0000025DCB7DC2B0>), ('classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity...obs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "Accuracy of Pipeline(memory=None,\n",
      "     steps=[('vectorize', <__main__.MeanEmbeddingTransformer object at 0x0000025DCB7DC2B0>), ('classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity...obs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))]) is 0.991000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      0.98      0.99       125\n",
      "           2       1.00      0.99      0.99        73\n",
      "           3       1.00      0.98      0.99       124\n",
      "           4       0.99      1.00      0.99       225\n",
      "           5       0.99      0.99      0.99       453\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1000\n",
      "   macro avg       0.99      0.99      0.99      1000\n",
      "weighted avg       0.99      0.99      0.99      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#really annoyting but rn the printout for this is pretty nasty. \n",
    "\n",
    "for model in models:\n",
    "    scores = []\n",
    "    model.fit(review_sample['review_text'], review_sample['stars'])\n",
    "    y_pred = model.predict(review_sample['review_text'])\n",
    "    score = accuracy_score(review_sample['stars'], y_pred)\n",
    "    scores.append(score)\n",
    "    print(model)\n",
    "    print (\"Accuracy of {} is {:03f}\".format(model, numpy.mean(scores)))\n",
    "    print(classification_report(review_sample['stars'], y_pred, labels= [1,2,3,4,5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "models3 = []\n",
    "for form in (MultinomialNB, SGDClassifier, KNeighborsClassifier, LogisticRegression, LinearSVC, RandomForestClassifier):\n",
    "    models3.append(create_pipeline(review_sample['review_text'],form(), TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun,token_pattern=None, ngram_range=(1,2)), 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Pipeline(memory=None,\n",
      "     steps=[('vectorize', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2',\n",
      "        preprocessor=<function ...      vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]) is 0.490000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       125\n",
      "           2       0.00      0.00      0.00        73\n",
      "           3       0.00      0.00      0.00       124\n",
      "           4       1.00      0.16      0.28       225\n",
      "           5       0.47      1.00      0.64       453\n",
      "\n",
      "   micro avg       0.49      0.49      0.49      1000\n",
      "   macro avg       0.29      0.23      0.18      1000\n",
      "weighted avg       0.44      0.49      0.35      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Pipeline(memory=None,\n",
      "     steps=[('vectorize', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2',\n",
      "        preprocessor=<function ...m_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False))]) is 1.000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       125\n",
      "           2       1.00      1.00      1.00        73\n",
      "           3       1.00      1.00      1.00       124\n",
      "           4       1.00      1.00      1.00       225\n",
      "           5       1.00      1.00      1.00       453\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n",
      "Accuracy of Pipeline(memory=None,\n",
      "     steps=[('vectorize', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2',\n",
      "        preprocessor=<function ...ki',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'))]) is 0.640000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.70      0.63       125\n",
      "           2       0.46      0.36      0.40        73\n",
      "           3       0.55      0.48      0.51       124\n",
      "           4       0.56      0.54      0.55       225\n",
      "           5       0.74      0.76      0.75       453\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      1000\n",
      "   macro avg       0.58      0.57      0.57      1000\n",
      "weighted avg       0.64      0.64      0.64      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Pipeline(memory=None,\n",
      "     steps=[('vectorize', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2',\n",
      "        preprocessor=<function ...penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))]) is 0.640000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.38      0.55       125\n",
      "           2       0.00      0.00      0.00        73\n",
      "           3       1.00      0.15      0.25       124\n",
      "           4       0.99      0.54      0.70       225\n",
      "           5       0.56      1.00      0.72       453\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      1000\n",
      "   macro avg       0.71      0.41      0.44      1000\n",
      "weighted avg       0.72      0.64      0.58      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Pipeline(memory=None,\n",
      "     steps=[('vectorize', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2',\n",
      "        preprocessor=<function ...ax_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))]) is 1.000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       125\n",
      "           2       1.00      1.00      1.00        73\n",
      "           3       1.00      1.00      1.00       124\n",
      "           4       1.00      1.00      1.00       225\n",
      "           5       1.00      1.00      1.00       453\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Pipeline(memory=None,\n",
      "     steps=[('vectorize', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2',\n",
      "        preprocessor=<function ...obs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))]) is 0.983000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.99      1.00       125\n",
      "           2       0.99      0.97      0.98        73\n",
      "           3       1.00      0.99      1.00       124\n",
      "           4       1.00      0.95      0.97       225\n",
      "           5       0.97      1.00      0.98       453\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1000\n",
      "   macro avg       0.99      0.98      0.98      1000\n",
      "weighted avg       0.98      0.98      0.98      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models3:\n",
    "    scores = []\n",
    "    model.fit(review_sample['review_text'], review_sample['stars'])\n",
    "    y_pred = model.predict(review_sample['review_text'])\n",
    "    score = accuracy_score(review_sample['stars'], y_pred)\n",
    "    scores.append(score)\n",
    "    print (\"Accuracy of {} is {:03f}\".format(model, numpy.mean(scores)))\n",
    "    y_pred = model.predict(review_sample['review_text'])\n",
    "    print(classification_report(review_sample['stars'], y_pred, labels= [1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is a visualization of our data. I was hoping that some visual clusters would pop out but no luck. If you get a  n_components must be < n_features; got 50 >= 50 error then set n_components less than 50 in TSNE visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the hobbies corpus (don't need to do again, included for completeness)\n",
    "\n",
    "vectorizor = MeanEmbeddingTransformer()\n",
    "vectorizor1 = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None, ngram_range = (1,2))\n",
    "\n",
    "docs = vectorizor.fit_transform(review_sample['review_text'])\n",
    "labels = review_sample['stars']\n",
    "\n",
    "# Create the visualizer and draw the vectors\n",
    "tsne = TSNEVisualizer(size=(1080, 720))\n",
    "tsne.fit(docs, labels)\n",
    "# Note: TSNE is a model visualizer not a feature visualizer, \n",
    "# so it has no transform method!\n",
    "tsne.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
