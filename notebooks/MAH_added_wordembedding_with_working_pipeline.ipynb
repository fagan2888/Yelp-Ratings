{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorry for this crazy import cell... this will be cleaned up\n",
    "\n",
    "import psycopg2\n",
    "import numpy\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import nltk.corpus\n",
    "from nltk.text import TextCollection\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "stops = nltk.corpus.stopwords.words('english')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input your PostGres credentials to connect\n",
    "\n",
    "dbname = 'yelp'\n",
    "username = 'postgres'\n",
    "host = 'localhost'\n",
    "password = 'Khoobam1234!'\n",
    "\n",
    "conn = psycopg2.connect('dbname={} user={} host={} password={}'.format(dbname, username, host, password))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    SELECT * FROM review LIMIT 2000\n",
    "\"\"\")\n",
    "\n",
    "cols = ['review_id', 'user_id', 'business_id', 'stars', 'review_date', 'review_text', 'useful', 'funny', 'cool']\n",
    "\n",
    "review_sample = pd.DataFrame(cur.fetchall(), columns=cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample['review_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contractions:\n",
    "https://gist.github.com/J3RN/ed7b420a6ea1d5bd6d06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_nonneg_contr = [\"could've\",\n",
    "\"he'd\",\n",
    "\"he'd've\",\n",
    "\"he'll\",\n",
    "\"he's\",\n",
    "\"how'd\",\n",
    "\"how'll\",\n",
    "\"how's\",\n",
    "\"i'd\",\n",
    "\"i'd've\",\n",
    "\"i'll\",\n",
    "\"i'm\",\n",
    "\"i've\",\n",
    "\"it'd\",\n",
    "\"it'd've\",\n",
    "\"it'll\",\n",
    "\"it's\",\n",
    "\"let's\",\n",
    "\"ma'am\",\n",
    "\"might've\",\n",
    "\"must've\",\n",
    "\"o'clock\",\n",
    "\"'ow's'at\",\n",
    "\"she'd\",\n",
    "\"she'd've\",\n",
    "\"she'll\",\n",
    "\"she's\",\n",
    "\"should've\",\n",
    "\"somebody'd\",\n",
    "\"somebody'd've\",\n",
    "\"somebody'll\",\n",
    "\"somebody's\",\n",
    "\"someone'd\",\n",
    "\"someone'd've\",\n",
    "\"someone'll\",\n",
    "\"someone's\",\n",
    "\"something'd\",\n",
    "\"something'd've\",\n",
    "\"something'll\",\n",
    "\"something's\",\n",
    "\"that'll\",\n",
    "\"that's\",\n",
    "\"there'd\",\n",
    "\"there'd've\",\n",
    "\"there're\",\n",
    "\"there's\",\n",
    "\"they'd\",\n",
    "\"they'd've\",\n",
    "\"they'll\",\n",
    "\"they're\",\n",
    "\"they've\",\n",
    "\"'twas\",\n",
    "\"we'd\",\n",
    "\"we'd've\",\n",
    "\"we'll\",\n",
    "\"we're\",\n",
    "\"we've\",\n",
    "\"what'll\",\n",
    "\"what're\",\n",
    "\"what's\",\n",
    "\"what've\",\n",
    "\"when's\",\n",
    "\"where'd\",\n",
    "\"where's\",\n",
    "\"where've\",\n",
    "\"who'd\",\n",
    "\"who'd've\",\n",
    "\"who'll\",\n",
    "\"who're\",\n",
    "\"who's\",\n",
    "\"who've\",\n",
    "\"why'll\",\n",
    "\"why're\",\n",
    "\"why's\",\n",
    "\"would've\",\n",
    "\"y'all\",\n",
    "\"y'all'll\",\n",
    "\"y'all'd've\",\n",
    "\"you'd\",\n",
    "\"you'd've\",\n",
    "\"you'll\",\n",
    "\"you're\",\n",
    "\"you've\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_stops1 = ['no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'don',\n",
    " \"don't\",\n",
    " 'ain',\n",
    " 'aren',\n",
    " \"aren't\",\n",
    " 'couldn',\n",
    " \"couldn't\",\n",
    " 'didn',\n",
    " \"didn't\",\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'hadn',\n",
    " \"hadn't\",\n",
    " 'hasn',\n",
    " \"hasn't\",\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'isn',\n",
    " \"isn't\",\n",
    " 'mightn',\n",
    " \"mightn't\",\n",
    " 'mustn',\n",
    " \"mustn't\",\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'shouldn',\n",
    " \"shouldn't\",\n",
    " 'wasn',\n",
    " \"wasn't\",\n",
    " 'weren',\n",
    " \"weren't\",\n",
    " 'won',\n",
    " \"won't\",\n",
    " 'wouldn',\n",
    " \"wouldn't\",\n",
    " 'but',\n",
    " \"don'\",\n",
    " \"ain't\"]\n",
    "\n",
    "words_to_be_added = ['us']\n",
    "stops1 = ['i',\n",
    " 'me',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'we',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'you',\n",
    " \"you're\",\n",
    " \"you've\",\n",
    " \"you'll\",\n",
    " \"you'd\",\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves',\n",
    " 'he',\n",
    " 'him',\n",
    " 'his',\n",
    " 'himself',\n",
    " 'she',\n",
    " \"she's\",\n",
    " 'her',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'it',\n",
    " \"it's\",\n",
    " 'its',\n",
    " 'itself',\n",
    " 'they',\n",
    " 'them',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'themselves',\n",
    " 'what',\n",
    " 'which',\n",
    " 'who',\n",
    " 'whom',\n",
    " 'this',\n",
    " 'that',\n",
    " \"that'll\",\n",
    " 'these',\n",
    " 'those',\n",
    " 'am',\n",
    " 'is',\n",
    " 'are',\n",
    " 'was',\n",
    " 'were',\n",
    " 'be',\n",
    " 'been',\n",
    " 'being',\n",
    " 'have',\n",
    " 'has',\n",
    " 'had',\n",
    " 'having',\n",
    " 'do',\n",
    " 'does',\n",
    " 'did',\n",
    " 'doing',\n",
    " 'a',\n",
    " 'an',\n",
    " 'the',\n",
    " 'and',\n",
    " 'but',\n",
    " 'if',\n",
    " 'or',\n",
    " 'because',\n",
    " 'as',\n",
    " 'until',\n",
    " 'while',\n",
    " 'of',\n",
    " 'at',\n",
    " 'by',\n",
    " 'for',\n",
    " 'with',\n",
    " 'about',\n",
    " 'against',\n",
    " 'between',\n",
    " 'into',\n",
    " 'through',\n",
    " 'during',\n",
    " 'before',\n",
    " 'after',\n",
    " 'above',\n",
    " 'below',\n",
    " 'to',\n",
    " 'from',\n",
    " 'up',\n",
    " 'down',\n",
    " 'in',\n",
    " 'out',\n",
    " 'on',\n",
    " 'off',\n",
    " 'over',\n",
    " 'under',\n",
    " 'again',\n",
    " 'further',\n",
    " 'then',\n",
    " 'once',\n",
    " 'here',\n",
    " 'there',\n",
    " 'when',\n",
    " 'where',\n",
    " 'why',\n",
    " 'how',\n",
    " 'all',\n",
    " 'any',\n",
    " 'both',\n",
    " 'each',\n",
    " 'few',\n",
    " 'more',\n",
    " 'most',\n",
    " 'other',\n",
    " 'some',\n",
    " 'such',\n",
    " 'no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'only',\n",
    " 'own',\n",
    " 'same',\n",
    " 'so',\n",
    " 'than',\n",
    " 'too',\n",
    " 'very',\n",
    " 's',\n",
    " 't',\n",
    " 'can',\n",
    " 'will',\n",
    " 'just',\n",
    " 'don',\n",
    " \"don't\",\n",
    " 'should',\n",
    " \"should've\",\n",
    " 'now',\n",
    " 'd',\n",
    " 'll',\n",
    " 'm',\n",
    " 'o',\n",
    " 're',\n",
    " 've',\n",
    " 'y',\n",
    " 'ain',\n",
    " 'aren',\n",
    " \"aren't\",\n",
    " 'couldn',\n",
    " \"couldn't\",\n",
    " 'didn',\n",
    " \"didn't\",\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'hadn',\n",
    " \"hadn't\",\n",
    " 'hasn',\n",
    " \"hasn't\",\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'isn',\n",
    " \"isn't\",\n",
    " 'ma',\n",
    " 'mightn',\n",
    " \"mightn't\",\n",
    " 'mustn',\n",
    " \"mustn't\",\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'shouldn',\n",
    " \"shouldn't\",\n",
    " 'wasn',\n",
    " \"wasn't\",\n",
    " 'weren',\n",
    " \"weren't\",\n",
    " 'won',\n",
    " \"won't\",\n",
    " 'wouldn',\n",
    " \"wouldn't\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in neg_stops1:\n",
    "    if x in stops1:\n",
    "        stops1.remove(x)\n",
    "        \n",
    "stops1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stops = stops1 + common_nonneg_contr\n",
    "list(set(new_stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell below is preproccessing and tokenezation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_stops(tokens):\n",
    "    stops = nltk.corpus.stopwords.words('english')\n",
    "    neg_stops = [\n",
    "    'no', 'not', 'nor', 'don\\'', 'don\\'t', 'ain', \n",
    "    'ain\\'t', 'aren\\'t', 'aren', 'couldn', 'couldn\\'t', \n",
    "    'didn', 'didn\\'t', 'doesn', 'doesn\\'t', 'hadn', \n",
    "    'hadn\\'t', 'hasn', 'hasn\\'t', 'haven', 'haven\\'t',\n",
    "    'isn', 'isn\\'t', 'mightn', 'mightn\\'t', 'mustn', \n",
    "    'mustn\\'t', 'needn', 'needn\\'t', 'shan', 'shan\\'t',\n",
    "    'shouldn', 'shouldn\\'t', 'wasn', 'wasn\\'t', 'weren',\n",
    "    'weren\\'t', 'won', 'won\\'t', 'wouldn', 'wouldn\\'t'\n",
    "    ]\n",
    "#still leaves in but and don.. fix this.. \n",
    "#doesn't get rid of other obvious stopwords, like i'm, they're....\n",
    "    for x in neg_stops:\n",
    "        if x in stops:\n",
    "            stops.remove(x)\n",
    "            \n",
    "    tokens_without_stops = [token for token in tokens if token not in stops]\n",
    "    return tokens_without_stops       \n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def _clean_review2(text):\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "    text = re.sub(r\"[^A-Za-z\\s']\", '', text)  \n",
    "    tokens = [token for token in text.split() if token not in new_stops]\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "    return tokens\n",
    "\n",
    "def _process_review2(text):\n",
    "    tokens = _remove_stops(_clean_review2(text))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _process_review(text):\n",
    "    def _create_stop_words():\n",
    "        stops = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "        neg_stops = ['no',\n",
    "         'nor',\n",
    "         'not',\n",
    "         'don',\n",
    "         \"don't\",\n",
    "         'ain',\n",
    "         'aren',\n",
    "         \"aren't\",\n",
    "         'couldn',\n",
    "         \"couldn't\",\n",
    "         'didn',\n",
    "         \"didn't\",\n",
    "         'doesn',\n",
    "         \"doesn't\",\n",
    "         'hadn',\n",
    "         \"hadn't\",\n",
    "         'hasn',\n",
    "         \"hasn't\",\n",
    "         'haven',\n",
    "         \"haven't\",\n",
    "         'isn',\n",
    "         \"isn't\",\n",
    "         'mightn',\n",
    "         \"mightn't\",\n",
    "         'mustn',\n",
    "         \"mustn't\",\n",
    "         'needn',\n",
    "         \"needn't\",\n",
    "         'shan',\n",
    "         \"shan't\",\n",
    "         'shouldn',\n",
    "         \"shouldn't\",\n",
    "         'wasn',\n",
    "         \"wasn't\",\n",
    "         'weren',\n",
    "         \"weren't\",\n",
    "         \"won'\",\n",
    "         \"won't\",\n",
    "         'wouldn',\n",
    "         \"wouldn't\",\n",
    "         'but',\n",
    "         \"don'\",\n",
    "         \"ain't\"]\n",
    "\n",
    "        common_nonneg_contr = [\"could've\",\n",
    "        \"he'd\",\n",
    "        \"he'd've\",\n",
    "        \"he'll\",\n",
    "        \"he's\",\n",
    "        \"how'd\",\n",
    "        \"how'll\",\n",
    "        \"how's\",\n",
    "        \"i'd\",\n",
    "        \"i'd've\",\n",
    "        \"i'll\",\n",
    "        \"i'm\",\n",
    "        \"i've\",\n",
    "        \"it'd\",\n",
    "        \"it'd've\",\n",
    "        \"it'll\",\n",
    "        \"it's\",\n",
    "        \"let's\",\n",
    "        \"ma'am\",\n",
    "        \"might've\",\n",
    "        \"must've\",\n",
    "        \"o'clock\",\n",
    "        \"'ow's'at\",\n",
    "        \"she'd\",\n",
    "        \"she'd've\",\n",
    "        \"she'll\",\n",
    "        \"she's\",\n",
    "        \"should've\",\n",
    "        \"somebody'd\",\n",
    "        \"somebody'd've\",\n",
    "        \"somebody'll\",\n",
    "        \"somebody's\",\n",
    "        \"someone'd\",\n",
    "        \"someone'd've\",\n",
    "        \"someone'll\",\n",
    "        \"someone's\",\n",
    "        \"something'd\",\n",
    "        \"something'd've\",\n",
    "        \"something'll\",\n",
    "        \"something's\",\n",
    "        \"that'll\",\n",
    "        \"that's\",\n",
    "        \"there'd\",\n",
    "        \"there'd've\",\n",
    "        \"there're\",\n",
    "        \"there's\",\n",
    "        \"they'd\",\n",
    "        \"they'd've\",\n",
    "        \"they'll\",\n",
    "        \"they're\",\n",
    "        \"they've\",\n",
    "        \"'twas\",\n",
    "        \"we'd\",\n",
    "        \"we'd've\",\n",
    "        \"we'll\",\n",
    "        \"we're\",\n",
    "        \"we've\",\n",
    "        \"what'll\",\n",
    "        \"what're\",\n",
    "        \"what's\",\n",
    "        \"what've\",\n",
    "        \"when's\",\n",
    "        \"where'd\",\n",
    "        \"where's\",\n",
    "        \"where've\",\n",
    "        \"who'd\",\n",
    "        \"who'd've\",\n",
    "        \"who'll\",\n",
    "        \"who're\",\n",
    "        \"who's\",\n",
    "        \"who've\",\n",
    "        \"why'll\",\n",
    "        \"why're\",\n",
    "        \"why's\",\n",
    "        \"would've\",\n",
    "        \"y'all\",\n",
    "        \"y'all'll\",\n",
    "        \"y'all'd've\",\n",
    "        \"you'd\",\n",
    "        \"you'd've\",\n",
    "        \"you'll\",\n",
    "        \"you're\",\n",
    "        \"you've\"]\n",
    "\n",
    "        letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
    "          'u', 'v', 'w', 'x', 'y', 'z']\n",
    "        \n",
    "        ranks = ['st', 'nd', 'rd', 'th']\n",
    "        \n",
    "        for x in neg_stops:\n",
    "            if x in stops:\n",
    "                stops.remove(x)\n",
    "\n",
    "        new_stops = stops + common_nonneg_contr + letters + ranks + [\"\"] + ['us'] + ['']\n",
    "        stops = list(set(new_stops))\n",
    "        return stops\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "        tag_dict = {\"a\": wordnet.ADJ,\n",
    "                    \"n\": wordnet.NOUN,\n",
    "                    \"v\": wordnet.VERB,\n",
    "                    \"r\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def _clean_review(text):\n",
    "        text = text.lower()\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8', 'ignore')\n",
    "        tokenizer = nltk.RegexpTokenizer('\\w+\\'?\\w+')\n",
    "        filtered_tokens = [(re.sub(r\"[^A-Za-z\\s']\", '', token)) for token in tokenizer.tokenize(text)]\n",
    "        stops = _create_stop_words()\n",
    "        tokens = [token for token in filtered_tokens if token not in stops]\n",
    "        tokens = [re.sub(\"'s\", '', token) for token in tokens if re.sub(\"'s\", '', token) != '']\n",
    "        for i, token in enumerate(tokens):\n",
    "            tokens[i] = wnl.lemmatize(token, pos= get_wordnet_pos(token))\n",
    "        tokens = [token for token in tokens if token != '' and token not in stops]\n",
    "        return tokens\n",
    "    \n",
    "    return _clean_review(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fyi, tweaked Alice's code here to allow map_on_column to accept another function\n",
    "def map_on_column(data, function):\n",
    "    data['review_text'] = data['review_text'].map(lambda x: function(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_on_column(review_sample, _process_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell below creates a class that will be used in the vectorization code a few cells down. FYI, path in \"_load_words\" is machine specific. Also, you have to download the GloVe training set from GloVe website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MeanEmbeddingTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._vocab, self._E = self._load_words()\n",
    "        \n",
    "    \n",
    "    def _load_words(self):\n",
    "        E = {}\n",
    "        vocab = []\n",
    "\n",
    "        with open(r'C:\\Users\\Mark\\YelpProject\\poop\\glove.6B.200d.txt', encoding=\"utf8\") as file:\n",
    "            for i, line in enumerate(file):\n",
    "                l = line.split(' ')\n",
    "                if l[0].isalpha():\n",
    "                    v = [float(i) for i in l[1:]]\n",
    "                    E[l[0]] = numpy.array(v)\n",
    "                    vocab.append(l[0])\n",
    "        return numpy.array(vocab), E            \n",
    "\n",
    "    \n",
    "    def _get_word(self, v):\n",
    "        for i, emb in enumerate(self._E):\n",
    "            if numpy.array_equal(emb, v):\n",
    "                return self._vocab[i]\n",
    "        return None\n",
    "    #in _doc_mean Ithink maybe we should get rid of w.lower.strip\n",
    "    def _doc_mean(self, doc):\n",
    "        return numpy.mean(numpy.array([self._E[w.lower().strip()] for w in doc if w.lower().strip() in self._E]), axis=0)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return numpy.array([self._doc_mean(doc) for doc in X])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    \n",
    "#retrived from https://www.kaggle.com/nhrade/text-classification-using-word-embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell below is attempting to create a class for gensim doc2vec but it's not working yet and tbh I'm not sure how close I am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import sparse2full\n",
    "\n",
    "class GensimDoc2Vec(TransformerMixin):\n",
    "    def __init__(self, path=None):\n",
    "        self.path = path\n",
    "        self.id2word = None\n",
    "        self.load()\n",
    "    \n",
    "    def load(self):\n",
    "        if os.path.exists(self.path):\n",
    "            self.id2word = Dictionary.load(self.path)\n",
    "    \n",
    "    def save(self):\n",
    "        self.id2word.save(self.path)\n",
    "    \n",
    "    def fit(self, documents, labels = None):\n",
    "        self.id2word = Dictionary(documents)\n",
    "        self.save()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            docvec = self.id2word.doc2vec\n",
    "            yield sparse2full(docvec, len(self.id2word))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GloVeMeanEmbedding(text):\n",
    "    met = MeanEmbeddingTransformer()\n",
    "    GloVeVector = numpy.array(met.fit_transform(map_on_column(text, _clean_review2)))\n",
    "    return GloVeVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenDoc2Vec(text, path):\n",
    "    met = GensimDoc2Vec()\n",
    "    GenVector = numpy.array(met.transform(map_on_column(text, _clean_review2)))\n",
    "    return GenVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell below is vectorization. So far it can do TFIDF on specified number of n-grams, and I think the part with GloVe word embedding is almost functioning, but I haven't started \"word2vec\" vectorization yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "#def GloVeDistributedRep(text):\n",
    "    ##GloVeVector = numpy.append(MeanEmbeddingTransformer().fit_transform(map_on_column(text, _clean_review2))\n",
    "    #return GloVeVector\n",
    "\n",
    "\n",
    "def vectorize(text, method, grams):\n",
    "    if method == 'TFIDF':\n",
    "        tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None, ngram_range = (1,grams))\n",
    "        TDIFvectors = tfidf.fit_transform(map_on_column(text, _clean_review2))\n",
    "        print(TDIFvectors)\n",
    "    if method == 'DistributedRep':\n",
    "        GloVevectors = GloVeDistributedRep(map_on_column(text, _clean_review2))\n",
    "        print(GloVevectors)\n",
    "    if method == 'Word2Vec':\n",
    "        print('not yet defined')\n",
    "    else:\n",
    "        print('choose vectorization method')\n",
    "        \n",
    "\n",
    "def vectorize2(text, method, grams):\n",
    "    if method == 'TFIDF':\n",
    "        tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None, ngram_range = (1,grams))\n",
    "        TDIFvectors = tfidf.fit_transform(text)\n",
    "        print(TDIFvectors)\n",
    "    if method == 'DistributedRep':\n",
    "        GloVevectors = GloVeDistributedRep(text)\n",
    "        print(GloVevectors)\n",
    "    if method == 'Word2Vec':\n",
    "        print('not yet defined')\n",
    "    else:\n",
    "        print('choose vectorization method')\n",
    "        \n",
    "        \n",
    "        \n",
    "def vectorize3(text, method, grams):\n",
    "    if method == 'TFIDF':\n",
    "        tfidf = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None, ngram_range = (1,grams))\n",
    "        TDIFvectors = tfidf.fit_transform(text)\n",
    "        print(TDIFvectors)\n",
    "    if method == 'DistributedRep':\n",
    "        GloVevectors = GloVeDistributedRep(text)\n",
    "        print(GloVevectors)\n",
    "    if method == 'Word2Vec':\n",
    "        print('not yet defined')\n",
    "    else:\n",
    "        print('choose vectorization method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nltk.corpus\n",
    "from nltk.text import TextCollection\n",
    "\n",
    "def TFIDFvectorize(corpus):\n",
    "    corpus = [_clean_review2(doc) for doc in corpus]\n",
    "    texts = TextCollection(corpus)\n",
    "    \n",
    "    for doc in corpus:\n",
    "        return {\n",
    "            term: texts.tf_idf(term, doc)\n",
    "            for term in doc\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# below is first attempt at pipeline. Just as a heads up, some models can't handel negative values and so can't be used with word embedding vectorization. Baysian, for example, can only handle positive vectors and so can be used with TF-IDF but not GloVe word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#the Vectorization_method arugment will be one of the classes that we wrote. That argument itself will have argumens within it.\n",
    "def create_pipeline(documents, estimator, vectorization_method, ngrams, c=10000, reduction=False):\n",
    "    steps = [\n",
    "        ('vectorize', vectorization_method) \n",
    "    ]\n",
    "    \n",
    "    if reduction:\n",
    "        steps.append((\n",
    "        'reduction', TruncatedSVD(n_components=c)\n",
    "        ))\n",
    "    \n",
    "    steps.append(('classifier', estimator))\n",
    "    return Pipeline(steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this line of code is basically just filler to make sure the pipeline is being used correct. It's functioning output should just be a list of the pipline steps.\n",
    "create_pipeline(review_sample['review_text'], MultinomialNB, MeanEmbeddingTransformer(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rn the models are just using the default hyperparameters (for exxample, n_neighbors = 5. I'm not sure how to change this. I get an error when I try to change it here.\n",
    "models = []\n",
    "for form in (SGDClassifier, KNeighborsClassifier, LogisticRegression, LinearSVC, RandomForestClassifier, SVC, BaggingClassifier):\n",
    "    models.append(create_pipeline(review_sample['review_text'],form(), MeanEmbeddingTransformer(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model.fit(review_sample['review_text'], review_sample['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#really annoyting but rn the printout for this is pretty nasty. \n",
    "\n",
    "for model in models:\n",
    "    scores = []\n",
    "    model.fit(review_sample['review_text'], review_sample['stars'])\n",
    "    y_pred = model.predict(review_sample['review_text'])\n",
    "    score = accuracy_score(review_sample['stars'], y_pred)\n",
    "    scores.append(score)\n",
    "    print(model)\n",
    "    print (\"Accuracy of {} is {:03f}\".format(model, numpy.mean(scores)))\n",
    "    print(classification_report(review_sample['stars'], y_pred, labels= [1,2,3,4,5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models3 = []\n",
    "for form in (MultinomialNB, SGDClassifier, KNeighborsClassifier, LogisticRegression, LinearSVC, RandomForestClassifier):\n",
    "    models3.append(create_pipeline(review_sample['review_text'],form(), TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun,token_pattern=None, ngram_range=(1,4)), 4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models3:\n",
    "    scores = []\n",
    "    model.fit(review_sample['review_text'], review_sample['stars'])\n",
    "    y_pred = model.predict(review_sample['review_text'])\n",
    "    score = accuracy_score(review_sample['stars'], y_pred)\n",
    "    scores.append(score)\n",
    "    print (\"Accuracy of {} is {:03f}\".format(model, numpy.mean(scores)))\n",
    "    y_pred = model.predict(review_sample['review_text'])\n",
    "    print(classification_report(review_sample['stars'], y_pred, labels= [1,2,3,4,5]))\n",
    "    #cm = ConfusionMatrix(model, classes=[1,2,3,4,5])\n",
    "    #cm.fit(review_sample['review_text'], review_sample['review_text'])\n",
    "    #cm.score(review_sample['review_text'], review_sample['review_text'])\n",
    "    #print(cm.poof())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is a visualization of our data. I was hoping that some visual clusters would pop out but no luck. If you get a  n_components must be < n_features; got 50 >= 50 error then set n_components less than 50 in TSNE visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizor = MeanEmbeddingTransformer()\n",
    "vectorizor1 = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None, ngram_range = (1,2))\n",
    "\n",
    "docs = vectorizor.fit_transform(review_sample['review_text'])\n",
    "labels = review_sample['stars']\n",
    "\n",
    "\n",
    "tsne = TSNEVisualizer(metric= \"euclidean\", size=(1080, 720))\n",
    "tsne.fit(docs, labels)\n",
    "\n",
    "print(tsne.poof())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "grams = [1,2,3,5]\n",
    "\n",
    "for n in grams:\n",
    "    vectorizor = MeanEmbeddingTransformer()\n",
    "    vectorizor1 = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None, ngram_range = (1,n))\n",
    "    docs = vectorizor1.fit_transform(review_sample['review_text'])\n",
    "    labels = review_sample['stars']\n",
    "    tsne = TSNEVisualizer(size=(1080, 720))\n",
    "    tsne.fit(docs, labels)\n",
    "    print(tsne.poof())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
